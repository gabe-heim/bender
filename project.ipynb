{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modin.pandas as pd\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import math\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # data_set = \n",
    "# data_sets = {\n",
    "#     'sp500': {\n",
    "#         'data': pd.read_csv(f'/Users/gabeheim/documents/concatenated_price_data/sp500.csv', index_col=False).drop(['Adj Close'], axis=1)\n",
    "#     },\n",
    "#     'BTCUSDT': {\n",
    "#         'data': pd.read_csv(f'/Users/gabeheim/documents/concatenated_price_data/BTCUSDT.csv', index_col=False)\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# data_sets['BTCUSDT']['data'] = data_sets['BTCUSDT']['data'][:math.floor(len(data_sets['BTCUSDT']['data'])/32)]#16)]\n",
    "\n",
    "# # display(data_sets['sp500']['data'])\n",
    "# data_sets['BTCUSDT']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data_sets['BTCUSDT']['data']\n",
    "end = 150\n",
    "plt.plot(data.index[50:end], data.Close[50:end])\n",
    "plt.plot(data.index[50:end], data.High[50:end])\n",
    "plt.plot(data.index[50:end], data.Low[50:end])\n",
    "plt.plot(data.index[50:end], data.Open[50:end])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007492893252647893"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_percent = 0.01\n",
    "# display(data.High[0:0+train_window+1])\n",
    "# print(1455.219971 * (1 + target_percent))\n",
    "# data.High[0:0+train_window+1].loc[data.High >= 1455.219971 * (1 + target_percent)]#.index[0]\n",
    "data_sets['BTCUSDT']['data'].Close.pct_change().mean() * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def get_labels(row, target_percent=.01, stop_loss_percent=.005, mode='since3'):\n",
    "    if mode == 'since':\n",
    "        try:\n",
    "            target_index = data.loc[row[6]:][data.Close >= row[4] * (1 + target_percent)].iloc[0]._name\n",
    "        except IndexError:\n",
    "            target_index = data.index.max() + 5\n",
    "        try:\n",
    "            stop_loss_index = data.loc[row[6]:][data.Close <= row[4] * (1 - stop_loss_percent)].iloc[0]._name\n",
    "        except IndexError:\n",
    "            stop_loss_index = data.index.max() + 5\n",
    "        if target_index > stop_loss_index:\n",
    "            return 0\n",
    "        elif target_index < stop_loss_index:\n",
    "            return 1\n",
    "        return None\n",
    "    if mode == 'average':\n",
    "        mean = data.Close[row[6]:row[6]+10].mean()\n",
    "#         print(mean)\n",
    "        if mean - row[4] > 0: #row[4] * (1 + target_percent):\n",
    "            return 1\n",
    "#         elif mean <= row[4] * (1 - stop_loss_percent):\n",
    "#             return -1\n",
    "        else:\n",
    "            return 0\n",
    "        return None\n",
    "    if mode == 'next':\n",
    "        print('nextnensnes')\n",
    "        try:\n",
    "            next = data.Close.values[row[6]+1]\n",
    "        except:\n",
    "            next = 0\n",
    "#         print(next)\n",
    "        if next > row[4]: #row[4] * (1 + target_percent):\n",
    "            return 1\n",
    "#         elif mean <= row[4] * (1 - stop_loss_percent):\n",
    "#             return -1\n",
    "        else:\n",
    "            return 0\n",
    "        return None\n",
    "    if mode == 'since3':\n",
    "#         max_index = data.High[row[6]:row[6]+21].idxmax()\n",
    "#         min_index = data.Low[row[6]:row[6]+21].idxmin()\n",
    "        try:\n",
    "#             target_index = data.High[row[6]:row[6]+train_window+1].loc[data.High >= row[4] * (1 + target_percent)].index[0]\n",
    "            target_index = data.Close[row[6]:row[6]+train_window+1].loc[data.Close >= row[4] * (1 + target_percent)].index[0]\n",
    "        except IndexError:\n",
    "            target_index = data.index.max() + 5\n",
    "        try:\n",
    "#             stop_loss_index = data.Low[row[6]:row[6]+train_window+1].loc[data.Low <= row[4] * (1 - stop_loss_percent)].index[0]\n",
    "            stop_loss_index = data.Close[row[6]:row[6]+train_window+1].loc[data.Close <= row[4] * (1 - stop_loss_percent)].index[0]\n",
    "        except IndexError:\n",
    "            stop_loss_index = data.index.max() + 5\n",
    "        if target_index > stop_loss_index:\n",
    "            return 0\n",
    "        elif target_index < stop_loss_index:\n",
    "            return 2\n",
    "        return 1\n",
    "\n",
    "for name, datad in data_sets.items():\n",
    "    data = datad['data']\n",
    "    train_window = 20\n",
    "    data['index'] = data.index\n",
    "    # too volatile class?\n",
    "    n = 0\n",
    "    pool = multiprocessing.Pool(multiprocessing.cpu_count() - n)                         # Create a multiprocessing Pool\n",
    "\n",
    "    start = time.time()\n",
    "    data['label'] = pool.map(partial(get_labels, mode='next'), [tuple(r) for r in data.to_numpy()])  # process data_inputs iterable with pool\n",
    "    print(name, 'pool label took: ', time.time() - start)\n",
    "\n",
    "data_sets['BTCUSDT']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT 0 9291\n",
      "BTCUSDT 1 13204\n",
      "BTCUSDT 2 3520\n"
     ]
    }
   ],
   "source": [
    "for name, datad in data_sets.items():\n",
    "    data = datad['data']\n",
    "    for label in sorted(data.label.unique()):\n",
    "        print(name, label, len(data.loc[data.label == label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>sma_10</th>\n",
       "      <th>macd</th>\n",
       "      <th>...</th>\n",
       "      <th>macd_signal_percentage</th>\n",
       "      <th>macd_hist_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>slowd_percentage</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17 04:00:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>1.775183</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17 04:01:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-17 04:02:00.000000</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>0.261074</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-17 04:03:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-17 04:04:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.140796</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>2017-09-04 05:30:00.000000</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26010</td>\n",
       "      <td>0</td>\n",
       "      <td>4448.883</td>\n",
       "      <td>7.942480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>-0.006731</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>-0.006597</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>2017-09-04 05:31:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.198468</td>\n",
       "      <td>26011</td>\n",
       "      <td>0</td>\n",
       "      <td>4448.980</td>\n",
       "      <td>8.922450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>-0.017800</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>2017-09-04 05:32:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26012</td>\n",
       "      <td>0</td>\n",
       "      <td>4449.077</td>\n",
       "      <td>9.592574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>2017-09-04 05:33:00.000000</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>0.784977</td>\n",
       "      <td>26013</td>\n",
       "      <td>0</td>\n",
       "      <td>4446.500</td>\n",
       "      <td>7.534293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.013418</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>-0.044959</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>2017-09-04 05:34:00.000000</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>1.211797</td>\n",
       "      <td>26014</td>\n",
       "      <td>1</td>\n",
       "      <td>4440.674</td>\n",
       "      <td>3.394458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.004839</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.994112</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Date     Open     High      Low    Close  \\\n",
       "0      2017-08-17 04:00:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "1      2017-08-17 04:01:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "2      2017-08-17 04:02:00.000000  4280.56  4280.56  4280.56  4280.56   \n",
       "3      2017-08-17 04:03:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "4      2017-08-17 04:04:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "...                           ...      ...      ...      ...      ...   \n",
       "26010  2017-09-04 05:30:00.000000  4462.50  4462.50  4462.50  4462.50   \n",
       "26011  2017-09-04 05:31:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26012  2017-09-04 05:32:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26013  2017-09-04 05:33:00.000000  4433.95  4433.95  4433.94  4433.94   \n",
       "26014  2017-09-04 05:34:00.000000  4434.17  4434.17  4401.45  4401.45   \n",
       "\n",
       "         Volume  index  label    sma_10      macd  ...  \\\n",
       "0      1.775183      0      1       NaN       NaN  ...   \n",
       "1      0.000000      1      1       NaN       NaN  ...   \n",
       "2      0.261074      2      1       NaN       NaN  ...   \n",
       "3      0.012008      3      1       NaN       NaN  ...   \n",
       "4      0.140796      4      1       NaN       NaN  ...   \n",
       "...         ...    ...    ...       ...       ...  ...   \n",
       "26010  0.000000  26010      0  4448.883  7.942480  ...   \n",
       "26011  0.198468  26011      0  4448.980  8.922450  ...   \n",
       "26012  0.000000  26012      0  4449.077  9.592574  ...   \n",
       "26013  0.784977  26013      0  4446.500  7.534293  ...   \n",
       "26014  1.211797  26014      1  4440.674  3.394458  ...   \n",
       "\n",
       "       macd_signal_percentage  macd_hist_percentage  cci_24_percentage  \\\n",
       "0                    0.001840             -0.002316           0.001399   \n",
       "1                    0.001840             -0.002316           0.001399   \n",
       "2                    0.001840             -0.002316           0.001399   \n",
       "3                    0.001840             -0.002316           0.001399   \n",
       "4                    0.001840             -0.002316           0.001399   \n",
       "...                       ...                   ...                ...   \n",
       "26010                0.001675             -0.006731           0.000452   \n",
       "26011                0.002119             -0.017800           0.002359   \n",
       "26012                0.002336              0.002387           0.000632   \n",
       "26013                0.001399             -0.013418          -0.013633   \n",
       "26014               -0.000230              0.018382           0.032995   \n",
       "\n",
       "       mom_10_percentage  roc_10_percentage  rsi_5_percentage  \\\n",
       "0              -0.006634          -0.006591              -1.0   \n",
       "1              -0.006634          -0.006591              -1.0   \n",
       "2              -0.006634          -0.006591              -1.0   \n",
       "3              -0.006634          -0.006591              -1.0   \n",
       "4              -0.006634          -0.006591              -1.0   \n",
       "...                  ...                ...               ...   \n",
       "26010          -0.006640          -0.006597              -1.0   \n",
       "26011          -0.007460          -0.007413              -1.0   \n",
       "26012          -0.006634          -0.006591              -1.0   \n",
       "26013          -0.045152          -0.044959              -1.0   \n",
       "26014          -0.004873          -0.004839              -1.0   \n",
       "\n",
       "       wnr_9_percentage  slowk_percentage  slowd_percentage  adosc_percentage  \n",
       "0             -0.997032          0.000144         -0.001621          0.002114  \n",
       "1             -0.997032          0.000144         -0.001621          0.002114  \n",
       "2             -0.997032          0.000144         -0.001621          0.002114  \n",
       "3             -0.997032          0.000144         -0.001621          0.002114  \n",
       "4             -0.997032          0.000144         -0.001621          0.002114  \n",
       "...                 ...               ...               ...               ...  \n",
       "26010         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26011         -1.000000          0.000144         -0.001621          0.002114  \n",
       "26012         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26013         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26014         -0.994112          0.000144         -0.001621          0.002114  \n",
       "\n",
       "[26015 rows x 45 columns]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import talib as ta\n",
    "for name, datad in data_sets.items():\n",
    "    data = datad['data']\n",
    "    data['sma_10'] = ta.SMA(data.Close, timeperiod=10)\n",
    "    macd, macd_signal, macd_hist = ta.MACDFIX(data.Close, signalperiod=9)\n",
    "    data['macd'] = macd\n",
    "    data['macd_signal'] = macd_signal\n",
    "    data['macd_hist'] = macd_hist\n",
    "    data['cci_24'] = ta.CCI(data.High, data.Low, data.Close, timeperiod=24)\n",
    "    data['mom_10'] = ta.MOM(data.Close, timeperiod=10)\n",
    "    data['roc_10'] = ta.ROC(data.Close, timeperiod=10)\n",
    "    data['rsi_5'] = ta.RSI(data.Close, timeperiod=5)\n",
    "    data['wnr_9'] = ta.WILLR(data.High, data.Low, data.Close, timeperiod=9)\n",
    "    slowk, slowd = ta.STOCH(data.High, data.Low, data.Close, fastk_period=5, \n",
    "                            slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "    data['slowk'] = slowk\n",
    "    data['slowd'] = slowd\n",
    "    data['adosc'] = ta.ADOSC(data.High, data.Low, data.Close, data.Volume, fastperiod=3, slowperiod=10)\n",
    "    data = data[10:].reset_index()\n",
    "    data = data.drop(['level_0'], axis=1)\n",
    "    data['index'] = data.index\n",
    "\n",
    "data_sets['BTCUSDT']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>sma_10</th>\n",
       "      <th>macd</th>\n",
       "      <th>...</th>\n",
       "      <th>macd_signal_percentage</th>\n",
       "      <th>macd_hist_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>slowd_percentage</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17 04:00:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>1.775183</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17 04:01:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-17 04:02:00.000000</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>0.261074</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-17 04:03:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-17 04:04:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.140796</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>2017-09-04 05:30:00.000000</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26010</td>\n",
       "      <td>0</td>\n",
       "      <td>4448.883</td>\n",
       "      <td>7.942480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>-0.006731</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>-0.006597</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>2017-09-04 05:31:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.198468</td>\n",
       "      <td>26011</td>\n",
       "      <td>0</td>\n",
       "      <td>4448.980</td>\n",
       "      <td>8.922450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>-0.017800</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>2017-09-04 05:32:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26012</td>\n",
       "      <td>0</td>\n",
       "      <td>4449.077</td>\n",
       "      <td>9.592574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>2017-09-04 05:33:00.000000</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>0.784977</td>\n",
       "      <td>26013</td>\n",
       "      <td>0</td>\n",
       "      <td>4446.500</td>\n",
       "      <td>7.534293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.013418</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>-0.044959</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>2017-09-04 05:34:00.000000</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>1.211797</td>\n",
       "      <td>26014</td>\n",
       "      <td>1</td>\n",
       "      <td>4440.674</td>\n",
       "      <td>3.394458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.004839</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.994112</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Date     Open     High      Low    Close  \\\n",
       "0      2017-08-17 04:00:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "1      2017-08-17 04:01:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "2      2017-08-17 04:02:00.000000  4280.56  4280.56  4280.56  4280.56   \n",
       "3      2017-08-17 04:03:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "4      2017-08-17 04:04:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "...                           ...      ...      ...      ...      ...   \n",
       "26010  2017-09-04 05:30:00.000000  4462.50  4462.50  4462.50  4462.50   \n",
       "26011  2017-09-04 05:31:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26012  2017-09-04 05:32:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26013  2017-09-04 05:33:00.000000  4433.95  4433.95  4433.94  4433.94   \n",
       "26014  2017-09-04 05:34:00.000000  4434.17  4434.17  4401.45  4401.45   \n",
       "\n",
       "         Volume  index  label    sma_10      macd  ...  \\\n",
       "0      1.775183      0      1       NaN       NaN  ...   \n",
       "1      0.000000      1      1       NaN       NaN  ...   \n",
       "2      0.261074      2      1       NaN       NaN  ...   \n",
       "3      0.012008      3      1       NaN       NaN  ...   \n",
       "4      0.140796      4      1       NaN       NaN  ...   \n",
       "...         ...    ...    ...       ...       ...  ...   \n",
       "26010  0.000000  26010      0  4448.883  7.942480  ...   \n",
       "26011  0.198468  26011      0  4448.980  8.922450  ...   \n",
       "26012  0.000000  26012      0  4449.077  9.592574  ...   \n",
       "26013  0.784977  26013      0  4446.500  7.534293  ...   \n",
       "26014  1.211797  26014      1  4440.674  3.394458  ...   \n",
       "\n",
       "       macd_signal_percentage  macd_hist_percentage  cci_24_percentage  \\\n",
       "0                    0.001840             -0.002316           0.001399   \n",
       "1                    0.001840             -0.002316           0.001399   \n",
       "2                    0.001840             -0.002316           0.001399   \n",
       "3                    0.001840             -0.002316           0.001399   \n",
       "4                    0.001840             -0.002316           0.001399   \n",
       "...                       ...                   ...                ...   \n",
       "26010                0.001675             -0.006731           0.000452   \n",
       "26011                0.002119             -0.017800           0.002359   \n",
       "26012                0.002336              0.002387           0.000632   \n",
       "26013                0.001399             -0.013418          -0.013633   \n",
       "26014               -0.000230              0.018382           0.032995   \n",
       "\n",
       "       mom_10_percentage  roc_10_percentage  rsi_5_percentage  \\\n",
       "0              -0.006634          -0.006591              -1.0   \n",
       "1              -0.006634          -0.006591              -1.0   \n",
       "2              -0.006634          -0.006591              -1.0   \n",
       "3              -0.006634          -0.006591              -1.0   \n",
       "4              -0.006634          -0.006591              -1.0   \n",
       "...                  ...                ...               ...   \n",
       "26010          -0.006640          -0.006597              -1.0   \n",
       "26011          -0.007460          -0.007413              -1.0   \n",
       "26012          -0.006634          -0.006591              -1.0   \n",
       "26013          -0.045152          -0.044959              -1.0   \n",
       "26014          -0.004873          -0.004839              -1.0   \n",
       "\n",
       "       wnr_9_percentage  slowk_percentage  slowd_percentage  adosc_percentage  \n",
       "0             -0.997032          0.000144         -0.001621          0.002114  \n",
       "1             -0.997032          0.000144         -0.001621          0.002114  \n",
       "2             -0.997032          0.000144         -0.001621          0.002114  \n",
       "3             -0.997032          0.000144         -0.001621          0.002114  \n",
       "4             -0.997032          0.000144         -0.001621          0.002114  \n",
       "...                 ...               ...               ...               ...  \n",
       "26010         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26011         -1.000000          0.000144         -0.001621          0.002114  \n",
       "26012         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26013         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26014         -0.994112          0.000144         -0.001621          0.002114  \n",
       "\n",
       "[26015 rows x 45 columns]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "independent_indicators = ['macd', 'macd_signal', 'macd_hist', 'cci_24', 'mom_10', 'roc_10','rsi_5','wnr_9','slowk','slowd','adosc'] \n",
    "for name, datad in data_sets.items():\n",
    "    data = datad['data']\n",
    "    for indicator in independent_indicators:\n",
    "        name = indicator + '_min_max'\n",
    "        mean = data[indicator].mean()\n",
    "        std = data[indicator].std()\n",
    "        data[indicator].loc[data[indicator] > mean + 3 * std] = mean + 3 * std\n",
    "        data[indicator].loc[data[indicator] < mean - 3 * std] = mean - 3 * std\n",
    "        data[name] = (data[indicator] - mean) / std \n",
    "        data[name] = minmax_scale(data[indicator], feature_range=(-1,1))\n",
    "data_sets['BTCUSDT']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>sma_10</th>\n",
       "      <th>macd</th>\n",
       "      <th>...</th>\n",
       "      <th>macd_signal_percentage</th>\n",
       "      <th>macd_hist_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>slowd_percentage</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17 04:00:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>1.775183</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17 04:01:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-17 04:02:00.000000</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>0.261074</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-17 04:03:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-17 04:04:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.140796</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>2017-09-04 05:30:00.000000</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26010</td>\n",
       "      <td>0</td>\n",
       "      <td>4448.883</td>\n",
       "      <td>7.942480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>-0.006731</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>-0.006597</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>2017-09-04 05:31:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.198468</td>\n",
       "      <td>26011</td>\n",
       "      <td>0</td>\n",
       "      <td>4448.980</td>\n",
       "      <td>8.922450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>-0.017800</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>2017-09-04 05:32:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26012</td>\n",
       "      <td>0</td>\n",
       "      <td>4449.077</td>\n",
       "      <td>9.592574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>2017-09-04 05:33:00.000000</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>0.784977</td>\n",
       "      <td>26013</td>\n",
       "      <td>0</td>\n",
       "      <td>4446.500</td>\n",
       "      <td>7.534293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.013418</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>-0.044959</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>2017-09-04 05:34:00.000000</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>1.211797</td>\n",
       "      <td>26014</td>\n",
       "      <td>1</td>\n",
       "      <td>4440.674</td>\n",
       "      <td>3.394458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.004839</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.994112</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Date     Open     High      Low    Close  \\\n",
       "0      2017-08-17 04:00:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "1      2017-08-17 04:01:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "2      2017-08-17 04:02:00.000000  4280.56  4280.56  4280.56  4280.56   \n",
       "3      2017-08-17 04:03:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "4      2017-08-17 04:04:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "...                           ...      ...      ...      ...      ...   \n",
       "26010  2017-09-04 05:30:00.000000  4462.50  4462.50  4462.50  4462.50   \n",
       "26011  2017-09-04 05:31:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26012  2017-09-04 05:32:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26013  2017-09-04 05:33:00.000000  4433.95  4433.95  4433.94  4433.94   \n",
       "26014  2017-09-04 05:34:00.000000  4434.17  4434.17  4401.45  4401.45   \n",
       "\n",
       "         Volume  index  label    sma_10      macd  ...  \\\n",
       "0      1.775183      0      1       NaN       NaN  ...   \n",
       "1      0.000000      1      1       NaN       NaN  ...   \n",
       "2      0.261074      2      1       NaN       NaN  ...   \n",
       "3      0.012008      3      1       NaN       NaN  ...   \n",
       "4      0.140796      4      1       NaN       NaN  ...   \n",
       "...         ...    ...    ...       ...       ...  ...   \n",
       "26010  0.000000  26010      0  4448.883  7.942480  ...   \n",
       "26011  0.198468  26011      0  4448.980  8.922450  ...   \n",
       "26012  0.000000  26012      0  4449.077  9.592574  ...   \n",
       "26013  0.784977  26013      0  4446.500  7.534293  ...   \n",
       "26014  1.211797  26014      1  4440.674  3.394458  ...   \n",
       "\n",
       "       macd_signal_percentage  macd_hist_percentage  cci_24_percentage  \\\n",
       "0                    0.001840             -0.002316           0.001399   \n",
       "1                    0.001840             -0.002316           0.001399   \n",
       "2                    0.001840             -0.002316           0.001399   \n",
       "3                    0.001840             -0.002316           0.001399   \n",
       "4                    0.001840             -0.002316           0.001399   \n",
       "...                       ...                   ...                ...   \n",
       "26010                0.001675             -0.006731           0.000452   \n",
       "26011                0.002119             -0.017800           0.002359   \n",
       "26012                0.002336              0.002387           0.000632   \n",
       "26013                0.001399             -0.013418          -0.013633   \n",
       "26014               -0.000230              0.018382           0.032995   \n",
       "\n",
       "       mom_10_percentage  roc_10_percentage  rsi_5_percentage  \\\n",
       "0              -0.006634          -0.006591              -1.0   \n",
       "1              -0.006634          -0.006591              -1.0   \n",
       "2              -0.006634          -0.006591              -1.0   \n",
       "3              -0.006634          -0.006591              -1.0   \n",
       "4              -0.006634          -0.006591              -1.0   \n",
       "...                  ...                ...               ...   \n",
       "26010          -0.006640          -0.006597              -1.0   \n",
       "26011          -0.007460          -0.007413              -1.0   \n",
       "26012          -0.006634          -0.006591              -1.0   \n",
       "26013          -0.045152          -0.044959              -1.0   \n",
       "26014          -0.004873          -0.004839              -1.0   \n",
       "\n",
       "       wnr_9_percentage  slowk_percentage  slowd_percentage  adosc_percentage  \n",
       "0             -0.997032          0.000144         -0.001621          0.002114  \n",
       "1             -0.997032          0.000144         -0.001621          0.002114  \n",
       "2             -0.997032          0.000144         -0.001621          0.002114  \n",
       "3             -0.997032          0.000144         -0.001621          0.002114  \n",
       "4             -0.997032          0.000144         -0.001621          0.002114  \n",
       "...                 ...               ...               ...               ...  \n",
       "26010         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26011         -1.000000          0.000144         -0.001621          0.002114  \n",
       "26012         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26013         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26014         -0.994112          0.000144         -0.001621          0.002114  \n",
       "\n",
       "[26015 rows x 45 columns]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_indicators = ['Close', 'Volume', 'sma_10'] + independent_indicators\n",
    "for name, datad in data_sets.items():\n",
    "    data = datad['data']\n",
    "    for indicator in percentage_indicators:\n",
    "        name = indicator + '_percentage'\n",
    "        data[name] = data[indicator].pct_change().replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        mean = data[name].mean()\n",
    "        std = data[name].std()\n",
    "        data[name].loc[data[name] > mean + 3 * std] = mean + 3 * std\n",
    "        data[name].loc[data[name] < mean - 3 * std] = mean - 3 * std\n",
    "        data[name] = (data[name] - mean) / std \n",
    "        data[name] = minmax_scale(data[name], feature_range=(-1, 1))\n",
    "data_sets['BTCUSDT']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macd_min_max</th>\n",
       "      <th>macd_signal_min_max</th>\n",
       "      <th>macd_hist_min_max</th>\n",
       "      <th>cci_24_min_max</th>\n",
       "      <th>mom_10_min_max</th>\n",
       "      <th>roc_10_min_max</th>\n",
       "      <th>rsi_5_min_max</th>\n",
       "      <th>wnr_9_min_max</th>\n",
       "      <th>slowk_min_max</th>\n",
       "      <th>slowd_min_max</th>\n",
       "      <th>...</th>\n",
       "      <th>macd_signal_percentage</th>\n",
       "      <th>macd_hist_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>slowd_percentage</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>0.375231</td>\n",
       "      <td>0.425596</td>\n",
       "      <td>-0.042448</td>\n",
       "      <td>0.218187</td>\n",
       "      <td>0.028397</td>\n",
       "      <td>0.025506</td>\n",
       "      <td>0.245191</td>\n",
       "      <td>0.874826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>-0.006731</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>-0.006597</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>0.421786</td>\n",
       "      <td>0.432460</td>\n",
       "      <td>0.070574</td>\n",
       "      <td>0.236779</td>\n",
       "      <td>0.011126</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>-0.017800</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>0.453621</td>\n",
       "      <td>0.444894</td>\n",
       "      <td>0.127932</td>\n",
       "      <td>0.220684</td>\n",
       "      <td>0.011126</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>0.355839</td>\n",
       "      <td>0.433519</td>\n",
       "      <td>-0.117287</td>\n",
       "      <td>-0.073693</td>\n",
       "      <td>-0.318760</td>\n",
       "      <td>-0.299752</td>\n",
       "      <td>-0.279231</td>\n",
       "      <td>0.080840</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.013418</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>-0.044959</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>0.159170</td>\n",
       "      <td>0.381533</td>\n",
       "      <td>-0.535554</td>\n",
       "      <td>-0.270454</td>\n",
       "      <td>-0.719582</td>\n",
       "      <td>-0.675293</td>\n",
       "      <td>-0.533946</td>\n",
       "      <td>-0.823292</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.004839</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.994112</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       macd_min_max  macd_signal_min_max  macd_hist_min_max  cci_24_min_max  \\\n",
       "0          0.000000             0.000000           0.000000        0.000000   \n",
       "1          0.000000             0.000000           0.000000        0.000000   \n",
       "2          0.000000             0.000000           0.000000        0.000000   \n",
       "3          0.000000             0.000000           0.000000        0.000000   \n",
       "4          0.000000             0.000000           0.000000        0.000000   \n",
       "...             ...                  ...                ...             ...   \n",
       "26010      0.375231             0.425596          -0.042448        0.218187   \n",
       "26011      0.421786             0.432460           0.070574        0.236779   \n",
       "26012      0.453621             0.444894           0.127932        0.220684   \n",
       "26013      0.355839             0.433519          -0.117287       -0.073693   \n",
       "26014      0.159170             0.381533          -0.535554       -0.270454   \n",
       "\n",
       "       mom_10_min_max  roc_10_min_max  rsi_5_min_max  wnr_9_min_max  \\\n",
       "0            0.000000        0.000000       0.000000       0.000000   \n",
       "1            0.000000        0.000000       0.000000       0.000000   \n",
       "2            0.000000        0.000000       0.000000       0.000000   \n",
       "3            0.000000        0.000000       0.000000       0.000000   \n",
       "4            0.000000        0.000000       0.000000       0.000000   \n",
       "...               ...             ...            ...            ...   \n",
       "26010        0.028397        0.025506       0.245191       0.874826   \n",
       "26011        0.011126        0.009311       0.297502       1.000000   \n",
       "26012        0.011126        0.009311       0.297502       1.000000   \n",
       "26013       -0.318760       -0.299752      -0.279231       0.080840   \n",
       "26014       -0.719582       -0.675293      -0.533946      -0.823292   \n",
       "\n",
       "       slowk_min_max  slowd_min_max  ...  macd_signal_percentage  \\\n",
       "0           0.000000       0.000000  ...                0.001840   \n",
       "1           0.000000       0.000000  ...                0.001840   \n",
       "2           0.000000       0.000000  ...                0.001840   \n",
       "3           0.000000       0.000000  ...                0.001840   \n",
       "4           0.000000       0.000000  ...                0.001840   \n",
       "...              ...            ...  ...                     ...   \n",
       "26010       1.000000       0.333333  ...                0.001675   \n",
       "26011       1.000000       0.777778  ...                0.002119   \n",
       "26012       1.000000       1.000000  ...                0.002336   \n",
       "26013       0.333333       0.777778  ...                0.001399   \n",
       "26014      -0.333333       0.333333  ...               -0.000230   \n",
       "\n",
       "       macd_hist_percentage  cci_24_percentage  mom_10_percentage  \\\n",
       "0                 -0.002316           0.001399          -0.006634   \n",
       "1                 -0.002316           0.001399          -0.006634   \n",
       "2                 -0.002316           0.001399          -0.006634   \n",
       "3                 -0.002316           0.001399          -0.006634   \n",
       "4                 -0.002316           0.001399          -0.006634   \n",
       "...                     ...                ...                ...   \n",
       "26010             -0.006731           0.000452          -0.006640   \n",
       "26011             -0.017800           0.002359          -0.007460   \n",
       "26012              0.002387           0.000632          -0.006634   \n",
       "26013             -0.013418          -0.013633          -0.045152   \n",
       "26014              0.018382           0.032995          -0.004873   \n",
       "\n",
       "       roc_10_percentage  rsi_5_percentage  wnr_9_percentage  \\\n",
       "0              -0.006591              -1.0         -0.997032   \n",
       "1              -0.006591              -1.0         -0.997032   \n",
       "2              -0.006591              -1.0         -0.997032   \n",
       "3              -0.006591              -1.0         -0.997032   \n",
       "4              -0.006591              -1.0         -0.997032   \n",
       "...                  ...               ...               ...   \n",
       "26010          -0.006597              -1.0         -0.997032   \n",
       "26011          -0.007413              -1.0         -1.000000   \n",
       "26012          -0.006591              -1.0         -0.997032   \n",
       "26013          -0.044959              -1.0         -0.997032   \n",
       "26014          -0.004839              -1.0         -0.994112   \n",
       "\n",
       "       slowk_percentage  slowd_percentage  adosc_percentage  \n",
       "0              0.000144         -0.001621          0.002114  \n",
       "1              0.000144         -0.001621          0.002114  \n",
       "2              0.000144         -0.001621          0.002114  \n",
       "3              0.000144         -0.001621          0.002114  \n",
       "4              0.000144         -0.001621          0.002114  \n",
       "...                 ...               ...               ...  \n",
       "26010          0.000144         -0.001621          0.002114  \n",
       "26011          0.000144         -0.001621          0.002114  \n",
       "26012          0.000144         -0.001621          0.002114  \n",
       "26013          0.000144         -0.001621          0.002114  \n",
       "26014          0.000144         -0.001621          0.002114  \n",
       "\n",
       "[26015 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "26010    0\n",
       "26011    0\n",
       "26012    0\n",
       "26013    0\n",
       "26014    1\n",
       "Name: label, Length: 26015, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "INVALID_LABEL = 99\n",
    "\n",
    "dependent_indicators = ['Open','High','Low','Close','Volume', 'sma_10']\n",
    "\n",
    "for name, datad in data_sets.items():\n",
    "    data = datad['data']\n",
    "    datad['features'] = data.copy().drop(['index', 'Date', 'label'] + dependent_indicators + independent_indicators, axis=1\n",
    "                               ).fillna(0).replace([np.inf, -np.inf], np.nan).ffill()\n",
    "\n",
    "    display(datad['features'])\n",
    "\n",
    "    datad['labels'] = data['label'].copy().replace([np.inf, -np.inf], np.nan).fillna(INVALID_LABEL)\n",
    "\n",
    "    display(datad['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT\n",
      "[14 11  1 12  6  5 22 23  9 10 15  2  3  7 20  8 16 21 13 18 24 19 17  4\n",
      " 25]\n",
      "[14, 11, 1, 12, 6, 5, 22, 23, 9, 10, 15, 2, 3, 7, 20, 8, 16, 21, 13, 18, 24, 19, 17, 4, 25]\n",
      "\n",
      "[ 1  2 23  5 10  9 22 25  7  8 13  3  4 11 16  6 17 24 18 15 20 14 21 12\n",
      " 19]\n",
      "[15, 13, 24, 17, 16, 14, 44, 48, 16, 18, 28, 5, 7, 18, 36, 14, 33, 45, 31, 33, 44, 33, 38, 16, 44]\n",
      "\n",
      "[ 4  5  7 15  2  1 20 19  9 11 23  6 13  3 16 10 12 14 18 21 24 17 22  8\n",
      " 25]\n",
      "[19, 18, 31, 32, 18, 15, 64, 67, 25, 29, 51, 11, 20, 21, 52, 24, 45, 59, 49, 54, 68, 50, 60, 24, 69]\n",
      "\n",
      "[ 5  6 22 10  3 17 19 18 12 13 21  1 24 20  2 15  4  7 23 16 11 14  9  8\n",
      " 25]\n",
      "[24, 24, 53, 42, 21, 32, 83, 85, 37, 42, 72, 12, 44, 41, 54, 39, 49, 66, 72, 70, 79, 64, 69, 32, 94]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>adosc_min_max</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>rsi_5_min_max</th>\n",
       "      <th>wnr_9_min_max</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006597</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>0.411670</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.245191</td>\n",
       "      <td>0.874826</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>0.336225</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.274284</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.044959</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>0.134002</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.279231</td>\n",
       "      <td>0.080840</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>-0.994112</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.004839</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.074237</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.533946</td>\n",
       "      <td>-0.823292</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       wnr_9_percentage  cci_24_percentage  slowk_percentage  \\\n",
       "0             -0.997032           0.001399          0.000144   \n",
       "1             -0.997032           0.001399          0.000144   \n",
       "2             -0.997032           0.001399          0.000144   \n",
       "3             -0.997032           0.001399          0.000144   \n",
       "4             -0.997032           0.001399          0.000144   \n",
       "...                 ...                ...               ...   \n",
       "26010         -0.997032           0.000452          0.000144   \n",
       "26011         -1.000000           0.002359          0.000144   \n",
       "26012         -0.997032           0.000632          0.000144   \n",
       "26013         -0.997032          -0.013633          0.000144   \n",
       "26014         -0.994112           0.032995          0.000144   \n",
       "\n",
       "       roc_10_percentage  mom_10_percentage  adosc_min_max  rsi_5_percentage  \\\n",
       "0              -0.006591          -0.006634       0.000000              -1.0   \n",
       "1              -0.006591          -0.006634       0.000000              -1.0   \n",
       "2              -0.006591          -0.006634       0.000000              -1.0   \n",
       "3              -0.006591          -0.006634       0.000000              -1.0   \n",
       "4              -0.006591          -0.006634       0.000000              -1.0   \n",
       "...                  ...                ...            ...               ...   \n",
       "26010          -0.006597          -0.006640       0.411670              -1.0   \n",
       "26011          -0.007413          -0.007460       0.336225              -1.0   \n",
       "26012          -0.006591          -0.006634       0.274284              -1.0   \n",
       "26013          -0.044959          -0.045152       0.134002              -1.0   \n",
       "26014          -0.004839          -0.004873      -0.074237              -1.0   \n",
       "\n",
       "       rsi_5_min_max  wnr_9_min_max  adosc_percentage  \n",
       "0           0.000000       0.000000          0.002114  \n",
       "1           0.000000       0.000000          0.002114  \n",
       "2           0.000000       0.000000          0.002114  \n",
       "3           0.000000       0.000000          0.002114  \n",
       "4           0.000000       0.000000          0.002114  \n",
       "...              ...            ...               ...  \n",
       "26010       0.245191       0.874826          0.002114  \n",
       "26011       0.297502       1.000000          0.002114  \n",
       "26012       0.297502       1.000000          0.002114  \n",
       "26013      -0.279231       0.080840          0.002114  \n",
       "26014      -0.533946      -0.823292          0.002114  \n",
       "\n",
       "[26015 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_components = 10\n",
    "def get_pca_features(features, pca_components=10):\n",
    "\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    features_pca = pd.DataFrame(pca.fit_transform(features))\n",
    "    return features_pca\n",
    "\n",
    "def rfe(features, labels, sample_size=2000, trials=4, select=10):\n",
    "    estimator = SVR(kernel=\"linear\")\n",
    "    selector = RFE(estimator, n_features_to_select=1)#, step=1)\n",
    "    sums = [0] * len(features.columns)\n",
    "    for trial in range(trials):\n",
    "        samples = features.sample(n=sample_size)\n",
    "        selector = selector.fit(samples, labels.loc[samples.index])\n",
    "        print(selector.ranking_)\n",
    "        sums = [sum(i) for i in zip(sums, selector.ranking_)]\n",
    "        print(sums)\n",
    "        print()\n",
    "    \n",
    "    return features[[features.columns[i] for i in np.argsort(sums)[-select:] ]]\n",
    "\n",
    "for name, datad in data_sets.items():\n",
    "    features = datad['features']\n",
    "    labels = datad['labels']\n",
    "    print(name)\n",
    "    datad['rfe_features'] = rfe(features, labels, sample_size=2000, trials=4, select=10)\n",
    "    display(datad['rfe_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>-0.016525</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>-0.002023</td>\n",
       "      <td>0.002334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>-0.016525</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>-0.002023</td>\n",
       "      <td>0.002334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>-0.016525</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>-0.002023</td>\n",
       "      <td>0.002334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>-0.016525</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>-0.002023</td>\n",
       "      <td>0.002334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>-0.016525</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>-0.002023</td>\n",
       "      <td>0.002334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>-0.911024</td>\n",
       "      <td>-0.248434</td>\n",
       "      <td>0.247482</td>\n",
       "      <td>-0.008016</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>-0.004168</td>\n",
       "      <td>0.001292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>-1.038927</td>\n",
       "      <td>-0.214203</td>\n",
       "      <td>0.166401</td>\n",
       "      <td>-0.007982</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>-0.004251</td>\n",
       "      <td>0.003328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>-1.033032</td>\n",
       "      <td>-0.187945</td>\n",
       "      <td>0.110677</td>\n",
       "      <td>-0.003779</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>-0.003971</td>\n",
       "      <td>0.001680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>0.050001</td>\n",
       "      <td>-0.308196</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>-0.026729</td>\n",
       "      <td>0.049920</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>-0.012371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>0.994709</td>\n",
       "      <td>-0.133002</td>\n",
       "      <td>-0.037839</td>\n",
       "      <td>-0.022068</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>-0.003099</td>\n",
       "      <td>0.033386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6\n",
       "0      0.022396  0.010892  0.010192 -0.016525 -0.002421 -0.002023  0.002334\n",
       "1      0.022396  0.010892  0.010192 -0.016525 -0.002421 -0.002023  0.002334\n",
       "2      0.022396  0.010892  0.010192 -0.016525 -0.002421 -0.002023  0.002334\n",
       "3      0.022396  0.010892  0.010192 -0.016525 -0.002421 -0.002023  0.002334\n",
       "4      0.022396  0.010892  0.010192 -0.016525 -0.002421 -0.002023  0.002334\n",
       "...         ...       ...       ...       ...       ...       ...       ...\n",
       "26010 -0.911024 -0.248434  0.247482 -0.008016 -0.000371 -0.004168  0.001292\n",
       "26011 -1.038927 -0.214203  0.166401 -0.007982  0.000853 -0.004251  0.003328\n",
       "26012 -1.033032 -0.187945  0.110677 -0.003779  0.000768 -0.003971  0.001680\n",
       "26013  0.050001 -0.308196  0.011966 -0.026729  0.049920 -0.001053 -0.012371\n",
       "26014  0.994709 -0.133002 -0.037839 -0.022068 -0.007926 -0.003099  0.033386\n",
       "\n",
       "[26015 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name, datad in data_sets.items():\n",
    "    rfe_features = datad['rfe_features']\n",
    "    datad['rfe_pca_features'] = get_pca_features(rfe_features, pca_components=7)\n",
    "    display(datad['rfe_pca_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>adosc_min_max</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>rsi_5_min_max</th>\n",
       "      <th>wnr_9_min_max</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>1.775183</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4280.56</td>\n",
       "      <td>0.261074</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.140796</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>4462.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006597</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>0.411670</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.245191</td>\n",
       "      <td>0.874826</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.198468</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>0.336225</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.274284</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>4433.94</td>\n",
       "      <td>0.784977</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.044959</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>0.134002</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.279231</td>\n",
       "      <td>0.080840</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>4401.45</td>\n",
       "      <td>1.211797</td>\n",
       "      <td>-0.994112</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.004839</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.074237</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.533946</td>\n",
       "      <td>-0.823292</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Close    Volume  wnr_9_percentage  cci_24_percentage  \\\n",
       "0      4261.48  1.775183         -0.997032           0.001399   \n",
       "1      4261.48  0.000000         -0.997032           0.001399   \n",
       "2      4280.56  0.261074         -0.997032           0.001399   \n",
       "3      4261.48  0.012008         -0.997032           0.001399   \n",
       "4      4261.48  0.140796         -0.997032           0.001399   \n",
       "...        ...       ...               ...                ...   \n",
       "26010  4462.50  0.000000         -0.997032           0.000452   \n",
       "26011  4466.97  0.198468         -1.000000           0.002359   \n",
       "26012  4466.97  0.000000         -0.997032           0.000632   \n",
       "26013  4433.94  0.784977         -0.997032          -0.013633   \n",
       "26014  4401.45  1.211797         -0.994112           0.032995   \n",
       "\n",
       "       slowk_percentage  roc_10_percentage  mom_10_percentage  adosc_min_max  \\\n",
       "0              0.000144          -0.006591          -0.006634       0.000000   \n",
       "1              0.000144          -0.006591          -0.006634       0.000000   \n",
       "2              0.000144          -0.006591          -0.006634       0.000000   \n",
       "3              0.000144          -0.006591          -0.006634       0.000000   \n",
       "4              0.000144          -0.006591          -0.006634       0.000000   \n",
       "...                 ...                ...                ...            ...   \n",
       "26010          0.000144          -0.006597          -0.006640       0.411670   \n",
       "26011          0.000144          -0.007413          -0.007460       0.336225   \n",
       "26012          0.000144          -0.006591          -0.006634       0.274284   \n",
       "26013          0.000144          -0.044959          -0.045152       0.134002   \n",
       "26014          0.000144          -0.004839          -0.004873      -0.074237   \n",
       "\n",
       "       rsi_5_percentage  rsi_5_min_max  wnr_9_min_max  adosc_percentage  \n",
       "0                  -1.0       0.000000       0.000000          0.002114  \n",
       "1                  -1.0       0.000000       0.000000          0.002114  \n",
       "2                  -1.0       0.000000       0.000000          0.002114  \n",
       "3                  -1.0       0.000000       0.000000          0.002114  \n",
       "4                  -1.0       0.000000       0.000000          0.002114  \n",
       "...                 ...            ...            ...               ...  \n",
       "26010              -1.0       0.245191       0.874826          0.002114  \n",
       "26011              -1.0       0.297502       1.000000          0.002114  \n",
       "26012              -1.0       0.297502       1.000000          0.002114  \n",
       "26013              -1.0      -0.279231       0.080840          0.002114  \n",
       "26014              -1.0      -0.533946      -0.823292          0.002114  \n",
       "\n",
       "[26015 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26015\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[tensor(4113.5898), tensor(0.3663), tensor(-0...</td>\n",
       "      <td>[tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[tensor(4109.9102), tensor(2.2514), tensor(-1...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[tensor(4808.1001), tensor(0.), tensor(-1.000...</td>\n",
       "      <td>[tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[tensor(4014.9900), tensor(0.), tensor(-1.), ...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[tensor(4422.9902), tensor(0.4367), tensor(-0...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15584</th>\n",
       "      <td>[[tensor(4298.8198), tensor(1.3552), tensor(-0...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15585</th>\n",
       "      <td>[[tensor(4166.0200), tensor(0.0482), tensor(-0...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15586</th>\n",
       "      <td>[[tensor(4169.1802), tensor(0.), tensor(-0.997...</td>\n",
       "      <td>[tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15587</th>\n",
       "      <td>[[tensor(3884.9900), tensor(2.6510), tensor(-1...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15588</th>\n",
       "      <td>[[tensor(4291.3799), tensor(0.), tensor(-0.997...</td>\n",
       "      <td>[tensor(2.)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15589 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0             1\n",
       "0      [[tensor(4113.5898), tensor(0.3663), tensor(-0...  [tensor(1.)]\n",
       "1      [[tensor(4109.9102), tensor(2.2514), tensor(-1...  [tensor(0.)]\n",
       "2      [[tensor(4808.1001), tensor(0.), tensor(-1.000...  [tensor(1.)]\n",
       "3      [[tensor(4014.9900), tensor(0.), tensor(-1.), ...  [tensor(0.)]\n",
       "4      [[tensor(4422.9902), tensor(0.4367), tensor(-0...  [tensor(0.)]\n",
       "...                                                  ...           ...\n",
       "15584  [[tensor(4298.8198), tensor(1.3552), tensor(-0...  [tensor(0.)]\n",
       "15585  [[tensor(4166.0200), tensor(0.0482), tensor(-0...  [tensor(0.)]\n",
       "15586  [[tensor(4169.1802), tensor(0.), tensor(-0.997...  [tensor(1.)]\n",
       "15587  [[tensor(3884.9900), tensor(2.6510), tensor(-1...  [tensor(0.)]\n",
       "15588  [[tensor(4291.3799), tensor(0.), tensor(-0.997...  [tensor(2.)]\n",
       "\n",
       "[15589 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_is_copy': None, '_mgr': BlockManager\n",
      "Items: RangeIndex(start=0, stop=2, step=1)\n",
      "Axis 1: RangeIndex(start=0, stop=15589, step=1)\n",
      "ObjectBlock: slice(0, 2, 1), 2 x 15589, dtype: object, '_item_cache': {1: 0        [tensor(1.)]\n",
      "1        [tensor(0.)]\n",
      "2        [tensor(1.)]\n",
      "3        [tensor(0.)]\n",
      "4        [tensor(0.)]\n",
      "             ...     \n",
      "15584    [tensor(0.)]\n",
      "15585    [tensor(0.)]\n",
      "15586    [tensor(1.)]\n",
      "15587    [tensor(0.)]\n",
      "15588    [tensor(2.)]\n",
      "Name: 1, Length: 15589, dtype: object}, '_attrs': {}}\n",
      "0 5608\n",
      "1 7848\n",
      "2 2133\n",
      "\n",
      "0 2133\n",
      "1 2133\n",
      "2 2133\n",
      "\n",
      "6399 3903 6503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4.4230e+03,  4.3666e-01, -9.9767e-01, -2.4153e-03,  1.4409e-04,\n",
       "          -6.9807e-03, -7.0257e-03, -5.4199e-01, -1.0000e+00, -2.3937e-01,\n",
       "          -5.6744e-01,  2.1143e-03],\n",
       "         [ 4.4230e+03,  0.0000e+00, -9.9703e-01,  1.1331e-03,  1.4409e-04,\n",
       "          -6.5912e-03, -6.6341e-03, -5.2106e-01, -1.0000e+00, -2.3937e-01,\n",
       "          -5.6744e-01,  2.1143e-03],\n",
       "         [ 4.4175e+03,  4.3700e-01, -9.9693e-01,  1.1757e-03,  1.4409e-04,\n",
       "          -6.5116e-03, -6.5551e-03, -4.6565e-01, -1.0000e+00, -3.1132e-01,\n",
       "          -6.2237e-01,  2.1143e-03],\n",
       "         [ 4.4176e+03,  1.8284e+00, -9.9704e-01,  8.5315e-04,  1.4409e-04,\n",
       "          -6.5999e-03, -6.6430e-03, -1.9269e-01, -1.0000e+00, -3.0907e-01,\n",
       "          -6.1995e-01,  2.1143e-03],\n",
       "         [ 4.4040e+03,  1.8238e-01, -9.9634e-01,  2.0909e-03,  1.4409e-04,\n",
       "          -6.2017e-03, -6.2426e-03, -6.4026e-02, -1.0000e+00, -4.9346e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.4040e+03,  5.0068e-01, -9.9703e-01,  6.0560e-05,  1.4409e-04,\n",
       "          -7.0473e-03, -7.0970e-03, -6.0799e-03, -1.0000e+00, -4.9346e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.4040e+03,  1.7212e+00, -9.9703e-01, -7.1477e-04,  1.4409e-04,\n",
       "          -5.7438e-03, -5.7694e-03, -1.7857e-01, -1.0000e+00, -4.9346e-01,\n",
       "          -1.0000e+00,  2.1151e-03],\n",
       "         [ 4.4000e+03,  2.4238e+00, -9.9703e-01,  2.2921e-03,  1.4409e-04,\n",
       "          -6.9935e-03, -7.0438e-03, -5.0974e-01, -1.0000e+00, -5.6092e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3848e+03,  4.7832e-01, -9.9703e-01,  4.1214e-03,  1.4409e-04,\n",
       "          -7.3824e-03, -7.4353e-03, -5.9940e-01, -1.0000e+00, -7.3113e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3808e+03,  1.6289e+00, -9.9703e-01, -9.5882e-04,  1.4409e-04,\n",
       "          -6.3192e-03, -6.3608e-03, -7.6780e-01, -1.0000e+00, -7.6125e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.4700e+03,  1.4255e+00, -1.0000e+00, -1.3769e-02, -1.0875e-01,\n",
       "          -1.0667e-02, -1.0743e-02, -6.0488e-01, -1.0000e+00,  5.7574e-01,\n",
       "           1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3753e+03,  6.7334e-01, -9.9703e-01, -5.3213e-02,  1.4409e-04,\n",
       "          -9.3920e-03, -9.4498e-03, -5.6052e-01, -1.0000e+00, -2.1510e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3753e+03,  1.5108e-01, -9.9703e-01,  1.0506e-03,  1.4409e-04,\n",
       "          -6.7503e-03, -6.7956e-03, -4.9193e-01, -1.0000e+00, -2.1502e-01,\n",
       "          -9.9979e-01,  2.1143e-03],\n",
       "         [ 4.3753e+03,  0.0000e+00, -9.9703e-01,  6.8612e-04,  1.4409e-04,\n",
       "          -6.5886e-03, -6.6315e-03, -4.1966e-01, -1.0000e+00, -2.1502e-01,\n",
       "          -9.9979e-01,  2.1143e-03],\n",
       "         [ 4.3753e+03,  0.0000e+00, -9.9703e-01,  6.9245e-04,  1.4409e-04,\n",
       "          -7.0348e-03, -7.0830e-03, -3.5245e-01, -1.0000e+00, -2.1502e-01,\n",
       "          -9.9979e-01,  2.1143e-03],\n",
       "         [ 4.3780e+03,  5.8117e-01, -9.9712e-01,  3.4638e-04,  1.4409e-04,\n",
       "          -6.7206e-03, -6.7643e-03, -2.2717e-01, -1.0000e+00, -1.7442e-01,\n",
       "          -9.4336e-01,  2.1143e-03],\n",
       "         [ 4.3753e+03,  1.1227e+00, -9.9695e-01,  1.3704e-03,  1.4409e-04,\n",
       "          -6.4479e-03, -6.4901e-03, -2.8378e-01, -1.0000e+00, -2.0764e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3753e+03,  0.0000e+00, -9.9703e-01,  9.6559e-04,  1.4409e-04,\n",
       "          -6.7839e-03, -6.8290e-03, -2.8165e-01, -1.0000e+00, -2.0764e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3286e+03,  7.3088e+00, -9.9703e-01,  1.0774e-02,  1.4409e-04,\n",
       "          -4.8079e-03, -4.8525e-03, -1.0000e+00, -1.0000e+00, -6.2180e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3286e+03,  0.0000e+00, -9.9703e-01,  2.6206e-03,  1.4409e-04,\n",
       "          -6.6880e-03, -6.7327e-03, -1.0000e+00, -1.0000e+00, -6.2180e-01,\n",
       "          -1.0000e+00,  2.1143e-03]]), tensor([0.]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch, random\n",
    "def create_inout_sequences(input_data, input_labels, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = torch.FloatTensor(input_data[i:i+tw])\n",
    "        train_label = torch.FloatTensor([input_labels[i+tw - 1]])\n",
    "        if train_label == INVALID_LABEL or torch.isnan(train_seq).any():\n",
    "            continue\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return inout_seq\n",
    "\n",
    "def get_sequenced_train_val_test(features, datad, train_window=20):\n",
    "    data = datad['data']\n",
    "    labels = datad['labels']\n",
    "    all = create_inout_sequences(features.values, list(labels), train_window)\n",
    "    random.shuffle(all)\n",
    "#     display(all)\n",
    "    train = all[:-math.floor(len(features)/2.5)]\n",
    "    validate = all[-math.floor(len(features)/2.5):-math.floor(len(features)/4)]\n",
    "    test = all[-math.floor(len(features)/4):]\n",
    "\n",
    "    tdf = pd.DataFrame(train)\n",
    "    display(tdf)\n",
    "    display(tdf[1].values[0])\n",
    "    print(tdf.__dict__)\n",
    "\n",
    "    \n",
    "\n",
    "    label_rows = {}\n",
    "    for label in sorted(list(data.label.unique())):\n",
    "        print(label, len(tdf.loc[tdf[1] == torch.tensor([label])]))\n",
    "        label_rows[label] = tdf.loc[tdf[1] == torch.tensor([label])]\n",
    "\n",
    "    \n",
    "    min_len = min(len(v) for k, v in label_rows.items())\n",
    "    min_key = [label for label in sorted(list(data.label.unique())) if len(label_rows[label]) == min_len]\n",
    "    print()\n",
    "    for label in sorted(list(data.label.unique())):\n",
    "        to_remove = np.random.choice(label_rows[label].index,size=len(label_rows[label]) - min_len,replace=False)\n",
    "        tdf = tdf.drop(to_remove)\n",
    "        print(label, len(tdf.loc[tdf[1] == torch.tensor([label])]))\n",
    "    print()\n",
    "\n",
    "    train = [tuple(r) for r in tdf.to_numpy()]\n",
    "    print(len(train), len(validate), len(test))\n",
    "    return train, validate, test\n",
    "\n",
    "for name, datad in data_sets.items():\n",
    "    print(name)\n",
    "    data = datad['data']\n",
    "    rfe_features = datad['rfe_features']\n",
    "    labels = datad['labels']\n",
    "    \n",
    "    chosen_indicators = rfe_features #features_pca\n",
    "    chosen_dependent = [ 'Close', 'Volume'] #dependent_indicators #'High', 'Low',\n",
    "    combined = pd.concat([data[chosen_dependent], chosen_indicators], axis=1)\n",
    "    display(combined)\n",
    "    print(len(labels))\n",
    "    train, validate, test = get_sequenced_train_val_test(combined, datad, train_window=train_window)\n",
    "    datad['train'] = train\n",
    "    datad['validate'] = validate\n",
    "    datad['test'] = test\n",
    "    display(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT\n",
      "6399 train took: 7.579445123672485\n",
      "3903 val took: 4.463438034057617\n",
      "6503 test took: 7.544755220413208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.3537e-01, -8.8051e-01, -9.9767e-01, -2.4153e-03,  1.4409e-04,\n",
       "          -6.9807e-03, -7.0257e-03, -5.4199e-01, -1.0000e+00, -2.3937e-01,\n",
       "          -5.6744e-01,  2.1143e-03],\n",
       "         [ 3.3537e-01, -1.0000e+00, -9.9703e-01,  1.1331e-03,  1.4409e-04,\n",
       "          -6.5912e-03, -6.6341e-03, -5.2106e-01, -1.0000e+00, -2.3937e-01,\n",
       "          -5.6744e-01,  2.1143e-03],\n",
       "         [ 2.5752e-01, -8.8042e-01, -9.9693e-01,  1.1757e-03,  1.4409e-04,\n",
       "          -6.5116e-03, -6.5551e-03, -4.6565e-01, -1.0000e+00, -3.1132e-01,\n",
       "          -6.2237e-01,  2.1143e-03],\n",
       "         [ 2.5865e-01, -4.9966e-01, -9.9704e-01,  8.5315e-04,  1.4409e-04,\n",
       "          -6.5999e-03, -6.6430e-03, -1.9269e-01, -1.0000e+00, -3.0907e-01,\n",
       "          -6.1995e-01,  2.1143e-03],\n",
       "         [ 6.6589e-02, -9.5009e-01, -9.9634e-01,  2.0909e-03,  1.4409e-04,\n",
       "          -6.2017e-03, -6.2426e-03, -6.4026e-02, -1.0000e+00, -4.9346e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 6.6589e-02, -8.6299e-01, -9.9703e-01,  6.0560e-05,  1.4409e-04,\n",
       "          -7.0473e-03, -7.0970e-03, -6.0799e-03, -1.0000e+00, -4.9346e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 6.6589e-02, -5.2901e-01, -9.9703e-01, -7.1477e-04,  1.4409e-04,\n",
       "          -5.7438e-03, -5.7694e-03, -1.7857e-01, -1.0000e+00, -4.9346e-01,\n",
       "          -1.0000e+00,  2.1151e-03],\n",
       "         [ 9.9792e-03, -3.3676e-01, -9.9703e-01,  2.2921e-03,  1.4409e-04,\n",
       "          -6.9935e-03, -7.0438e-03, -5.0974e-01, -1.0000e+00, -5.6092e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [-2.0529e-01, -8.6911e-01, -9.9703e-01,  4.1214e-03,  1.4409e-04,\n",
       "          -7.3824e-03, -7.4353e-03, -5.9940e-01, -1.0000e+00, -7.3113e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [-2.6134e-01, -5.5427e-01, -9.9703e-01, -9.5882e-04,  1.4409e-04,\n",
       "          -6.3192e-03, -6.3608e-03, -7.6780e-01, -1.0000e+00, -7.6125e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 1.0000e+00, -6.0991e-01, -1.0000e+00, -1.3769e-02, -1.0875e-01,\n",
       "          -1.0667e-02, -1.0743e-02, -6.0488e-01, -1.0000e+00,  5.7574e-01,\n",
       "           1.0000e+00,  2.1143e-03],\n",
       "         [-3.3933e-01, -8.1575e-01, -9.9703e-01, -5.3213e-02,  1.4409e-04,\n",
       "          -9.3920e-03, -9.4498e-03, -5.6052e-01, -1.0000e+00, -2.1510e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [-3.3918e-01, -9.5866e-01, -9.9703e-01,  1.0506e-03,  1.4409e-04,\n",
       "          -6.7503e-03, -6.7956e-03, -4.9193e-01, -1.0000e+00, -2.1502e-01,\n",
       "          -9.9979e-01,  2.1143e-03],\n",
       "         [-3.3918e-01, -1.0000e+00, -9.9703e-01,  6.8612e-04,  1.4409e-04,\n",
       "          -6.5886e-03, -6.6315e-03, -4.1966e-01, -1.0000e+00, -2.1502e-01,\n",
       "          -9.9979e-01,  2.1143e-03],\n",
       "         [-3.3918e-01, -1.0000e+00, -9.9703e-01,  6.9245e-04,  1.4409e-04,\n",
       "          -7.0348e-03, -7.0830e-03, -3.5245e-01, -1.0000e+00, -2.1502e-01,\n",
       "          -9.9979e-01,  2.1143e-03],\n",
       "         [-3.0140e-01, -8.4097e-01, -9.9712e-01,  3.4638e-04,  1.4409e-04,\n",
       "          -6.7206e-03, -6.7643e-03, -2.2717e-01, -1.0000e+00, -1.7442e-01,\n",
       "          -9.4336e-01,  2.1143e-03],\n",
       "         [-3.3933e-01, -6.9279e-01, -9.9695e-01,  1.3704e-03,  1.4409e-04,\n",
       "          -6.4479e-03, -6.4901e-03, -2.8378e-01, -1.0000e+00, -2.0764e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [-3.3933e-01, -1.0000e+00, -9.9703e-01,  9.6559e-04,  1.4409e-04,\n",
       "          -6.7839e-03, -6.8290e-03, -2.8165e-01, -1.0000e+00, -2.0764e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [-1.0000e+00,  1.0000e+00, -9.9703e-01,  1.0774e-02,  1.4409e-04,\n",
       "          -4.8079e-03, -4.8525e-03, -1.0000e+00, -1.0000e+00, -6.2180e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [-1.0000e+00, -1.0000e+00, -9.9703e-01,  2.6206e-03,  1.4409e-04,\n",
       "          -6.6880e-03, -6.7327e-03, -1.0000e+00, -1.0000e+00, -6.2180e-01,\n",
       "          -1.0000e+00,  2.1143e-03]]), tensor([0.]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_sequence_columns(sequence_label, range=(-1, 1), indices=list(range(len(chosen_dependent)))):\n",
    "    sequence, label = sequence_label\n",
    "        \n",
    "    input_df = pd.DataFrame(sequence, dtype=np.float32)\n",
    "    for index in indices:\n",
    "        input_df[index] = minmax_scale(input_df[index], feature_range=range)\n",
    "    seq = torch.FloatTensor(input_df.values)\n",
    "    label = torch.FloatTensor([label])\n",
    "    return (seq, label)\n",
    "    \n",
    "\n",
    "# data[indicator + '_percentage'] = ( (data[indicator + '_percentage'] - data[indicator + '_percentage'].min()) / \n",
    "#                                    (data[indicator + '_percentage'].max() - data[indicator + '_percentage'].min()) ) * (1 - -1) + -1 \n",
    "\n",
    "        \n",
    "# df = pd.DataFrame(train)\n",
    "# # display(df[0].tolist()[0:2])\n",
    "# df_t = pd.concat([df.drop(columns=0), pd.DataFrame(df[0].tolist(), index=df.index).add_prefix(0)], \n",
    "#                axis=1)   #pd.DataFrame(df[0].tolist())\n",
    "# display(df_t)\n",
    "# # df[3] = ( (df[0] - min(df[0])) / \n",
    "# #                    (max(df[0]) - min(df[0])) ) * (1 - -1) + -1 \n",
    "# # display(df)\n",
    "\n",
    "n = 2\n",
    "pool = multiprocessing.Pool(multiprocessing.cpu_count() - n)                         # Create a multiprocessing Pool\n",
    "attempts = 3\n",
    "for name, datad in data_sets.items():\n",
    "    print(name)\n",
    "    train = datad['train']\n",
    "    validate = datad['validate']\n",
    "    test = datad['test']\n",
    "    \n",
    "    for a in range(attempts):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            train = pool.map(normalize_sequence_columns, train.copy())\n",
    "            print(len(train), 'train took:', time.time() - start)\n",
    "            break\n",
    "        except RuntimeError:\n",
    "            print('train err ', a)\n",
    "            continue\n",
    "\n",
    "    for a in range(attempts):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            validate = pool.map(normalize_sequence_columns, validate.copy())\n",
    "            print(len(validate), 'val took:', time.time() - start)\n",
    "            break\n",
    "        except RuntimeError:\n",
    "            print('val err ', a)\n",
    "            pass\n",
    "\n",
    "    for a in range(attempts):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            test = pool.map(normalize_sequence_columns, test.copy())\n",
    "            print(len(test), 'test took:', time.time() - start)\n",
    "            break\n",
    "        except RuntimeError:\n",
    "            print('test err ', a)\n",
    "            pass\n",
    "    display(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules to build RunBuilder and RunManager helper classes\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "import time\n",
    "import talib as ta\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import torch\n",
    "import time, copy, json\n",
    "\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "from IPython.display import clear_output\n",
    "import sklearn\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "INVALID_LABEL = 99\n",
    "\n",
    "def get_labels(row, data=None, train_window=None, target_percent=.01, stop_loss_percent=.005, mode='since3'):\n",
    "    if mode == 'since':\n",
    "        try:\n",
    "            target_index = data.loc[row[6]:][data.Close >= row[4] * (1 + target_percent)].iloc[0]._name\n",
    "        except IndexError:\n",
    "            target_index = data.index.max() + 5\n",
    "        try:\n",
    "            stop_loss_index = data.loc[row[6]:][data.Close <= row[4] * (1 - stop_loss_percent)].iloc[0]._name\n",
    "        except IndexError:\n",
    "            stop_loss_index = data.index.max() + 5\n",
    "        if target_index > stop_loss_index:\n",
    "            return 0\n",
    "        elif target_index < stop_loss_index:\n",
    "            return 1\n",
    "        return None\n",
    "    if mode == 'average':\n",
    "        mean = data.Close[row[6]:row[6]+10].mean()\n",
    "#         print(mean)\n",
    "        if mean - row[4] > 0: #row[4] * (1 + target_percent):\n",
    "            return 1\n",
    "#         elif mean <= row[4] * (1 - stop_loss_percent):\n",
    "#             return -1\n",
    "        else:\n",
    "            return 0\n",
    "        return None\n",
    "    if mode == 'next':\n",
    "        try:\n",
    "            next = data.Close.values[row[6]+1]\n",
    "        except:\n",
    "            next = 0\n",
    "#         print(next)\n",
    "        if next > row[4]: #row[4] * (1 + target_percent):\n",
    "            return 1\n",
    "#         elif mean <= row[4] * (1 - stop_loss_percent):\n",
    "#             return -1\n",
    "        else:\n",
    "            return 0\n",
    "        return None\n",
    "    if mode == 'since3':\n",
    "#         max_index = data.High[row[6]:row[6]+21].idxmax()\n",
    "#         min_index = data.Low[row[6]:row[6]+21].idxmin()\n",
    "        try:\n",
    "#             target_index = data.High[row[6]:row[6]+train_window+1].loc[data.High >= row[4] * (1 + target_percent)].index[0]\n",
    "            target_index = data.Close[row[6]:row[6]+train_window+1].loc[data.Close >= row[4] * (1 + target_percent)].index[0]\n",
    "        except IndexError:\n",
    "            target_index = data.index.max() + 5\n",
    "        try:\n",
    "#             stop_loss_index = data.Low[row[6]:row[6]+train_window+1].loc[data.Low <= row[4] * (1 - stop_loss_percent)].index[0]\n",
    "            stop_loss_index = data.Close[row[6]:row[6]+train_window+1].loc[data.Close <= row[4] * (1 - stop_loss_percent)].index[0]\n",
    "        except IndexError:\n",
    "            stop_loss_index = data.index.max() + 5\n",
    "        if target_index > stop_loss_index:\n",
    "            return 0\n",
    "        elif target_index < stop_loss_index:\n",
    "            return 2\n",
    "        return 1\n",
    "    \n",
    "def normalize_sequence_columns(sequence_label, range=(-1, 1), indices=None):\n",
    "    sequence, label = sequence_label\n",
    "\n",
    "    input_df = pd.DataFrame(sequence, dtype=np.float32)\n",
    "    for index in indices:\n",
    "        input_df[index] = minmax_scale(input_df[index], feature_range=range)\n",
    "    seq = torch.FloatTensor(input_df.values)\n",
    "    label = torch.FloatTensor([label])\n",
    "    return (seq, label)\n",
    "\n",
    "# Read in the hyper-parameters and return a Run namedtuple containing all the \n",
    "# combinations of hyper-parameters\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "\n",
    "        return runs\n",
    "\n",
    "class RunManager():\n",
    "    def __init__(self):\n",
    "        \n",
    "        def view(image):\n",
    "            return image.view(28*28)\n",
    "\n",
    "        compose_transforms = [\n",
    "            transforms.ToTensor(),\n",
    "            view\n",
    "        ]\n",
    "        \n",
    "        # Data sets to choose from\n",
    "#         self.data_sets = {\n",
    "# #             'sp500': {\n",
    "# #                 'train': data_sets['sp500']['train'],\n",
    "# #                 'validate': data_sets['sp500']['validate'],\n",
    "# #                 'test': data_sets['sp500']['test']\n",
    "# #             },\n",
    "#             'BTCUSDT': {\n",
    "#                 'train': data_sets['BTCUSDT']['train'],\n",
    "#                 'validate': data_sets['BTCUSDT']['validate'],\n",
    "#                 'test': data_sets['BTCUSDT']['test']\n",
    "#             }\n",
    "#         }\n",
    "        self.data_sets = {\n",
    "            'sp500': {\n",
    "                'data': pd.read_csv(f'/Users/gabeheim/documents/concatenated_price_data/sp500.csv', index_col=False).drop(['Adj Close'], axis=1)\n",
    "            },\n",
    "            'BTCUSDT': {\n",
    "                'data': pd.read_csv(f'/Users/gabeheim/documents/concatenated_price_data/BTCUSDT.csv', index_col=False)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.data_sets['BTCUSDT']['data'] = self.data_sets['BTCUSDT']['data'][:math.floor(\n",
    "            len(self.data_sets['BTCUSDT']['data'])/64)]#16)]\n",
    "        \n",
    "        self.global_labels = None\n",
    "        \n",
    "        # tracking every epoch count, loss, accuracy, time\n",
    "        self.epoch_count = 0\n",
    "        self.epoch_loss = {'train': 0, 'validate': 0}\n",
    "        self.epoch_num_correct = None\n",
    "        self.epoch_start_time = None\n",
    "\n",
    "        # tracking every run count, run data, hyper-params used, time\n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "        self.runs = pd.DataFrame()\n",
    "#         self.run_plot_statistics = {}\n",
    "        \n",
    "        # testing data\n",
    "        self.test_predictions = []\n",
    "        self.test_labels = []\n",
    "        self.test_correct_count = None\n",
    "\n",
    "        # record model, loader and TensorBoard \n",
    "        self.network = None\n",
    "        print(\"Run manager initialized\")\n",
    "        \n",
    "    def create_inout_sequences(self, input_data, input_labels, tw):\n",
    "        inout_seq = []\n",
    "        L = len(input_data)\n",
    "        for i in range(L-tw):\n",
    "            train_seq = torch.FloatTensor(input_data[i:i+tw])\n",
    "            train_label = torch.FloatTensor([input_labels[i+tw - 1]])\n",
    "            if train_label == INVALID_LABEL or torch.isnan(train_seq).any():\n",
    "                continue\n",
    "            inout_seq.append((train_seq ,train_label))\n",
    "        return inout_seq\n",
    "\n",
    "    def get_sequenced_train_val_test(self, features, datad, train_window=20):\n",
    "        data = datad['data']\n",
    "        labels = datad['labels']\n",
    "        all = self.create_inout_sequences(features.values, list(labels), train_window)\n",
    "        random.shuffle(all)\n",
    "    #     display(all)\n",
    "        train = all[:-math.floor(len(features)/2.5)]\n",
    "        validate = all[-math.floor(len(features)/2.5):-math.floor(len(features)/4)]\n",
    "        test = all[-math.floor(len(features)/4):]\n",
    "\n",
    "        tdf = pd.DataFrame(train)\n",
    "        display(tdf)\n",
    "        display(tdf[1].values[0])\n",
    "        print(tdf.__dict__)\n",
    "\n",
    "        all_labels = [x for x in data.label.unique() if str(x) != 'nan']\n",
    "\n",
    "        label_rows = {}\n",
    "        for label in all_labels:\n",
    "            print(label, len(tdf.loc[tdf[1] == torch.tensor([label])]))\n",
    "            label_rows[label] = tdf.loc[tdf[1] == torch.tensor([label])]\n",
    "\n",
    "\n",
    "        min_len = min(len(v) for k, v in label_rows.items())\n",
    "        min_key = [label for label in all_labels if len(label_rows[label]) == min_len]\n",
    "        print()\n",
    "        for label in all_labels:\n",
    "            to_remove = np.random.choice(label_rows[label].index,size=len(label_rows[label]) - min_len,replace=False)\n",
    "            tdf = tdf.drop(to_remove)\n",
    "            print(label, len(tdf.loc[tdf[1] == torch.tensor([label])]))\n",
    "        print()\n",
    "\n",
    "        train = [tuple(r) for r in tdf.to_numpy()]\n",
    "        print(len(train), len(validate), len(test))\n",
    "        return train, validate, test\n",
    "    \n",
    "    \n",
    "    def get_pca_features(self, features, pca_components=10):\n",
    "\n",
    "        pca = PCA(n_components=pca_components)\n",
    "        features_pca = pd.DataFrame(pca.fit_transform(features))\n",
    "        return features_pca\n",
    "\n",
    "    def rfe(self, features, labels, sample_size=2000, trials=4, select=10):\n",
    "        estimator = SVR(kernel=\"linear\")\n",
    "        selector = RFE(estimator, n_features_to_select=1)#, step=1)\n",
    "        sums = [0] * len(features.columns)\n",
    "        for trial in range(trials):\n",
    "            samples = features.sample(n=sample_size)\n",
    "            selector = selector.fit(samples, labels.loc[samples.index])\n",
    "            print(selector.ranking_)\n",
    "            sums = [sum(i) for i in zip(sums, selector.ranking_)]\n",
    "            print(sums)\n",
    "            print()\n",
    "        return features[[features.columns[i] for i in np.argsort(sums)[-select:] ]]\n",
    "    \n",
    "    def prepare_data(self, run):\n",
    "        \n",
    "        name = run.data_set\n",
    "        datad = self.data_sets[name]\n",
    "        data = datad['data']\n",
    "        train_window = run.train_window\n",
    "        n = 2\n",
    "        pool = multiprocessing.Pool(multiprocessing.cpu_count() - n) \n",
    "        pca_components = run.pca_components\n",
    "        label_mode = run.label_mode['mode']\n",
    "        target_percent = run.label_mode['target_percent']\n",
    "        stop_loss_percent = run.label_mode['stop_loss_percent']\n",
    "        rfe_select = run.rfe_select\n",
    "        chosen_dependent = run.chosen_dependent\n",
    "        \n",
    "        # labels\n",
    "        print('\\n Getting labels \\n')\n",
    "        \n",
    "        data['index'] = data.index\n",
    "        # too volatile class?\n",
    "        n = 0\n",
    "\n",
    "        start = time.time()\n",
    "        data['label'] = pool.map(partial(get_labels, data=data, train_window=train_window, mode=label_mode,\n",
    "                                       target_percent=target_percent, stop_loss_percent=stop_loss_percent), \n",
    "                                 [tuple(r) for r in data.to_numpy()] )  # process data_inputs iterable with pool\n",
    "        print(name, 'pool label took: ', time.time() - start)\n",
    "        self.global_labels = [x for x in sorted(self.data_sets[run.data_set]['data'].label.unique()) if str(x) != 'nan']\n",
    "        self.epoch_num_correct = {'train': {k: 0 for k in self.global_labels},\n",
    "                                  'validate': {k: 0 for k in self.global_labels}}\n",
    "        self.test_correct_count = {k: 0 for k in self.global_labels}\n",
    "            \n",
    "        # log class distribution\n",
    "        print('\\n Class distribution: \\n')\n",
    "        \n",
    "        for label in sorted(data.label.unique()):\n",
    "            print(name, label, len(data.loc[data.label == label]))\n",
    "                \n",
    "        # get indicators\n",
    "        print('\\n Getting indicators \\n')\n",
    "        \n",
    "        data['sma_10'] = ta.SMA(data.Close, timeperiod=10)\n",
    "        macd, macd_signal, macd_hist = ta.MACDFIX(data.Close, signalperiod=9)\n",
    "        data['macd'] = macd\n",
    "        data['macd_signal'] = macd_signal\n",
    "        data['macd_hist'] = macd_hist\n",
    "        data['cci_24'] = ta.CCI(data.High, data.Low, data.Close, timeperiod=24)\n",
    "        data['mom_10'] = ta.MOM(data.Close, timeperiod=10)\n",
    "        data['roc_10'] = ta.ROC(data.Close, timeperiod=10)\n",
    "        data['rsi_5'] = ta.RSI(data.Close, timeperiod=5)\n",
    "        data['wnr_9'] = ta.WILLR(data.High, data.Low, data.Close, timeperiod=9)\n",
    "        slowk, slowd = ta.STOCH(data.High, data.Low, data.Close, fastk_period=5, \n",
    "                                slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "        data['slowk'] = slowk\n",
    "        data['slowd'] = slowd\n",
    "        data['adosc'] = ta.ADOSC(data.High, data.Low, data.Close, data.Volume, fastperiod=3, slowperiod=10)\n",
    "        data = data[30:].reset_index()\n",
    "        data = data.drop(['level_0'], axis=1)\n",
    "        data['index'] = data.index\n",
    "            \n",
    "        # min max them\n",
    "        print('\\n Min-max scaling indicators \\n')\n",
    "        independent_indicators = ['macd', 'macd_signal', 'macd_hist', 'cci_24', 'mom_10', 'roc_10','rsi_5','wnr_9','slowk','slowd','adosc']\n",
    "        for indicator in independent_indicators:\n",
    "            name = indicator + '_min_max'\n",
    "            mean = data[indicator].mean()\n",
    "            std = data[indicator].std()\n",
    "            data[indicator].loc[data[indicator] > mean + 3 * std] = mean + 3 * std\n",
    "            data[indicator].loc[data[indicator] < mean - 3 * std] = mean - 3 * std\n",
    "            data[name] = (data[indicator] - mean) / std \n",
    "            data[name] = minmax_scale(data[indicator], feature_range=(-1,1))\n",
    "                \n",
    "        # percentage them \n",
    "        print('\\n Getting percentage fluctuation of indicators \\n')\n",
    "        percentage_indicators = ['Close', 'Volume', 'sma_10'] + independent_indicators\n",
    "        for indicator in percentage_indicators:\n",
    "            name = indicator + '_percentage'\n",
    "            data[name] = data[indicator].pct_change().replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "            mean = data[name].mean()\n",
    "            std = data[name].std()\n",
    "            data[name].loc[data[name] > mean + 3 * std] = mean + 3 * std\n",
    "            data[name].loc[data[name] < mean - 3 * std] = mean - 3 * std\n",
    "            data[name] = (data[name] - mean) / std \n",
    "            data[name] = minmax_scale(data[name], feature_range=(-1, 1))\n",
    "                \n",
    "        # isolate features / labels, fillnas\n",
    "        print('\\n Isolating features and labels, filling their nas \\n')\n",
    "        dependent_indicators = ['Open','High','Low','Close','Volume', 'sma_10']\n",
    "\n",
    "        datad['features'] = data.copy().drop(['index', 'Date', 'label'] + dependent_indicators + independent_indicators, axis=1\n",
    "                                   ).fillna(0).replace([np.inf, -np.inf], np.nan).ffill()\n",
    "\n",
    "        display(datad['features'])\n",
    "\n",
    "        datad['labels'] = data['label'].copy().replace([np.inf, -np.inf], np.nan).fillna(INVALID_LABEL)\n",
    "        features = datad['features']\n",
    "        labels = datad['labels']\n",
    "        display(datad['labels'])\n",
    "            \n",
    "        # rfe\n",
    "        if rfe_select > 0:\n",
    "            print('\\n Performing RFE \\n')\n",
    "            datad['rfe_features'] = self.rfe(features, labels, sample_size=2000, trials=4, select=rfe_select)\n",
    "            display(datad['rfe_features'])\n",
    "            rfe_features = datad['rfe_features']\n",
    "            \n",
    "        # pca\n",
    "        if pca_components:\n",
    "            print('\\n Performing PCA \\n')\n",
    "            datad['rfe_pca_features'] = get_pca_features(rfe_features, pca_components=7)\n",
    "            display(datad['rfe_pca_features'])\n",
    "            \n",
    "        # sequence and split to train val test\n",
    "        print('\\n Building sequences and splitting to train, val, and test sets \\n')\n",
    "        if rfe_select > 0:\n",
    "            chosen_indicators = rfe_features #features_pca\n",
    "            combined = pd.concat([data[chosen_dependent], chosen_indicators], axis=1)\n",
    "        else:\n",
    "            print(\"WORKS\")\n",
    "            display(data[chosen_dependent])\n",
    "            combined = data[chosen_dependent]\n",
    "        display(combined)\n",
    "        print(len(labels))\n",
    "        train, validate, test = self.get_sequenced_train_val_test(combined, datad, train_window=train_window)\n",
    "        datad['train'] = train\n",
    "        datad['validate'] = validate\n",
    "        datad['test'] = test\n",
    "        display(train[0])\n",
    "        \n",
    "        # normalize certain features within sequence\n",
    "        print('\\n Normalizing certain features within sequences \\n')\n",
    "        attempts = 3\n",
    "        train = datad['train']\n",
    "        validate = datad['validate']\n",
    "        test = datad['test']\n",
    "\n",
    "        for a in range(attempts):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                train = pool.map(partial(normalize_sequence_columns, indices=list(range(len(chosen_dependent)))), train.copy())\n",
    "                print(len(train), 'train took:', time.time() - start)\n",
    "                break\n",
    "            except RuntimeError:\n",
    "                print('train err ', a)\n",
    "                pass\n",
    "\n",
    "        for a in range(attempts):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                validate = pool.map(partial(normalize_sequence_columns, indices=list(range(len(chosen_dependent)))), validate.copy())\n",
    "                print(len(validate), 'val took:', time.time() - start)\n",
    "                break\n",
    "            except RuntimeError:\n",
    "                print('val err ', a)\n",
    "                pass\n",
    "\n",
    "        for a in range(attempts):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                test = pool.map(partial(normalize_sequence_columns, indices=list(range(len(chosen_dependent)))), test.copy())\n",
    "                print(len(test), 'test took:', time.time() - start)\n",
    "                break\n",
    "            except RuntimeError:\n",
    "                print('test err ', a)\n",
    "                pass\n",
    "        display(train[0])\n",
    "            \n",
    "        pool.close()\n",
    "        \n",
    "    # record the count, hyper-param, model, loader of each run\n",
    "    # record sample images and network graph to TensorBoard    \n",
    "    def begin_run(self, run, network):\n",
    "\n",
    "        self.run_start_time = time.time()\n",
    "\n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "#         self.run_plot_statistics[self.run_count] = {}\n",
    "    \n",
    "        self.network = network\n",
    "        \n",
    "        self.prepare_data(run)\n",
    "\n",
    "    # when run ends, close TensorBoard, zero epoch count\n",
    "    def end_run(self, net):\n",
    "        self.epoch_count = 0\n",
    "        \n",
    "        self.run_data[-1]['net'] = net\n",
    "        \n",
    "        test_accuracy = sum([v for k, v in self.test_correct_count.items()]) / (len(self.data_sets[self.run_params.data_set][phase]))\n",
    "        self.run_data[-1]['test_accuracy'] = test_accuracy\n",
    "        \n",
    "        cnf_matrix = sklearn.metrics.confusion_matrix(self.test_labels, self.test_predictions)\n",
    "        self.run_data[-1]['confusion_matrix'] = cnf_matrix\n",
    "        \n",
    "        self.runs.append(self.run_data)\n",
    "        display(pd.DataFrame(self.run_data))\n",
    "        \n",
    "        \n",
    "\n",
    "        # Plot normalized confusion matrix\n",
    "#         fig = plt.figure()\n",
    "#         fig.set_size_inches(7, 6, forward=True)\n",
    "        #fig.align_labels()\n",
    "\n",
    "        # fig.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
    "#         self.plot_confusion_matrix(cnf_matrix, classes=self.run_params.label_subset, normalize=True,\n",
    "#                               title='Normalized confusion matrix')\n",
    "        \n",
    "\n",
    "    # zero epoch count, loss, accuracy, \n",
    "    def begin_epoch(self, epoch_number):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "        self.epoch_count = epoch_number\n",
    "#         self.run_plot_statistics[self.run_count][self.epoch_count] = {\n",
    "#             'loss': {phase: [] for phase in self.loaders.keys()},\n",
    "#             'accuracy': {phase: [] for phase in self.loaders.keys()}\n",
    "#         }\n",
    "        self.epoch_loss = {'train': 0, 'validate': 0}\n",
    "        self.epoch_num_correct = {'train': {k: 0 for k in self.global_labels},\n",
    "                                  'validate': {k: 0 for k in self.global_labels}}\n",
    "\n",
    "    def end_epoch(self):\n",
    "        # calculate epoch duration and run duration(accumulate)\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "\n",
    "        # record epoch loss and accuracy\n",
    "        def get_all_correct(dict):\n",
    "            return sum([v for k, v in dict.items()])\n",
    "        loss = {phase: self.epoch_loss[phase] / (len(self.data_sets[self.run_params.data_set][phase])) for phase in ['train', 'validate']}\n",
    "        accuracy = {phase: get_all_correct(self.epoch_num_correct[phase]) / (len(self.data_sets[self.run_params.data_set][phase])) for phase in ['train', 'validate']}\n",
    "        \n",
    "        # Write into 'results' (OrderedDict) for all run related data\n",
    "        results = OrderedDict()\n",
    "        results['run'] = self.run_count\n",
    "        results['epoch'] = self.epoch_count\n",
    "        results['train loss'] = loss['train']\n",
    "        results['validate loss'] = loss['validate']\n",
    "        results['train accuracy'] = accuracy['train']\n",
    "        results['validate accuracy'] = accuracy['validate']\n",
    "        results['epoch duration'] = epoch_duration\n",
    "        results['run duration'] = run_duration\n",
    "\n",
    "        # Record hyper-params into 'results'\n",
    "        for parameter, value in self.run_params._asdict().items(): \n",
    "            if type(value) == dict:\n",
    "                for true_parameter, true_value in value.items():\n",
    "                    results[true_parameter] = true_value\n",
    "                continue\n",
    "                \n",
    "            results[parameter] = value\n",
    "            \n",
    "        self.run_data.append(results)\n",
    "\n",
    "#         print(results)\n",
    "\n",
    "    # accumulate loss of batch into entire epoch loss\n",
    "    def track_loss(self, phase, raw_loss):\n",
    "        loss = raw_loss.item() \n",
    "        self.epoch_loss[phase] += loss\n",
    "        \n",
    "#         self.run_plot_statistics[self.run_count][self.epoch_count]['loss'][phase].append(loss)\n",
    "\n",
    "    # accumulate number of corrects of batch into entire epoch num_correct\n",
    "    def track_num_correct(self, phase, outputs, labels):\n",
    "        try:\n",
    "            l_i = int(labels.item())\n",
    "        except:\n",
    "            l_i = 0\n",
    "        self.epoch_num_correct[phase][l_i] += self._get_num_correct(outputs, labels)\n",
    "#         try:\n",
    "#             self.run_plot_statistics[self.run_count][self.epoch_count]['accuracy'][phase].append(self.epoch_num_correct[phase] / \\\n",
    "#                                                         len(self.run_plot_statistics[self.run_count][self.epoch_count]['accuracy']))\n",
    "#         except: # if first image\n",
    "#             self.run_plot_statistics[self.run_count][self.epoch_count]['accuracy'][phase].append(self.epoch_num_correct[phase])\n",
    "        \n",
    "    def track_test_predictions(self, prediction, label):\n",
    "        self.test_predictions.append(prediction)\n",
    "        self.test_labels.append(label)\n",
    "        try:\n",
    "            l_i = int(labels.item())\n",
    "        except:\n",
    "            l_i = 0\n",
    "        self.test_correct_count[l_i] += 1 if prediction == label else 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_num_correct(self, output, label):\n",
    "        try:\n",
    "            l_i = int(labels.item())\n",
    "        except:\n",
    "            l_i = 0\n",
    "        return 1 if int(torch.argmax(output)) == l_i else 0\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm, classes, variables, normalize=False, cmap=plt.cm.Blues):\n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \"\"\"\n",
    "                \n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            print(f\"Normalized Confusion Matrix (Run #{len(self.runs) - 1})\")\n",
    "        else:\n",
    "            print('Confusion matrix, without normalization')        \n",
    "\n",
    "        ax = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(f'{df_row.data_set}: Confusion matrix')\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=90)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        \n",
    "        x_label = \"Predicted label\"\n",
    "        for variable in variables:\n",
    "            x_label += f\"\\n{variable} = {run_data[variable].values[0]}\"\n",
    "        ax.set_xlabel(x_label)\n",
    "        plt.show()\n",
    "    \n",
    "    # save end results of all runs into json for further analysis\n",
    "    def results(self, fileName):\n",
    "        return\n",
    "        \n",
    "#         result_df = pd.DataFrame.from_dict(\n",
    "#                 self.run_data, \n",
    "#                 orient = 'columns',\n",
    "#         )\n",
    "#         display(result_df)\n",
    "        \n",
    "\n",
    "#         with open(f'results/{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "#             json.dump(self.run_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run manager initialized\n"
     ]
    }
   ],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, weight_init={'function':torch.nn.init.xavier_uniform}, hidden_neurons=128, output_neurons=None, \n",
    "                 hidden_activation=functional.relu, output_activation=torch.nn.Softmax(dim=2), input_size=None,\n",
    "                dropout_p=None):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        # hyper parameters\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_neurons)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        self.hidden_cell = (torch.zeros(1,1,hidden_neurons),\n",
    "                            torch.zeros(1,1,hidden_neurons))\n",
    "        \n",
    "#         self.cnn = [#nn.Sequential(\n",
    "#             nn.Conv1d(input_size, 128, 1),\n",
    "#             nn.Conv1d(in_channels=128, out_channels=256, kernel_size=8, stride=1),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(256, 128, kernel_size=5, stride=1),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(128, 128, kernel_size=3, stride=1),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.ReLU()#,\n",
    "# #             nn.AvgPool1d(kernel_size=7, stride=1,padding=0) #(Lin+2*p-k)/s+1\n",
    "#         ]#)\n",
    "        \n",
    "        self.out = nn.Linear(in_features=hidden_neurons, out_features=output_neurons) # hidden layer to output\n",
    "        if weight_init:\n",
    "            weight_init['function'](self.out.weight)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def forward(self, x, count):\n",
    "#         print('raw input size', x.size())\n",
    "#         print('lstm altered size', x.view(len(x) ,1, self.input_size).size())\n",
    "        lstm_input = x.view(len(x) ,1, self.input_size)\n",
    "        lstm_out, self.hidden_cell = self.lstm(lstm_input, self.hidden_cell)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "#         if count % 50 == 0:\n",
    "#             print('lstm out:', lstm_out.size())\n",
    "#             print('lstm out out:', self.out(lstm_out).size())#, self.out(lstm_out))\n",
    "#             print('lstm activation:', self.output_activation(self.out(lstm_out)).size(), self.output_activation(self.out(lstm_out))[-1])\n",
    "#         predictions = self.output_activation(self.out(lstm_out))\n",
    "#         return predictions[-1]\n",
    "\n",
    "# #         print('cnn altered size', x.reshape(1, self.input_size, len(x)).size())\n",
    "#         cnn_out = x.reshape(1, self.input_size, len(x))\n",
    "#         for i, layer in enumerate(self.cnn):\n",
    "# #             print(i, '\\nx: ', cnn_out.size())\n",
    "#             cnn_out = layer(cnn_out)\n",
    "# #             print('result: ', cnn_out.size(), '\\n')\n",
    "#         cnn_out = cnn_out.reshape(-1, 1, 128)\n",
    "    \n",
    "    \n",
    "#         print(lstm_out.size(), cnn_out.size())\n",
    "        features = lstm_out#torch.cat((lstm_out, cnn_out))\n",
    "#         print(features.size())\n",
    "        result = self.out(features)\n",
    "#         print('res', result.size())\n",
    "        result = self.output_activation(result)\n",
    "#         print('res act', result.size())\n",
    "#         print(result[-1])\n",
    "        return result[-1]\n",
    "        \n",
    "\n",
    "def sum_squared_error(out, label):\n",
    "#     print(label, out)\n",
    "#     print(label - out)\n",
    "#     print()\n",
    "    return ((label - out) ** 2).sum()\n",
    "\n",
    "def mean_squared_error(outputs, labels):\n",
    "    return sum_squared_error(outputs, labels) / len(outputs)\n",
    "\n",
    "def cross_entropy(outputs, labels):\n",
    "    return -1 * (torch.log(outputs) * labels + (torch.log(1 - outputs)) * (1 - labels)).sum()\n",
    "\n",
    "def dummy_activation(x):\n",
    "    return x\n",
    "\n",
    "# put all hyper params into a OrderedDict, easily expandable\n",
    "params = OrderedDict(\n",
    "    data_set = ['sp500'],#'BTCUSDT'], #'sp500', \n",
    "    hidden_neurons = [50],#, 100, 5], #1\n",
    "    \n",
    "    batch_size = [1],\n",
    "    \n",
    "    weight_init = [{\n",
    "        'function': torch.nn.init.xavier_uniform,\n",
    "        'name': \"Xavier Uniform\"\n",
    "    }],\n",
    "    \n",
    "    hidden_activation = [torch.relu],#, torch.tanh, torch.relu],\n",
    "    loss_output = [\n",
    "        {\n",
    "        'criterion': sum_squared_error,\n",
    "        'output_activation': torch.nn.Softmax(dim=2)\n",
    "    },  \n",
    "\n",
    "    ],\n",
    "    \n",
    "    learning_rate = [0.01],#, .001],\n",
    "    momentum = [0.1],#, 0],\n",
    "    optimizer = [optim.SGD],#, optim.Adam], #optim.Adam(network.parameters(), lr=run.lr)\n",
    "    validation_split = [0.1],\n",
    "    \n",
    "    \n",
    "    train_window = [10],\n",
    "    pca_components = [None],#10\n",
    "    label_mode = [\n",
    "#         {\n",
    "#         'mode': 'since3',\n",
    "#         'label_count': 3,\n",
    "#         'target_percent': .03, \n",
    "#         'stop_loss_percent': .03\n",
    "#     },\n",
    "#         {\n",
    "#         'mode': 'average',\n",
    "#         'label_count': 2,\n",
    "#         'target_percent': None, \n",
    "#         'stop_loss_percent': None\n",
    "#     },\n",
    "        {\n",
    "        'mode': 'next',\n",
    "        'label_count': 2,\n",
    "        'target_percent': None, \n",
    "        'stop_loss_percent': None\n",
    "    }\n",
    "    ],\n",
    "    rfe_select = [3],\n",
    "    chosen_dependent = [ ['Close', 'Volume', 'sma_10'] ],\n",
    "    dropout_p = [.4]\n",
    ")\n",
    "\n",
    "\n",
    "def negative_one(x):\n",
    "    return -1\n",
    "\n",
    "def zero(x):\n",
    "    return 0\n",
    "\n",
    "def one(x):\n",
    "    return 1\n",
    "\n",
    "def argmax(x):\n",
    "    return x[0][0][torch.argmax(x)].item()\n",
    "\n",
    "error_encoding_map = {\n",
    "    torch.sigmoid: {\n",
    "        'cold': zero,\n",
    "        'hot': one\n",
    "    },\n",
    "    torch.relu: {\n",
    "        'cold': zero,\n",
    "        'hot': one\n",
    "    },\n",
    "    torch.tanh: {\n",
    "        'cold': negative_one,\n",
    "        'hot': one\n",
    "    },\n",
    "    torch.nn.Softmax(dim=2): {\n",
    "        'cold': zero,\n",
    "        'hot': one\n",
    "    },\n",
    "    dummy_activation: {\n",
    "        'cold': zero,\n",
    "        'hot': argmax\n",
    "    }\n",
    "}\n",
    "\n",
    "m = RunManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.data_sets['BTCUSDT']['test'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(data_set='sp500', hidden_neurons=50, batch_size=1, weight_init={'function': <function _make_deprecate.<locals>.deprecated_init at 0x13d2d9268>, 'name': 'Xavier Uniform'}, hidden_activation=<built-in method relu of type object at 0x12deeac20>, loss_output={'criterion': <function sum_squared_error at 0x143aa7b70>, 'output_activation': Softmax(dim=2)}, learning_rate=0.01, momentum=0.1, optimizer=<class 'torch.optim.sgd.SGD'>, validation_split=0.1, train_window=10, pca_components=None, label_mode={'mode': 'next', 'label_count': 2, 'target_percent': None, 'stop_loss_percent': None}, rfe_select=3, chosen_dependent=['Close', 'Volume', 'sma_10'], dropout_p=0.4)\n",
      "\n",
      " Getting labels \n",
      "\n",
      "sp500 pool label took:  0.17734909057617188\n",
      "\n",
      " Class distribution: \n",
      "\n",
      "sp500 0 8371\n",
      "sp500 1 9473\n",
      "\n",
      " Getting indicators \n",
      "\n",
      "\n",
      " Min-max scaling indicators \n",
      "\n",
      "\n",
      " Getting percentage fluctuation of indicators \n",
      "\n",
      "\n",
      " Isolating features and labels, filling their nas \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macd_min_max</th>\n",
       "      <th>macd_signal_min_max</th>\n",
       "      <th>macd_hist_min_max</th>\n",
       "      <th>cci_24_min_max</th>\n",
       "      <th>mom_10_min_max</th>\n",
       "      <th>roc_10_min_max</th>\n",
       "      <th>rsi_5_min_max</th>\n",
       "      <th>wnr_9_min_max</th>\n",
       "      <th>slowk_min_max</th>\n",
       "      <th>slowd_min_max</th>\n",
       "      <th>...</th>\n",
       "      <th>macd_signal_percentage</th>\n",
       "      <th>macd_hist_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>slowd_percentage</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008518</td>\n",
       "      <td>-0.020072</td>\n",
       "      <td>-0.031943</td>\n",
       "      <td>-0.310868</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.818177</td>\n",
       "      <td>-0.535344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>-0.002437</td>\n",
       "      <td>-0.002102</td>\n",
       "      <td>-0.002089</td>\n",
       "      <td>-0.293429</td>\n",
       "      <td>-0.990266</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.003477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.080950</td>\n",
       "      <td>-0.020773</td>\n",
       "      <td>-0.077547</td>\n",
       "      <td>-0.460013</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.737368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>-0.002509</td>\n",
       "      <td>-0.091192</td>\n",
       "      <td>-0.092265</td>\n",
       "      <td>-0.472543</td>\n",
       "      <td>-0.990266</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.003477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092886</td>\n",
       "      <td>-0.020974</td>\n",
       "      <td>-0.090034</td>\n",
       "      <td>0.088463</td>\n",
       "      <td>-0.030303</td>\n",
       "      <td>-0.573333</td>\n",
       "      <td>-0.797170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>-0.003520</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.536391</td>\n",
       "      <td>-0.994985</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.003477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.037349</td>\n",
       "      <td>-0.039248</td>\n",
       "      <td>-0.002154</td>\n",
       "      <td>0.138872</td>\n",
       "      <td>-0.021074</td>\n",
       "      <td>-0.096279</td>\n",
       "      <td>0.206224</td>\n",
       "      <td>0.448278</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>-0.493333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>-0.002418</td>\n",
       "      <td>-0.000511</td>\n",
       "      <td>-0.000529</td>\n",
       "      <td>-0.202315</td>\n",
       "      <td>-0.994787</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.003477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.037364</td>\n",
       "      <td>-0.039329</td>\n",
       "      <td>-0.001963</td>\n",
       "      <td>0.087020</td>\n",
       "      <td>-0.021675</td>\n",
       "      <td>-0.134660</td>\n",
       "      <td>0.098738</td>\n",
       "      <td>0.241375</td>\n",
       "      <td>0.664759</td>\n",
       "      <td>0.061586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001412</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>-0.002452</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>-0.368696</td>\n",
       "      <td>-0.986615</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.003477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17809</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.481441</td>\n",
       "      <td>0.134518</td>\n",
       "      <td>0.251186</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.122966</td>\n",
       "      <td>0.073593</td>\n",
       "      <td>-0.308853</td>\n",
       "      <td>-0.151227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>-0.000952</td>\n",
       "      <td>-0.002436</td>\n",
       "      <td>-0.007662</td>\n",
       "      <td>-0.007800</td>\n",
       "      <td>-0.163228</td>\n",
       "      <td>-0.992592</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.003401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17810</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619184</td>\n",
       "      <td>0.216770</td>\n",
       "      <td>0.880144</td>\n",
       "      <td>0.243135</td>\n",
       "      <td>0.429311</td>\n",
       "      <td>0.888464</td>\n",
       "      <td>0.011660</td>\n",
       "      <td>-0.197151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>-0.002411</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.027830</td>\n",
       "      <td>-0.063471</td>\n",
       "      <td>-0.998828</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.005640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17811</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.603678</td>\n",
       "      <td>0.201933</td>\n",
       "      <td>0.550690</td>\n",
       "      <td>0.138736</td>\n",
       "      <td>0.369573</td>\n",
       "      <td>0.742863</td>\n",
       "      <td>0.484745</td>\n",
       "      <td>0.062517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>-0.002440</td>\n",
       "      <td>-0.006760</td>\n",
       "      <td>-0.006865</td>\n",
       "      <td>-0.328884</td>\n",
       "      <td>-0.977558</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.002215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17812</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.575053</td>\n",
       "      <td>0.200863</td>\n",
       "      <td>0.994940</td>\n",
       "      <td>0.279805</td>\n",
       "      <td>0.416067</td>\n",
       "      <td>0.881358</td>\n",
       "      <td>0.824167</td>\n",
       "      <td>0.440191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>-0.002437</td>\n",
       "      <td>0.007802</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>-0.264660</td>\n",
       "      <td>-0.995509</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.003197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17813</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.390986</td>\n",
       "      <td>0.147655</td>\n",
       "      <td>0.345244</td>\n",
       "      <td>0.074571</td>\n",
       "      <td>0.203348</td>\n",
       "      <td>0.548518</td>\n",
       "      <td>0.705707</td>\n",
       "      <td>0.671540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>-0.000985</td>\n",
       "      <td>-0.002449</td>\n",
       "      <td>-0.010248</td>\n",
       "      <td>-0.010397</td>\n",
       "      <td>-0.420832</td>\n",
       "      <td>-0.962956</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.003013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17814 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       macd_min_max  macd_signal_min_max  macd_hist_min_max  cci_24_min_max  \\\n",
       "0          0.000000             0.000000           0.000000        0.008518   \n",
       "1          0.000000             0.000000           0.000000       -0.080950   \n",
       "2          0.000000             0.000000           0.000000        0.092886   \n",
       "3         -0.037349            -0.039248          -0.002154        0.138872   \n",
       "4         -0.037364            -0.039329          -0.001963        0.087020   \n",
       "...             ...                  ...                ...             ...   \n",
       "17809      1.000000             1.000000           0.481441        0.134518   \n",
       "17810      1.000000             1.000000           0.619184        0.216770   \n",
       "17811      1.000000             1.000000           0.603678        0.201933   \n",
       "17812      1.000000             1.000000           0.575053        0.200863   \n",
       "17813      1.000000             1.000000           0.390986        0.147655   \n",
       "\n",
       "       mom_10_min_max  roc_10_min_max  rsi_5_min_max  wnr_9_min_max  \\\n",
       "0           -0.020072       -0.031943      -0.310868      -1.000000   \n",
       "1           -0.020773       -0.077547      -0.460013      -1.000000   \n",
       "2           -0.020974       -0.090034       0.088463      -0.030303   \n",
       "3           -0.021074       -0.096279       0.206224       0.448278   \n",
       "4           -0.021675       -0.134660       0.098738       0.241375   \n",
       "...               ...             ...            ...            ...   \n",
       "17809        0.251186        0.046296       0.122966       0.073593   \n",
       "17810        0.880144        0.243135       0.429311       0.888464   \n",
       "17811        0.550690        0.138736       0.369573       0.742863   \n",
       "17812        0.994940        0.279805       0.416067       0.881358   \n",
       "17813        0.345244        0.074571       0.203348       0.548518   \n",
       "\n",
       "       slowk_min_max  slowd_min_max  ...  macd_signal_percentage  \\\n",
       "0          -0.818177      -0.535344  ...                0.000643   \n",
       "1          -1.000000      -0.737368  ...                0.000643   \n",
       "2          -0.573333      -0.797170  ...                0.000643   \n",
       "3           0.093333      -0.493333  ...                0.000643   \n",
       "4           0.664759       0.061586  ...               -0.001412   \n",
       "...              ...            ...  ...                     ...   \n",
       "17809      -0.308853      -0.151227  ...                0.000643   \n",
       "17810       0.011660      -0.197151  ...                0.000643   \n",
       "17811       0.484745       0.062517  ...                0.000643   \n",
       "17812       0.824167       0.440191  ...                0.000643   \n",
       "17813       0.705707       0.671540  ...                0.000643   \n",
       "\n",
       "       macd_hist_percentage  cci_24_percentage  mom_10_percentage  \\\n",
       "0                  0.000308          -0.002437          -0.002102   \n",
       "1                  0.000308          -0.002509          -0.091192   \n",
       "2                  0.000308          -0.003520           0.002140   \n",
       "3                  0.000308          -0.002418          -0.000511   \n",
       "4                 -0.000363          -0.002452           0.006383   \n",
       "...                     ...                ...                ...   \n",
       "17809             -0.000952          -0.002436          -0.007662   \n",
       "17810              0.001464          -0.002411           0.027397   \n",
       "17811              0.000207          -0.002440          -0.006760   \n",
       "17812              0.000117          -0.002437           0.007802   \n",
       "17813             -0.000985          -0.002449          -0.010248   \n",
       "\n",
       "       roc_10_percentage  rsi_5_percentage  wnr_9_percentage  \\\n",
       "0              -0.002089         -0.293429         -0.990266   \n",
       "1              -0.092265         -0.472543         -0.990266   \n",
       "2               0.002026          0.536391         -0.994985   \n",
       "3              -0.000529         -0.202315         -0.994787   \n",
       "4               0.006462         -0.368696         -0.986615   \n",
       "...                  ...               ...               ...   \n",
       "17809          -0.007800         -0.163228         -0.992592   \n",
       "17810           0.027830         -0.063471         -0.998828   \n",
       "17811          -0.006865         -0.328884         -0.977558   \n",
       "17812           0.008167         -0.264660         -0.995509   \n",
       "17813          -0.010397         -0.420832         -0.962956   \n",
       "\n",
       "       slowk_percentage  slowd_percentage  adosc_percentage  \n",
       "0              0.001953              -1.0         -0.003477  \n",
       "1              0.001953              -1.0         -0.003477  \n",
       "2             -1.000000              -1.0         -0.003477  \n",
       "3              0.001953              -1.0         -0.003477  \n",
       "4              0.001953              -1.0         -0.003477  \n",
       "...                 ...               ...               ...  \n",
       "17809          0.001953              -1.0         -0.003401  \n",
       "17810          0.001953              -1.0          0.005640  \n",
       "17811          0.001953              -1.0         -0.002215  \n",
       "17812          0.001953              -1.0         -0.003197  \n",
       "17813          0.001953              -1.0         -0.003013  \n",
       "\n",
       "[17814 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        1\n",
       "2        1\n",
       "3        0\n",
       "4        1\n",
       "        ..\n",
       "17809    1\n",
       "17810    0\n",
       "17811    1\n",
       "17812    0\n",
       "17813    0\n",
       "Name: label, Length: 17814, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performing RFE \n",
      "\n",
      "[19 18 24 21 14  8 17 23 16 15 20 12 25  7  9 22  5  1 11 10 13  3  4  6\n",
      "  2]\n",
      "[19, 18, 24, 21, 14, 8, 17, 23, 16, 15, 20, 12, 25, 7, 9, 22, 5, 1, 11, 10, 13, 3, 4, 6, 2]\n",
      "\n",
      "[15 14  5 10  1 22  3 12 17 18 16  9 21 19  2  4 25 24 23 20  8  6 13  7\n",
      " 11]\n",
      "[34, 32, 29, 31, 15, 30, 20, 35, 33, 33, 36, 21, 46, 26, 11, 26, 30, 25, 34, 30, 21, 9, 17, 13, 13]\n",
      "\n",
      "[25 11  6  7  5 15 13 18 23 14 22 20 10 16  3 12  9 21 24 17 19  4  8  1\n",
      "  2]\n",
      "[59, 43, 35, 38, 20, 45, 33, 53, 56, 47, 58, 41, 56, 42, 14, 38, 39, 46, 58, 47, 40, 13, 25, 14, 15]\n",
      "\n",
      "[ 6  5  7 19 11  9 14 21 22 18 25 15 16  8  2 24  1 10 17 12 13 23  3 20\n",
      "  4]\n",
      "[65, 48, 42, 57, 31, 54, 47, 74, 78, 65, 83, 56, 72, 50, 16, 62, 40, 56, 75, 59, 53, 36, 28, 34, 19]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>slowk_min_max</th>\n",
       "      <th>adosc_min_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.002102</td>\n",
       "      <td>-0.818177</td>\n",
       "      <td>-0.089903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.091192</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.089903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002140</td>\n",
       "      <td>-0.573333</td>\n",
       "      <td>-0.089903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000511</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>-0.089903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.664759</td>\n",
       "      <td>-0.089903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17809</th>\n",
       "      <td>-0.007662</td>\n",
       "      <td>-0.308853</td>\n",
       "      <td>-0.003972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17810</th>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.011660</td>\n",
       "      <td>0.330232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17811</th>\n",
       "      <td>-0.006760</td>\n",
       "      <td>0.484745</td>\n",
       "      <td>0.556409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17812</th>\n",
       "      <td>0.007802</td>\n",
       "      <td>0.824167</td>\n",
       "      <td>0.633586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17813</th>\n",
       "      <td>-0.010248</td>\n",
       "      <td>0.705707</td>\n",
       "      <td>0.777020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17814 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mom_10_percentage  slowk_min_max  adosc_min_max\n",
       "0              -0.002102      -0.818177      -0.089903\n",
       "1              -0.091192      -1.000000      -0.089903\n",
       "2               0.002140      -0.573333      -0.089903\n",
       "3              -0.000511       0.093333      -0.089903\n",
       "4               0.006383       0.664759      -0.089903\n",
       "...                  ...            ...            ...\n",
       "17809          -0.007662      -0.308853      -0.003972\n",
       "17810           0.027397       0.011660       0.330232\n",
       "17811          -0.006760       0.484745       0.556409\n",
       "17812           0.007802       0.824167       0.633586\n",
       "17813          -0.010248       0.705707       0.777020\n",
       "\n",
       "[17814 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Building sequences and splitting to train, val, and test sets \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>sma_10</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>slowk_min_max</th>\n",
       "      <th>adosc_min_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.059999</td>\n",
       "      <td>1730000</td>\n",
       "      <td>17.197000</td>\n",
       "      <td>-0.002102</td>\n",
       "      <td>-0.818177</td>\n",
       "      <td>-0.089903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.990000</td>\n",
       "      <td>1920000</td>\n",
       "      <td>17.191000</td>\n",
       "      <td>-0.091192</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.089903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.150000</td>\n",
       "      <td>1940000</td>\n",
       "      <td>17.183000</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>-0.573333</td>\n",
       "      <td>-0.089903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.200001</td>\n",
       "      <td>1420000</td>\n",
       "      <td>17.174000</td>\n",
       "      <td>-0.000511</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>-0.089903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.170000</td>\n",
       "      <td>1260000</td>\n",
       "      <td>17.159000</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.664759</td>\n",
       "      <td>-0.089903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17809</th>\n",
       "      <td>3577.590088</td>\n",
       "      <td>5036290000</td>\n",
       "      <td>3576.158008</td>\n",
       "      <td>-0.007662</td>\n",
       "      <td>-0.308853</td>\n",
       "      <td>-0.003972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17810</th>\n",
       "      <td>3635.409912</td>\n",
       "      <td>6267570000</td>\n",
       "      <td>3585.145996</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.011660</td>\n",
       "      <td>0.330232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17811</th>\n",
       "      <td>3629.649902</td>\n",
       "      <td>4902560000</td>\n",
       "      <td>3590.844995</td>\n",
       "      <td>-0.006760</td>\n",
       "      <td>0.484745</td>\n",
       "      <td>0.556409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17812</th>\n",
       "      <td>3638.350098</td>\n",
       "      <td>2778450000</td>\n",
       "      <td>3600.979004</td>\n",
       "      <td>0.007802</td>\n",
       "      <td>0.824167</td>\n",
       "      <td>0.633586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17813</th>\n",
       "      <td>3621.629883</td>\n",
       "      <td>6291400000</td>\n",
       "      <td>3604.627002</td>\n",
       "      <td>-0.010248</td>\n",
       "      <td>0.705707</td>\n",
       "      <td>0.777020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17814 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Close      Volume       sma_10  mom_10_percentage  slowk_min_max  \\\n",
       "0        17.059999     1730000    17.197000          -0.002102      -0.818177   \n",
       "1        16.990000     1920000    17.191000          -0.091192      -1.000000   \n",
       "2        17.150000     1940000    17.183000           0.002140      -0.573333   \n",
       "3        17.200001     1420000    17.174000          -0.000511       0.093333   \n",
       "4        17.170000     1260000    17.159000           0.006383       0.664759   \n",
       "...            ...         ...          ...                ...            ...   \n",
       "17809  3577.590088  5036290000  3576.158008          -0.007662      -0.308853   \n",
       "17810  3635.409912  6267570000  3585.145996           0.027397       0.011660   \n",
       "17811  3629.649902  4902560000  3590.844995          -0.006760       0.484745   \n",
       "17812  3638.350098  2778450000  3600.979004           0.007802       0.824167   \n",
       "17813  3621.629883  6291400000  3604.627002          -0.010248       0.705707   \n",
       "\n",
       "       adosc_min_max  \n",
       "0          -0.089903  \n",
       "1          -0.089903  \n",
       "2          -0.089903  \n",
       "3          -0.089903  \n",
       "4          -0.089903  \n",
       "...              ...  \n",
       "17809      -0.003972  \n",
       "17810       0.330232  \n",
       "17811       0.556409  \n",
       "17812       0.633586  \n",
       "17813       0.777020  \n",
       "\n",
       "[17814 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17814\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[tensor(447.1000), tensor(2.2217e+08), tensor...</td>\n",
       "      <td>[tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[tensor(88.5300), tensor(9060000.), tensor(89...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[tensor(559.9700), tensor(3.9017e+08), tensor...</td>\n",
       "      <td>[tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[tensor(1837.4900), tensor(3.6521e+09), tenso...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[tensor(2058.2000), tensor(2.7087e+09), tenso...</td>\n",
       "      <td>[tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10674</th>\n",
       "      <td>[[tensor(348.7000), tensor(1.4214e+08), tensor...</td>\n",
       "      <td>[tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10675</th>\n",
       "      <td>[[tensor(55.2700), tensor(3240000.), tensor(56...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10676</th>\n",
       "      <td>[[tensor(166.0700), tensor(70860000.), tensor(...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10677</th>\n",
       "      <td>[[tensor(1324.4600), tensor(3.4790e+09), tenso...</td>\n",
       "      <td>[tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10678</th>\n",
       "      <td>[[tensor(1441.4800), tensor(3.2163e+09), tenso...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10679 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0             1\n",
       "0      [[tensor(447.1000), tensor(2.2217e+08), tensor...  [tensor(1.)]\n",
       "1      [[tensor(88.5300), tensor(9060000.), tensor(89...  [tensor(0.)]\n",
       "2      [[tensor(559.9700), tensor(3.9017e+08), tensor...  [tensor(1.)]\n",
       "3      [[tensor(1837.4900), tensor(3.6521e+09), tenso...  [tensor(0.)]\n",
       "4      [[tensor(2058.2000), tensor(2.7087e+09), tenso...  [tensor(1.)]\n",
       "...                                                  ...           ...\n",
       "10674  [[tensor(348.7000), tensor(1.4214e+08), tensor...  [tensor(1.)]\n",
       "10675  [[tensor(55.2700), tensor(3240000.), tensor(56...  [tensor(0.)]\n",
       "10676  [[tensor(166.0700), tensor(70860000.), tensor(...  [tensor(0.)]\n",
       "10677  [[tensor(1324.4600), tensor(3.4790e+09), tenso...  [tensor(1.)]\n",
       "10678  [[tensor(1441.4800), tensor(3.2163e+09), tenso...  [tensor(0.)]\n",
       "\n",
       "[10679 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_is_copy': None, '_mgr': BlockManager\n",
      "Items: RangeIndex(start=0, stop=2, step=1)\n",
      "Axis 1: RangeIndex(start=0, stop=10679, step=1)\n",
      "ObjectBlock: slice(0, 2, 1), 2 x 10679, dtype: object, '_item_cache': {1: 0        [tensor(1.)]\n",
      "1        [tensor(0.)]\n",
      "2        [tensor(1.)]\n",
      "3        [tensor(0.)]\n",
      "4        [tensor(1.)]\n",
      "             ...     \n",
      "10674    [tensor(1.)]\n",
      "10675    [tensor(0.)]\n",
      "10676    [tensor(0.)]\n",
      "10677    [tensor(1.)]\n",
      "10678    [tensor(0.)]\n",
      "Name: 1, Length: 10679, dtype: object}, '_attrs': {}}\n",
      "1 5644\n",
      "0 5035\n",
      "\n",
      "1 5035\n",
      "0 5035\n",
      "\n",
      "10070 2672 4453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4.4710e+02,  2.2217e+08,  4.4742e+02, -1.1717e-02,  3.6745e-02,\n",
       "          -5.2182e-02],\n",
       "         [ 4.4909e+02,  2.2258e+08,  4.4744e+02, -1.6216e-02,  2.9531e-01,\n",
       "          -3.9180e-02],\n",
       "         [ 4.4824e+02,  2.5675e+08,  4.4745e+02,  2.5274e-03,  7.1733e-01,\n",
       "          -3.6421e-02],\n",
       "         [ 4.4719e+02,  2.7310e+08,  4.4716e+02, -2.6005e-01,  5.4103e-01,\n",
       "          -4.8633e-02],\n",
       "         [ 4.5024e+02,  2.6124e+08,  4.4726e+02, -1.9322e-02,  5.3152e-01,\n",
       "          -4.3317e-02],\n",
       "         [ 4.4813e+02,  2.5442e+08,  4.4750e+02,  1.4868e-02,  2.5580e-01,\n",
       "          -5.1023e-02],\n",
       "         [ 4.5015e+02,  2.3038e+08,  4.4791e+02,  7.2024e-03,  4.2335e-01,\n",
       "          -4.0091e-02],\n",
       "         [ 4.4927e+02,  2.5311e+08,  4.4811e+02, -8.7749e-03,  2.4083e-01,\n",
       "          -3.6603e-02],\n",
       "         [ 4.4854e+02,  2.3004e+08,  4.4825e+02, -5.9983e-03,  2.6962e-01,\n",
       "          -4.5623e-02],\n",
       "         [ 4.4813e+02,  2.6190e+08,  4.4861e+02,  1.9047e-02, -7.0843e-02,\n",
       "          -5.5514e-02]]), tensor([1.]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Normalizing certain features within sequences \n",
      "\n",
      "10070 train took: 10.544425249099731\n",
      "2672 val took: 2.6794631481170654\n",
      "4453 test took: 4.442286014556885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0000, -1.0000, -0.6362, -0.0117,  0.0367, -0.0522],\n",
       "         [ 0.2675, -0.9839, -0.6210, -0.0162,  0.2953, -0.0392],\n",
       "         [-0.2739,  0.3579, -0.6003,  0.0025,  0.7173, -0.0364],\n",
       "         [-0.9427,  1.0000, -1.0000, -0.2601,  0.5410, -0.0486],\n",
       "         [ 1.0000,  0.5343, -0.8589, -0.0193,  0.5315, -0.0433],\n",
       "         [-0.3439,  0.2664, -0.5298,  0.0149,  0.2558, -0.0510],\n",
       "         [ 0.9427, -0.6776,  0.0401,  0.0072,  0.4234, -0.0401],\n",
       "         [ 0.3822,  0.2150,  0.3112, -0.0088,  0.2408, -0.0366],\n",
       "         [-0.0828, -0.6909,  0.4993, -0.0060,  0.2696, -0.0456],\n",
       "         [-0.3439,  0.5602,  1.0000,  0.0190, -0.0708, -0.0555]]),\n",
       " tensor([1.]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample #0 train 0.0 {0: 0, 1: 0}\n",
      "sample #1000 train 0.5845667123794556 {0: 214, 1: 279}\n",
      "sample #2000 train 0.5592778921127319 {0: 477, 1: 516}\n",
      "sample #3000 train 0.5612137317657471 {0: 682, 1: 815}\n",
      "sample #4000 train 0.5644726157188416 {0: 966, 1: 1031}\n",
      "sample #5000 train 0.5607473254203796 {0: 1234, 1: 1259}\n",
      "sample #6000 train 0.5529829263687134 {0: 1483, 1: 1522}\n",
      "sample #7000 train 0.5555899739265442 {0: 1739, 1: 1788}\n",
      "sample #8000 train 0.555131733417511 {0: 1989, 1: 2063}\n",
      "sample #9000 train 0.5630013942718506 {0: 2222, 1: 2322}\n",
      "sample #10000 train 0.5544167160987854 {0: 2517, 1: 2534}\n",
      "sample #0 validate 0.04055023938417435 {0: 0, 1: 0}\n",
      "sample #1000 validate 0.5662282705307007 {0: 333, 1: 119}\n",
      "sample #2000 validate 0.5392870306968689 {0: 673, 1: 284}\n",
      "sample #0 train 0.3779163062572479 {0: 0, 1: 0}\n",
      "sample #1000 train 0.5617916584014893 {0: 223, 1: 282}\n",
      "sample #2000 train 0.5663018226623535 {0: 493, 1: 515}\n",
      "sample #3000 train 0.5542238354682922 {0: 694, 1: 809}\n",
      "sample #4000 train 0.5621654391288757 {0: 968, 1: 1035}\n",
      "sample #5000 train 0.5546456575393677 {0: 1229, 1: 1275}\n",
      "sample #6000 train 0.5606856346130371 {0: 1484, 1: 1530}\n",
      "sample #7000 train 0.5526214838027954 {0: 1727, 1: 1802}\n",
      "sample #8000 train 0.5685145854949951 {0: 1969, 1: 2053}\n",
      "sample #9000 train 0.5615916848182678 {0: 2185, 1: 2319}\n",
      "sample #10000 train 0.5583791732788086 {0: 2474, 1: 2537}\n",
      "sample #0 validate 0.039685510098934174 {0: 0, 1: 0}\n",
      "sample #1000 validate 0.5296814441680908 {0: 349, 1: 164}\n",
      "sample #2000 validate 0.5325667262077332 {0: 700, 1: 306}\n",
      "sample #0 train 0.3747776746749878 {0: 0, 1: 0}\n",
      "sample #1000 train 0.5677303075790405 {0: 220, 1: 270}\n",
      "sample #2000 train 0.5645972490310669 {0: 494, 1: 500}\n",
      "sample #3000 train 0.5487896800041199 {0: 709, 1: 793}\n",
      "sample #4000 train 0.5648539066314697 {0: 1011, 1: 1014}\n",
      "sample #5000 train 0.5623160600662231 {0: 1281, 1: 1248}\n",
      "sample #6000 train 0.5560035109519958 {0: 1511, 1: 1522}\n",
      "sample #7000 train 0.5562176704406738 {0: 1771, 1: 1791}\n",
      "sample #8000 train 0.5566931366920471 {0: 2029, 1: 2049}\n",
      "sample #9000 train 0.5625258684158325 {0: 2251, 1: 2313}\n",
      "sample #10000 train 0.5561067461967468 {0: 2546, 1: 2534}\n",
      "sample #0 validate 0.046193379908800125 {0: 0, 1: 0}\n",
      "sample #1000 validate 0.5196795463562012 {0: 257, 1: 264}\n",
      "sample #2000 validate 0.5207316875457764 {0: 489, 1: 541}\n",
      "sample #0: tensor([[0.2902, 0.7098]]) tensor([1.]) 1\n",
      "sample #250: tensor([[0.4692, 0.5308]]) tensor([1.]) 1\n",
      "sample #500: tensor([[0.5920, 0.4080]]) tensor([1.]) 0\n",
      "sample #750: tensor([[0.5256, 0.4744]]) tensor([0.]) 0\n",
      "sample #1000: tensor([[0.4347, 0.5653]]) tensor([1.]) 1\n",
      "sample #1250: tensor([[0.7133, 0.2867]]) tensor([0.]) 0\n",
      "sample #1500: tensor([[0.5013, 0.4987]]) tensor([1.]) 0\n",
      "sample #1750: tensor([[0.5653, 0.4347]]) tensor([1.]) 0\n",
      "sample #2000: tensor([[0.4429, 0.5571]]) tensor([1.]) 1\n",
      "sample #2250: tensor([[0.4418, 0.5582]]) tensor([1.]) 1\n",
      "sample #2500: tensor([[0.4633, 0.5367]]) tensor([1.]) 1\n",
      "sample #2750: tensor([[0.6177, 0.3823]]) tensor([1.]) 0\n",
      "sample #3000: tensor([[0.6159, 0.3841]]) tensor([1.]) 0\n",
      "sample #3250: tensor([[0.7076, 0.2924]]) tensor([1.]) 0\n",
      "sample #3500: tensor([[0.3478, 0.6522]]) tensor([1.]) 1\n",
      "sample #3750: tensor([[0.4935, 0.5065]]) tensor([1.]) 1\n",
      "sample #4000: tensor([[0.6348, 0.3652]]) tensor([0.]) 0\n",
      "sample #4250: tensor([[0.4572, 0.5428]]) tensor([1.]) 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train loss</th>\n",
       "      <th>validate loss</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>validate accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>data_set</th>\n",
       "      <th>hidden_neurons</th>\n",
       "      <th>...</th>\n",
       "      <th>mode</th>\n",
       "      <th>label_count</th>\n",
       "      <th>target_percent</th>\n",
       "      <th>stop_loss_percent</th>\n",
       "      <th>rfe_select</th>\n",
       "      <th>chosen_dependent</th>\n",
       "      <th>dropout_p</th>\n",
       "      <th>net</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.561266</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.504965</td>\n",
       "      <td>0.472305</td>\n",
       "      <td>52.064101</td>\n",
       "      <td>89.557570</td>\n",
       "      <td>sp500</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>next</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>[Close, Volume, sma_10]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.560140</td>\n",
       "      <td>0.537809</td>\n",
       "      <td>0.500397</td>\n",
       "      <td>0.492889</td>\n",
       "      <td>51.573809</td>\n",
       "      <td>141.131433</td>\n",
       "      <td>sp500</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>next</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>[Close, Volume, sma_10]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.560281</td>\n",
       "      <td>0.522357</td>\n",
       "      <td>0.507349</td>\n",
       "      <td>0.509731</td>\n",
       "      <td>51.682374</td>\n",
       "      <td>192.813864</td>\n",
       "      <td>sp500</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>next</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>[Close, Volume, sma_10]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NeuralNet(\\n  (output_activation): Softmax(dim...</td>\n",
       "      <td>0.5064</td>\n",
       "      <td>[[1103, 983], [1215, 1152]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   run  epoch  train loss  validate loss  train accuracy  validate accuracy  \\\n",
       "0    1      1    0.561266       0.555176        0.504965           0.472305   \n",
       "1    1      2    0.560140       0.537809        0.500397           0.492889   \n",
       "2    1      3    0.560281       0.522357        0.507349           0.509731   \n",
       "\n",
       "   epoch duration  run duration data_set  hidden_neurons  ...  mode  \\\n",
       "0       52.064101     89.557570    sp500              50  ...  next   \n",
       "1       51.573809    141.131433    sp500              50  ...  next   \n",
       "2       51.682374    192.813864    sp500              50  ...  next   \n",
       "\n",
       "  label_count target_percent stop_loss_percent rfe_select  \\\n",
       "0           2           None              None          3   \n",
       "1           2           None              None          3   \n",
       "2           2           None              None          3   \n",
       "\n",
       "          chosen_dependent  dropout_p  \\\n",
       "0  [Close, Volume, sma_10]        0.4   \n",
       "1  [Close, Volume, sma_10]        0.4   \n",
       "2  [Close, Volume, sma_10]        0.4   \n",
       "\n",
       "                                                 net test_accuracy  \\\n",
       "0                                                NaN           NaN   \n",
       "1                                                NaN           NaN   \n",
       "2  NeuralNet(\\n  (output_activation): Softmax(dim...        0.5064   \n",
       "\n",
       "              confusion_matrix  \n",
       "0                          NaN  \n",
       "1                          NaN  \n",
       "2  [[1103, 983], [1215, 1152]]  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "epochs = 3\n",
    "# get all runs from params using RunBuilder class\n",
    "# print(f\"Runs: {RunBuilder.get_runs(params)}\")\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    print(run)\n",
    "    # if params changes, following line of code should reflect the changes too\n",
    "#     len(m.data_sets[run.data_set]['train'][0][0][0])\n",
    "\n",
    "    use_last_data = False\n",
    "    \n",
    "    if not use_last_data:\n",
    "        input_size = len(run.chosen_dependent) + run.rfe_select\n",
    "\n",
    "        net = NeuralNet(input_size=input_size, output_neurons=run.label_mode['label_count'],\n",
    "                       dropout_p=run.dropout_p)\n",
    "    #         hidden_neurons=run.hidden_neurons,\n",
    "    #         hidden_activation=run.hidden_activation, output_activation=run.loss_output['output_activation'])\n",
    "        optimizer = run.optimizer(net.parameters(), lr=run.learning_rate, momentum=run.momentum)#copy.deepcopy(run.optimizer)\n",
    "\n",
    "        sum_loss = 0\n",
    "        criterion = run.loss_output['criterion']\n",
    "\n",
    "    \n",
    "        m.begin_run(run, net)\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        m.begin_epoch(epoch + 1)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'validate']:\n",
    "            count = 0\n",
    "            if phase == 'train':\n",
    "                net.train()  # Set model to training mode\n",
    "#             else:\n",
    "#                 net.eval()   # Set model to evaluate mode\n",
    "                \n",
    "            # Iterate over data.\n",
    "            for images, labels in m.data_sets[m.run_params.data_set][phase]:\n",
    "\n",
    "                if count % 1000 == 0:\n",
    "                    print(f'sample #{count} {phase} {sum_loss / 1000} {m.epoch_num_correct[phase]}')\n",
    "                    sum_loss = 0\n",
    "                    \n",
    "                net.hidden_cell = (torch.zeros(1, 1, net.hidden_neurons),\n",
    "                    torch.zeros(1, 1, net.hidden_neurons))\n",
    "\n",
    "                X = Variable(images)#.reshape(1, 784, 1).squeeze(0)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(X, count)         \n",
    "                    Y = np.zeros(len(m.global_labels))\n",
    "                    try:\n",
    "                        l_i = int(labels.item())\n",
    "                    except:\n",
    "                        l_i = 0\n",
    "                    try:\n",
    "                        Y[l_i] = 1 \n",
    "                    except:\n",
    "                        print('bad label')\n",
    "                        Y[0] = 1\n",
    "                    Y = Variable(torch.from_numpy(Y).long()).unsqueeze(0)\n",
    "                    loss = criterion(outputs, Y)\n",
    "#                     if math.isnan(loss):\n",
    "#                         print(outputs, Y)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                sum_loss += loss\n",
    "                m.track_loss(phase, loss)\n",
    "                m.track_num_correct(phase, outputs, labels)\n",
    "                \n",
    "#                 print('\\n')\n",
    "                count += 1\n",
    "#                 if count > 500:\n",
    "#                     break\n",
    "                \n",
    "                    \n",
    "        m.end_epoch()\n",
    "    \n",
    "    # Testing\n",
    "    y_true = []\n",
    "    y_predict = []\n",
    "    phase = 'test'\n",
    "    count = 0\n",
    "#     net.eval()\n",
    "    for images, labels in m.data_sets[m.run_params.data_set]['test']:\n",
    "#         print('sample')\n",
    "        with torch.set_grad_enabled(False):\n",
    "            X = Variable(images)#.unsqueeze(2)\n",
    "            Y = Variable(labels)\n",
    "            outputs = net(X, count)\n",
    "            predicted_class = int(torch.argmax(outputs))\n",
    "\n",
    "            m.track_test_predictions(predicted_class, labels.item())\n",
    "\n",
    "        if count % 250 == 0:\n",
    "            print(f'sample #{count}: {outputs} {labels} {predicted_class}')\n",
    "        count += 1\n",
    "\n",
    "#         if count > 500:\n",
    "#             break\n",
    "#         print('\\n\\n')\n",
    "\n",
    "    if not use_last_data:\n",
    "        m.end_run(net)\n",
    "    else:\n",
    "        break\n",
    "# when all runs are done, show results\n",
    "m.results('trial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('run', 1),\n",
       "              ('epoch', 1),\n",
       "              ('train loss', 0.5054013383072672),\n",
       "              ('validate loss', 0.4940976263252127),\n",
       "              ('train accuracy', 0.4952726473175022),\n",
       "              ('validate accuracy', 0.565282454171343),\n",
       "              ('epoch duration', 51.0260648727417),\n",
       "              ('run duration', 51.02607083320618),\n",
       "              ('data_set', 'xrp_btc'),\n",
       "              ('hidden_neurons', 100),\n",
       "              ('batch_size', 1),\n",
       "              ('function',\n",
       "               <function torch.nn.init._make_deprecate.<locals>.deprecated_init(*args, **kwargs)>),\n",
       "              ('name', 'Xavier Uniform'),\n",
       "              ('hidden_activation', <function _VariableFunctionsClass.relu>),\n",
       "              ('criterion', <function __main__.sum_squared_error(out, label)>),\n",
       "              ('output_activation', Softmax(dim=2)),\n",
       "              ('learning_rate', 0.01),\n",
       "              ('momentum', 0.1),\n",
       "              ('optimizer', torch.optim.sgd.SGD),\n",
       "              ('validation_split', 0.1)]),\n",
       " OrderedDict([('run', 1),\n",
       "              ('epoch', 2),\n",
       "              ('train loss', 0.5030166666017699),\n",
       "              ('validate loss', 0.49491386166280726),\n",
       "              ('train accuracy', 0.5049472295514512),\n",
       "              ('validate accuracy', 0.5712682379349046),\n",
       "              ('epoch duration', 52.33416700363159),\n",
       "              ('run duration', 103.36028099060059),\n",
       "              ('data_set', 'xrp_btc'),\n",
       "              ('hidden_neurons', 100),\n",
       "              ('batch_size', 1),\n",
       "              ('function',\n",
       "               <function torch.nn.init._make_deprecate.<locals>.deprecated_init(*args, **kwargs)>),\n",
       "              ('name', 'Xavier Uniform'),\n",
       "              ('hidden_activation', <function _VariableFunctionsClass.relu>),\n",
       "              ('criterion', <function __main__.sum_squared_error(out, label)>),\n",
       "              ('output_activation', Softmax(dim=2)),\n",
       "              ('learning_rate', 0.01),\n",
       "              ('momentum', 0.1),\n",
       "              ('optimizer', torch.optim.sgd.SGD),\n",
       "              ('validation_split', 0.1)]),\n",
       " OrderedDict([('run', 1),\n",
       "              ('epoch', 3),\n",
       "              ('train loss', 0.5021216979048811),\n",
       "              ('validate loss', 0.49541631800335445),\n",
       "              ('train accuracy', 0.5019788918205804),\n",
       "              ('validate accuracy', 0.5679012345679012),\n",
       "              ('epoch duration', 52.470519065856934),\n",
       "              ('run duration', 155.83084201812744),\n",
       "              ('data_set', 'xrp_btc'),\n",
       "              ('hidden_neurons', 100),\n",
       "              ('batch_size', 1),\n",
       "              ('function',\n",
       "               <function torch.nn.init._make_deprecate.<locals>.deprecated_init(*args, **kwargs)>),\n",
       "              ('name', 'Xavier Uniform'),\n",
       "              ('hidden_activation', <function _VariableFunctionsClass.relu>),\n",
       "              ('criterion', <function __main__.sum_squared_error(out, label)>),\n",
       "              ('output_activation', Softmax(dim=2)),\n",
       "              ('learning_rate', 0.01),\n",
       "              ('momentum', 0.1),\n",
       "              ('optimizer', torch.optim.sgd.SGD),\n",
       "              ('validation_split', 0.1)]),\n",
       " OrderedDict([('run', 1),\n",
       "              ('epoch', 4),\n",
       "              ('train loss', 0.5018801802315519),\n",
       "              ('validate loss', 0.49581602228031235),\n",
       "              ('train accuracy', 0.5042875989445911),\n",
       "              ('validate accuracy', 0.5615413393191171),\n",
       "              ('epoch duration', 53.39913988113403),\n",
       "              ('run duration', 209.23002576828003),\n",
       "              ('data_set', 'xrp_btc'),\n",
       "              ('hidden_neurons', 100),\n",
       "              ('batch_size', 1),\n",
       "              ('function',\n",
       "               <function torch.nn.init._make_deprecate.<locals>.deprecated_init(*args, **kwargs)>),\n",
       "              ('name', 'Xavier Uniform'),\n",
       "              ('hidden_activation', <function _VariableFunctionsClass.relu>),\n",
       "              ('criterion', <function __main__.sum_squared_error(out, label)>),\n",
       "              ('output_activation', Softmax(dim=2)),\n",
       "              ('learning_rate', 0.01),\n",
       "              ('momentum', 0.1),\n",
       "              ('optimizer', torch.optim.sgd.SGD),\n",
       "              ('validation_split', 0.1)]),\n",
       " OrderedDict([('run', 1),\n",
       "              ('epoch', 5),\n",
       "              ('train loss', 0.5017599487151351),\n",
       "              ('validate loss', 0.495741448819838),\n",
       "              ('train accuracy', 0.5038478452066842),\n",
       "              ('validate accuracy', 0.5611672278338945),\n",
       "              ('epoch duration', 51.936245918273926),\n",
       "              ('run duration', 261.16631174087524),\n",
       "              ('data_set', 'xrp_btc'),\n",
       "              ('hidden_neurons', 100),\n",
       "              ('batch_size', 1),\n",
       "              ('function',\n",
       "               <function torch.nn.init._make_deprecate.<locals>.deprecated_init(*args, **kwargs)>),\n",
       "              ('name', 'Xavier Uniform'),\n",
       "              ('hidden_activation', <function _VariableFunctionsClass.relu>),\n",
       "              ('criterion', <function __main__.sum_squared_error(out, label)>),\n",
       "              ('output_activation', Softmax(dim=2)),\n",
       "              ('learning_rate', 0.01),\n",
       "              ('momentum', 0.1),\n",
       "              ('optimizer', torch.optim.sgd.SGD),\n",
       "              ('validation_split', 0.1)]),\n",
       " OrderedDict([('run', 1),\n",
       "              ('epoch', 6),\n",
       "              ('train loss', 0.5016201729529923),\n",
       "              ('validate loss', 0.4960048052642928),\n",
       "              ('train accuracy', 0.5046174142480211),\n",
       "              ('validate accuracy', 0.5548073325851104),\n",
       "              ('epoch duration', 53.67730498313904),\n",
       "              ('run duration', 314.84365797042847),\n",
       "              ('data_set', 'xrp_btc'),\n",
       "              ('hidden_neurons', 100),\n",
       "              ('batch_size', 1),\n",
       "              ('function',\n",
       "               <function torch.nn.init._make_deprecate.<locals>.deprecated_init(*args, **kwargs)>),\n",
       "              ('name', 'Xavier Uniform'),\n",
       "              ('hidden_activation', <function _VariableFunctionsClass.relu>),\n",
       "              ('criterion', <function __main__.sum_squared_error(out, label)>),\n",
       "              ('output_activation', Softmax(dim=2)),\n",
       "              ('learning_rate', 0.01),\n",
       "              ('momentum', 0.1),\n",
       "              ('optimizer', torch.optim.sgd.SGD),\n",
       "              ('validation_split', 0.1),\n",
       "              ('net', NeuralNet(\n",
       "                 (output_activation): Softmax(dim=2)\n",
       "                 (lstm): LSTM(12, 128)\n",
       "                 (dropout): Dropout(p=0.4, inplace=False)\n",
       "                 (out): Linear(in_features=128, out_features=2, bias=True)\n",
       "               )),\n",
       "              ('test_accuracy', 0.5637342908438061),\n",
       "              ('confusion_matrix', array([[ 340, 1572],\n",
       "                      [ 372, 2172]]))])]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cnf_matrix = sklearn.metrics.confusion_matrix(pd.DataFrame(m.test_labels).fillna(0).values, m.test_predictions)\n",
    "# m.run_data[-1]['confusion_matrix'] = cnf_matrix\n",
    "\n",
    "# test_accuracy = sum([v for k, v in m.test_correct_count.items()]) / (len(m.data_sets['xrp_btc'][phase]))\n",
    "# m.run_data[-1]['test_accuracy'] = test_accuracy\n",
    "\n",
    "# m.runs.append(m.run_data)\n",
    "m.run_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train loss</th>\n",
       "      <th>validate loss</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>validate accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>data_set</th>\n",
       "      <th>hidden_neurons</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_components</th>\n",
       "      <th>mode</th>\n",
       "      <th>label_count</th>\n",
       "      <th>target_percent</th>\n",
       "      <th>stop_loss_percent</th>\n",
       "      <th>rfe_select</th>\n",
       "      <th>chosen_dependent</th>\n",
       "      <th>net</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.708146</td>\n",
       "      <td>0.666682</td>\n",
       "      <td>0.336121</td>\n",
       "      <td>0.408928</td>\n",
       "      <td>133.900440</td>\n",
       "      <td>191.243614</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>since3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>[Close, Volume]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.707059</td>\n",
       "      <td>0.665574</td>\n",
       "      <td>0.335734</td>\n",
       "      <td>0.408928</td>\n",
       "      <td>129.249920</td>\n",
       "      <td>320.493679</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>since3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>[Close, Volume]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.705538</td>\n",
       "      <td>0.671844</td>\n",
       "      <td>0.335114</td>\n",
       "      <td>0.269112</td>\n",
       "      <td>128.221113</td>\n",
       "      <td>448.714846</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>since3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>[Close, Volume]</td>\n",
       "      <td>NeuralNet(\\n  (output_activation): Softmax(dim...</td>\n",
       "      <td>0.269243</td>\n",
       "      <td>[[0, 2178, 0], [0, 1749, 0], [0, 2569, 0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   run  epoch  train loss  validate loss  train accuracy  validate accuracy  \\\n",
       "0    1      1    0.708146       0.666682        0.336121           0.408928   \n",
       "1    1      2    0.707059       0.665574        0.335734           0.408928   \n",
       "2    1      3    0.705538       0.671844        0.335114           0.269112   \n",
       "\n",
       "   epoch duration  run duration data_set  hidden_neurons  ...  pca_components  \\\n",
       "0      133.900440    191.243614  BTCUSDT              50  ...            None   \n",
       "1      129.249920    320.493679  BTCUSDT              50  ...            None   \n",
       "2      128.221113    448.714846  BTCUSDT              50  ...            None   \n",
       "\n",
       "     mode label_count target_percent stop_loss_percent rfe_select  \\\n",
       "0  since3           3          0.005             0.005         10   \n",
       "1  since3           3          0.005             0.005         10   \n",
       "2  since3           3          0.005             0.005         10   \n",
       "\n",
       "   chosen_dependent                                                net  \\\n",
       "0   [Close, Volume]                                                NaN   \n",
       "1   [Close, Volume]                                                NaN   \n",
       "2   [Close, Volume]  NeuralNet(\\n  (output_activation): Softmax(dim...   \n",
       "\n",
       "  test_accuracy                            confusion_matrix  \n",
       "0           NaN                                         NaN  \n",
       "1           NaN                                         NaN  \n",
       "2      0.269243  [[0, 2178, 0], [0, 1749, 0], [0, 2569, 0]]  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas.plotting import table \n",
    "source_df = pd.DataFrame(m.run_data)\n",
    "display(source_df)\n",
    "record = {\n",
    "    'sp500': [],\n",
    "    'BTCUSDT': []\n",
    "}\n",
    "data_set = 'BTCUSDT'\n",
    "    \n",
    "df = source_df.loc[source_df.data_set == data_set]\n",
    "\n",
    "for run_i in df['run'].unique():\n",
    "    run_data = df.loc[df.run == run_i]\n",
    "    epochs = run_data.epoch.values\n",
    "\n",
    "    # Accuracy 1st y-axis\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Loss 2nd y-axis\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    colors = ['red', 'green', 'blue']\n",
    "\n",
    "    for phase_i, phase in enumerate(['train', 'validate']):\n",
    "\n",
    "        accuracy = run_data[f'{phase} accuracy'].values\n",
    "\n",
    "        # record record\n",
    "        if phase == 'validate':\n",
    "            record[data_set].append({\n",
    "                'max_accuracy': np.max(accuracy).round(3),\n",
    "                'epoch': np.where(accuracy == np.max(accuracy))[0] + 1,\n",
    "\n",
    "#                 'run': run_i\n",
    "            })\n",
    "#             for variable in variables:\n",
    "#                 record[data_set][-1][variable] = run_data[variable].values[0]\n",
    "\n",
    "        loss = run_data[f'{phase} loss'].values\n",
    "        phase_accuracy, = ax1.plot(epochs, accuracy, \n",
    "             color=colors[phase_i],   \n",
    "             linewidth=1.0\n",
    "        )\n",
    "        phase_accuracy.set_label(f\"{phase.capitalize()} Accuracy\")\n",
    "\n",
    "        phase_loss, = ax2.plot(epochs, loss, \n",
    "             color=colors[phase_i],   \n",
    "             linewidth=1.0,\n",
    "             linestyle='--' \n",
    "        )\n",
    "        phase_loss.set_label(f\"{phase.capitalize()} Loss\")\n",
    "\n",
    "    ax1.legend(loc='lower left')\n",
    "    ax2.legend(loc='lower right')\n",
    "\n",
    "    x_label = \"Epoch\"\n",
    "#     for variable in variables:\n",
    "#         x_label += f\"\\n{variable} = {run_data[variable].values[0]}\"\n",
    "    ax1.set_xlabel(x_label)\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "\n",
    "    plt.title(f\"{run_data['data_set'].values[0]}: Epoch vs. Accuracy and Loss\")\n",
    "\n",
    "#     ax1.set_ybound(lower=0.45, upper=.55)\n",
    "\n",
    "    save_string = \"sigma_relationship.png\"\n",
    "#     for variable in variables:\n",
    "#         save_string = f\"{data_set}_{variable}_{run_data[variable].values[0]}_\" + save_string\n",
    "#     plt.savefig(f\"./{variables[0]}/\" + save_string, bbox_inches='tight')\n",
    "    plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT\n",
      "[array([[   0, 2178,    0],\n",
      "       [   0, 1749,    0],\n",
      "       [   0, 2569,    0]])]\n",
      "[[   0 2178    0]\n",
      " [   0 1749    0]\n",
      " [   0 2569    0]]\n",
      "Normalized Confusion Matrix (Run #1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "source_df = pd.DataFrame(m.run_data)\n",
    "print(data_set)\n",
    "def plot_confusion_matrix(df_row, normalize=True, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    print(df_row.confusion_matrix.values)\n",
    "    cm = deepcopy(df_row.confusion_matrix.values[0])\n",
    "    classes = m.global_labels #df_row['label_subset'].values[0]\n",
    "    print(cm)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(f\"Normalized Confusion Matrix (Run #{df_row.run.values[0]})\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "#     new = [[] for class_ in range(len(classes))]\n",
    "#     print(new[0])\n",
    "#     for row_i, row in enumerate(cm):\n",
    "#         for col_i, col in enumerate(row):\n",
    "#             print(row, col)\n",
    "#             print(row_i, col_i)\n",
    "#             print(col.item(), row.sum().item(), round(col.item() / row.sum().item(), 2))\n",
    "#             new[row_i].append(round(col.item() / row.sum().item(), 2))\n",
    "#             print(new)\n",
    "#             print()\n",
    "#         print()\n",
    "    \n",
    "#     cm = np.asarray(new)\n",
    "#     print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, aspect='auto')\n",
    "    plt.title(f'{data_set}: Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "\n",
    "    x_label = \"Predicted label\"\n",
    "#     for variable in variables:\n",
    "#         x_label += f\"\\n{variable} = {df_row[variable].values[0]}\"\n",
    "    plt.xlabel(x_label)\n",
    "    \n",
    "    save_string = \"confusion_matrix.png\"\n",
    "#     for variable in variables:\n",
    "#         save_string = f\"{data_set}_{variable}_{df_row[variable].values[0]}_\" + save_string\n",
    "#     plt.savefig(f\"./{variables[0]}/\" + save_string, bbox_inches='tight')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "for data_set in list(source_df.data_set.unique()):\n",
    "    df = source_df.loc[source_df.data_set == data_set]\n",
    "#     display(df)\n",
    "    for run_i in df['run'].unique():\n",
    "        final_epoch_df = df.loc[df.run == run_i][-1:]\n",
    "#         print(final_epoch_df.confusion_matrix)\n",
    "#         plot_confusion_matrix(final_epoch_df)#, variables)\n",
    "#         break\n",
    "#     break\n",
    "plot_confusion_matrix(df.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = 100\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo",
   "language": "python",
   "name": "algo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
