{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17 04:00:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>1.775183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17 04:01:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-17 04:02:00.000000</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>0.261074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-17 04:03:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.012008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-17 04:04:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.140796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>2017-09-04 05:30:00.000000</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>2017-09-04 05:31:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.198468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>2017-09-04 05:32:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>2017-09-04 05:33:00.000000</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>0.784977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>2017-09-04 05:34:00.000000</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>1.211797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Date     Open     High      Low    Close  \\\n",
       "0      2017-08-17 04:00:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "1      2017-08-17 04:01:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "2      2017-08-17 04:02:00.000000  4280.56  4280.56  4280.56  4280.56   \n",
       "3      2017-08-17 04:03:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "4      2017-08-17 04:04:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "...                           ...      ...      ...      ...      ...   \n",
       "26010  2017-09-04 05:30:00.000000  4462.50  4462.50  4462.50  4462.50   \n",
       "26011  2017-09-04 05:31:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26012  2017-09-04 05:32:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26013  2017-09-04 05:33:00.000000  4433.95  4433.95  4433.94  4433.94   \n",
       "26014  2017-09-04 05:34:00.000000  4434.17  4434.17  4401.45  4401.45   \n",
       "\n",
       "         Volume  \n",
       "0      1.775183  \n",
       "1      0.000000  \n",
       "2      0.261074  \n",
       "3      0.012008  \n",
       "4      0.140796  \n",
       "...         ...  \n",
       "26010  0.000000  \n",
       "26011  0.198468  \n",
       "26012  0.000000  \n",
       "26013  0.784977  \n",
       "26014  1.211797  \n",
       "\n",
       "[26015 rows x 6 columns]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import modin.pandas as pd\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import math\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data_set = \n",
    "data_sets = {\n",
    "    'sp500': {\n",
    "        'data': pd.read_csv(f'/Users/gabeheim/documents/concatenated_price_data/sp500.csv', index_col=False).drop(['Adj Close'], axis=1)\n",
    "    },\n",
    "    'BTCUSDT': {\n",
    "        'data': pd.read_csv(f'/Users/gabeheim/documents/concatenated_price_data/BTCUSDT.csv', index_col=False)\n",
    "    }\n",
    "}\n",
    "\n",
    "data_sets['BTCUSDT']['data'] = data_sets['BTCUSDT']['data'][:math.floor(len(data_sets['BTCUSDT']['data'])/32)]#16)]\n",
    "\n",
    "# display(data_sets['sp500']['data'])\n",
    "data_sets['BTCUSDT']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data_sets['BTCUSDT']['data']\n",
    "end = 150\n",
    "plt.plot(data.index[50:end], data.Close[50:end])\n",
    "plt.plot(data.index[50:end], data.High[50:end])\n",
    "plt.plot(data.index[50:end], data.Low[50:end])\n",
    "plt.plot(data.index[50:end], data.Open[50:end])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007492893252647893"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_percent = 0.01\n",
    "# display(data.High[0:0+train_window+1])\n",
    "# print(1455.219971 * (1 + target_percent))\n",
    "# data.High[0:0+train_window+1].loc[data.High >= 1455.219971 * (1 + target_percent)]#.index[0]\n",
    "data_sets['BTCUSDT']['data'].Close.pct_change().mean() * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def get_labels(row, target_percent=.01, stop_loss_percent=.005, mode='since3'):\n",
    "    if mode == 'since':\n",
    "        try:\n",
    "            target_index = data.loc[row[6]:][data.Close >= row[4] * (1 + target_percent)].iloc[0]._name\n",
    "        except IndexError:\n",
    "            target_index = data.index.max() + 5\n",
    "        try:\n",
    "            stop_loss_index = data.loc[row[6]:][data.Close <= row[4] * (1 - stop_loss_percent)].iloc[0]._name\n",
    "        except IndexError:\n",
    "            stop_loss_index = data.index.max() + 5\n",
    "        if target_index > stop_loss_index:\n",
    "            return 0\n",
    "        elif target_index < stop_loss_index:\n",
    "            return 1\n",
    "        return None\n",
    "    if mode == 'average':\n",
    "        mean = data.Close[row[6]:row[6]+10].mean()\n",
    "#         print(mean)\n",
    "        if mean - row[4] > 0: #row[4] * (1 + target_percent):\n",
    "            return 1\n",
    "#         elif mean <= row[4] * (1 - stop_loss_percent):\n",
    "#             return -1\n",
    "        else:\n",
    "            return 0\n",
    "        return None\n",
    "    if mode == 'next':\n",
    "        print('nextnensnes')\n",
    "        try:\n",
    "            next = data.Close.values[row[6]+1]\n",
    "        except:\n",
    "            next = 0\n",
    "#         print(next)\n",
    "        if next > row[4]: #row[4] * (1 + target_percent):\n",
    "            return 1\n",
    "#         elif mean <= row[4] * (1 - stop_loss_percent):\n",
    "#             return -1\n",
    "        else:\n",
    "            return 0\n",
    "        return None\n",
    "    if mode == 'since3':\n",
    "#         max_index = data.High[row[6]:row[6]+21].idxmax()\n",
    "#         min_index = data.Low[row[6]:row[6]+21].idxmin()\n",
    "        try:\n",
    "#             target_index = data.High[row[6]:row[6]+train_window+1].loc[data.High >= row[4] * (1 + target_percent)].index[0]\n",
    "            target_index = data.Close[row[6]:row[6]+train_window+1].loc[data.Close >= row[4] * (1 + target_percent)].index[0]\n",
    "        except IndexError:\n",
    "            target_index = data.index.max() + 5\n",
    "        try:\n",
    "#             stop_loss_index = data.Low[row[6]:row[6]+train_window+1].loc[data.Low <= row[4] * (1 - stop_loss_percent)].index[0]\n",
    "            stop_loss_index = data.Close[row[6]:row[6]+train_window+1].loc[data.Close <= row[4] * (1 - stop_loss_percent)].index[0]\n",
    "        except IndexError:\n",
    "            stop_loss_index = data.index.max() + 5\n",
    "        if target_index > stop_loss_index:\n",
    "            return 0\n",
    "        elif target_index < stop_loss_index:\n",
    "            return 2\n",
    "        return 1\n",
    "\n",
    "for name, datad in data_sets.items():\n",
    "    data = datad['data']\n",
    "    train_window = 20\n",
    "    data['index'] = data.index\n",
    "    # too volatile class?\n",
    "    n = 0\n",
    "    pool = multiprocessing.Pool(multiprocessing.cpu_count() - n)                         # Create a multiprocessing Pool\n",
    "\n",
    "    start = time.time()\n",
    "    data['label'] = pool.map(partial(get_labels, mode='next'), [tuple(r) for r in data.to_numpy()])  # process data_inputs iterable with pool\n",
    "    print(name, 'pool label took: ', time.time() - start)\n",
    "\n",
    "data_sets['BTCUSDT']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT 0 9291\n",
      "BTCUSDT 1 13204\n",
      "BTCUSDT 2 3520\n"
     ]
    }
   ],
   "source": [
    "for name, datad in data_sets.items():\n",
    "    data = datad['data']\n",
    "    for label in sorted(data.label.unique()):\n",
    "        print(name, label, len(data.loc[data.label == label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>sma_10</th>\n",
       "      <th>macd</th>\n",
       "      <th>...</th>\n",
       "      <th>macd_signal_percentage</th>\n",
       "      <th>macd_hist_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>slowd_percentage</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17 04:00:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>1.775183</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17 04:01:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-17 04:02:00.000000</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>0.261074</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-17 04:03:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-17 04:04:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.140796</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>2017-09-04 05:30:00.000000</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26010</td>\n",
       "      <td>0</td>\n",
       "      <td>4448.883</td>\n",
       "      <td>7.942480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>-0.006731</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>-0.006597</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>2017-09-04 05:31:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.198468</td>\n",
       "      <td>26011</td>\n",
       "      <td>0</td>\n",
       "      <td>4448.980</td>\n",
       "      <td>8.922450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>-0.017800</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>2017-09-04 05:32:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26012</td>\n",
       "      <td>0</td>\n",
       "      <td>4449.077</td>\n",
       "      <td>9.592574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>2017-09-04 05:33:00.000000</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>0.784977</td>\n",
       "      <td>26013</td>\n",
       "      <td>0</td>\n",
       "      <td>4446.500</td>\n",
       "      <td>7.534293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.013418</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>-0.044959</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>2017-09-04 05:34:00.000000</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>1.211797</td>\n",
       "      <td>26014</td>\n",
       "      <td>1</td>\n",
       "      <td>4440.674</td>\n",
       "      <td>3.394458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.004839</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.994112</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Date     Open     High      Low    Close  \\\n",
       "0      2017-08-17 04:00:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "1      2017-08-17 04:01:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "2      2017-08-17 04:02:00.000000  4280.56  4280.56  4280.56  4280.56   \n",
       "3      2017-08-17 04:03:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "4      2017-08-17 04:04:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "...                           ...      ...      ...      ...      ...   \n",
       "26010  2017-09-04 05:30:00.000000  4462.50  4462.50  4462.50  4462.50   \n",
       "26011  2017-09-04 05:31:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26012  2017-09-04 05:32:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26013  2017-09-04 05:33:00.000000  4433.95  4433.95  4433.94  4433.94   \n",
       "26014  2017-09-04 05:34:00.000000  4434.17  4434.17  4401.45  4401.45   \n",
       "\n",
       "         Volume  index  label    sma_10      macd  ...  \\\n",
       "0      1.775183      0      1       NaN       NaN  ...   \n",
       "1      0.000000      1      1       NaN       NaN  ...   \n",
       "2      0.261074      2      1       NaN       NaN  ...   \n",
       "3      0.012008      3      1       NaN       NaN  ...   \n",
       "4      0.140796      4      1       NaN       NaN  ...   \n",
       "...         ...    ...    ...       ...       ...  ...   \n",
       "26010  0.000000  26010      0  4448.883  7.942480  ...   \n",
       "26011  0.198468  26011      0  4448.980  8.922450  ...   \n",
       "26012  0.000000  26012      0  4449.077  9.592574  ...   \n",
       "26013  0.784977  26013      0  4446.500  7.534293  ...   \n",
       "26014  1.211797  26014      1  4440.674  3.394458  ...   \n",
       "\n",
       "       macd_signal_percentage  macd_hist_percentage  cci_24_percentage  \\\n",
       "0                    0.001840             -0.002316           0.001399   \n",
       "1                    0.001840             -0.002316           0.001399   \n",
       "2                    0.001840             -0.002316           0.001399   \n",
       "3                    0.001840             -0.002316           0.001399   \n",
       "4                    0.001840             -0.002316           0.001399   \n",
       "...                       ...                   ...                ...   \n",
       "26010                0.001675             -0.006731           0.000452   \n",
       "26011                0.002119             -0.017800           0.002359   \n",
       "26012                0.002336              0.002387           0.000632   \n",
       "26013                0.001399             -0.013418          -0.013633   \n",
       "26014               -0.000230              0.018382           0.032995   \n",
       "\n",
       "       mom_10_percentage  roc_10_percentage  rsi_5_percentage  \\\n",
       "0              -0.006634          -0.006591              -1.0   \n",
       "1              -0.006634          -0.006591              -1.0   \n",
       "2              -0.006634          -0.006591              -1.0   \n",
       "3              -0.006634          -0.006591              -1.0   \n",
       "4              -0.006634          -0.006591              -1.0   \n",
       "...                  ...                ...               ...   \n",
       "26010          -0.006640          -0.006597              -1.0   \n",
       "26011          -0.007460          -0.007413              -1.0   \n",
       "26012          -0.006634          -0.006591              -1.0   \n",
       "26013          -0.045152          -0.044959              -1.0   \n",
       "26014          -0.004873          -0.004839              -1.0   \n",
       "\n",
       "       wnr_9_percentage  slowk_percentage  slowd_percentage  adosc_percentage  \n",
       "0             -0.997032          0.000144         -0.001621          0.002114  \n",
       "1             -0.997032          0.000144         -0.001621          0.002114  \n",
       "2             -0.997032          0.000144         -0.001621          0.002114  \n",
       "3             -0.997032          0.000144         -0.001621          0.002114  \n",
       "4             -0.997032          0.000144         -0.001621          0.002114  \n",
       "...                 ...               ...               ...               ...  \n",
       "26010         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26011         -1.000000          0.000144         -0.001621          0.002114  \n",
       "26012         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26013         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26014         -0.994112          0.000144         -0.001621          0.002114  \n",
       "\n",
       "[26015 rows x 45 columns]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import talib as ta\n",
    "for name, datad in data_sets.items():\n",
    "    data = datad['data']\n",
    "    data['sma_10'] = ta.SMA(data.Close, timeperiod=10)\n",
    "    macd, macd_signal, macd_hist = ta.MACDFIX(data.Close, signalperiod=9)\n",
    "    data['macd'] = macd\n",
    "    data['macd_signal'] = macd_signal\n",
    "    data['macd_hist'] = macd_hist\n",
    "    data['cci_24'] = ta.CCI(data.High, data.Low, data.Close, timeperiod=24)\n",
    "    data['mom_10'] = ta.MOM(data.Close, timeperiod=10)\n",
    "    data['roc_10'] = ta.ROC(data.Close, timeperiod=10)\n",
    "    data['rsi_5'] = ta.RSI(data.Close, timeperiod=5)\n",
    "    data['wnr_9'] = ta.WILLR(data.High, data.Low, data.Close, timeperiod=9)\n",
    "    slowk, slowd = ta.STOCH(data.High, data.Low, data.Close, fastk_period=5, \n",
    "                            slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "    data['slowk'] = slowk\n",
    "    data['slowd'] = slowd\n",
    "    data['adosc'] = ta.ADOSC(data.High, data.Low, data.Close, data.Volume, fastperiod=3, slowperiod=10)\n",
    "    data = data[10:].reset_index()\n",
    "    data = data.drop(['level_0'], axis=1)\n",
    "    data['index'] = data.index\n",
    "\n",
    "data_sets['BTCUSDT']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>sma_10</th>\n",
       "      <th>macd</th>\n",
       "      <th>...</th>\n",
       "      <th>macd_signal_percentage</th>\n",
       "      <th>macd_hist_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>slowd_percentage</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17 04:00:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>1.775183</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17 04:01:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-17 04:02:00.000000</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>0.261074</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-17 04:03:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-17 04:04:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.140796</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>2017-09-04 05:30:00.000000</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26010</td>\n",
       "      <td>0</td>\n",
       "      <td>4448.883</td>\n",
       "      <td>7.942480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>-0.006731</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>-0.006597</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>2017-09-04 05:31:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.198468</td>\n",
       "      <td>26011</td>\n",
       "      <td>0</td>\n",
       "      <td>4448.980</td>\n",
       "      <td>8.922450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>-0.017800</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>2017-09-04 05:32:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26012</td>\n",
       "      <td>0</td>\n",
       "      <td>4449.077</td>\n",
       "      <td>9.592574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>2017-09-04 05:33:00.000000</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>0.784977</td>\n",
       "      <td>26013</td>\n",
       "      <td>0</td>\n",
       "      <td>4446.500</td>\n",
       "      <td>7.534293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.013418</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>-0.044959</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>2017-09-04 05:34:00.000000</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>1.211797</td>\n",
       "      <td>26014</td>\n",
       "      <td>1</td>\n",
       "      <td>4440.674</td>\n",
       "      <td>3.394458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.004839</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.994112</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Date     Open     High      Low    Close  \\\n",
       "0      2017-08-17 04:00:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "1      2017-08-17 04:01:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "2      2017-08-17 04:02:00.000000  4280.56  4280.56  4280.56  4280.56   \n",
       "3      2017-08-17 04:03:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "4      2017-08-17 04:04:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "...                           ...      ...      ...      ...      ...   \n",
       "26010  2017-09-04 05:30:00.000000  4462.50  4462.50  4462.50  4462.50   \n",
       "26011  2017-09-04 05:31:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26012  2017-09-04 05:32:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26013  2017-09-04 05:33:00.000000  4433.95  4433.95  4433.94  4433.94   \n",
       "26014  2017-09-04 05:34:00.000000  4434.17  4434.17  4401.45  4401.45   \n",
       "\n",
       "         Volume  index  label    sma_10      macd  ...  \\\n",
       "0      1.775183      0      1       NaN       NaN  ...   \n",
       "1      0.000000      1      1       NaN       NaN  ...   \n",
       "2      0.261074      2      1       NaN       NaN  ...   \n",
       "3      0.012008      3      1       NaN       NaN  ...   \n",
       "4      0.140796      4      1       NaN       NaN  ...   \n",
       "...         ...    ...    ...       ...       ...  ...   \n",
       "26010  0.000000  26010      0  4448.883  7.942480  ...   \n",
       "26011  0.198468  26011      0  4448.980  8.922450  ...   \n",
       "26012  0.000000  26012      0  4449.077  9.592574  ...   \n",
       "26013  0.784977  26013      0  4446.500  7.534293  ...   \n",
       "26014  1.211797  26014      1  4440.674  3.394458  ...   \n",
       "\n",
       "       macd_signal_percentage  macd_hist_percentage  cci_24_percentage  \\\n",
       "0                    0.001840             -0.002316           0.001399   \n",
       "1                    0.001840             -0.002316           0.001399   \n",
       "2                    0.001840             -0.002316           0.001399   \n",
       "3                    0.001840             -0.002316           0.001399   \n",
       "4                    0.001840             -0.002316           0.001399   \n",
       "...                       ...                   ...                ...   \n",
       "26010                0.001675             -0.006731           0.000452   \n",
       "26011                0.002119             -0.017800           0.002359   \n",
       "26012                0.002336              0.002387           0.000632   \n",
       "26013                0.001399             -0.013418          -0.013633   \n",
       "26014               -0.000230              0.018382           0.032995   \n",
       "\n",
       "       mom_10_percentage  roc_10_percentage  rsi_5_percentage  \\\n",
       "0              -0.006634          -0.006591              -1.0   \n",
       "1              -0.006634          -0.006591              -1.0   \n",
       "2              -0.006634          -0.006591              -1.0   \n",
       "3              -0.006634          -0.006591              -1.0   \n",
       "4              -0.006634          -0.006591              -1.0   \n",
       "...                  ...                ...               ...   \n",
       "26010          -0.006640          -0.006597              -1.0   \n",
       "26011          -0.007460          -0.007413              -1.0   \n",
       "26012          -0.006634          -0.006591              -1.0   \n",
       "26013          -0.045152          -0.044959              -1.0   \n",
       "26014          -0.004873          -0.004839              -1.0   \n",
       "\n",
       "       wnr_9_percentage  slowk_percentage  slowd_percentage  adosc_percentage  \n",
       "0             -0.997032          0.000144         -0.001621          0.002114  \n",
       "1             -0.997032          0.000144         -0.001621          0.002114  \n",
       "2             -0.997032          0.000144         -0.001621          0.002114  \n",
       "3             -0.997032          0.000144         -0.001621          0.002114  \n",
       "4             -0.997032          0.000144         -0.001621          0.002114  \n",
       "...                 ...               ...               ...               ...  \n",
       "26010         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26011         -1.000000          0.000144         -0.001621          0.002114  \n",
       "26012         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26013         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26014         -0.994112          0.000144         -0.001621          0.002114  \n",
       "\n",
       "[26015 rows x 45 columns]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "independent_indicators = ['macd', 'macd_signal', 'macd_hist', 'cci_24', 'mom_10', 'roc_10','rsi_5','wnr_9','slowk','slowd','adosc'] \n",
    "for name, datad in data_sets.items():\n",
    "    data = datad['data']\n",
    "    for indicator in independent_indicators:\n",
    "        name = indicator + '_min_max'\n",
    "        mean = data[indicator].mean()\n",
    "        std = data[indicator].std()\n",
    "        data[indicator].loc[data[indicator] > mean + 3 * std] = mean + 3 * std\n",
    "        data[indicator].loc[data[indicator] < mean - 3 * std] = mean - 3 * std\n",
    "        data[name] = (data[indicator] - mean) / std \n",
    "        data[name] = minmax_scale(data[indicator], feature_range=(-1,1))\n",
    "data_sets['BTCUSDT']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>sma_10</th>\n",
       "      <th>macd</th>\n",
       "      <th>...</th>\n",
       "      <th>macd_signal_percentage</th>\n",
       "      <th>macd_hist_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>slowd_percentage</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17 04:00:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>1.775183</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17 04:01:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-17 04:02:00.000000</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>4280.56</td>\n",
       "      <td>0.261074</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-17 04:03:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-17 04:04:00.000000</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.140796</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>2017-09-04 05:30:00.000000</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>4462.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26010</td>\n",
       "      <td>0</td>\n",
       "      <td>4448.883</td>\n",
       "      <td>7.942480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>-0.006731</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>-0.006597</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>2017-09-04 05:31:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.198468</td>\n",
       "      <td>26011</td>\n",
       "      <td>0</td>\n",
       "      <td>4448.980</td>\n",
       "      <td>8.922450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>-0.017800</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>2017-09-04 05:32:00.000000</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26012</td>\n",
       "      <td>0</td>\n",
       "      <td>4449.077</td>\n",
       "      <td>9.592574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>2017-09-04 05:33:00.000000</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.95</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>4433.94</td>\n",
       "      <td>0.784977</td>\n",
       "      <td>26013</td>\n",
       "      <td>0</td>\n",
       "      <td>4446.500</td>\n",
       "      <td>7.534293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.013418</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>-0.044959</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>2017-09-04 05:34:00.000000</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4434.17</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>4401.45</td>\n",
       "      <td>1.211797</td>\n",
       "      <td>26014</td>\n",
       "      <td>1</td>\n",
       "      <td>4440.674</td>\n",
       "      <td>3.394458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.004839</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.994112</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Date     Open     High      Low    Close  \\\n",
       "0      2017-08-17 04:00:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "1      2017-08-17 04:01:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "2      2017-08-17 04:02:00.000000  4280.56  4280.56  4280.56  4280.56   \n",
       "3      2017-08-17 04:03:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "4      2017-08-17 04:04:00.000000  4261.48  4261.48  4261.48  4261.48   \n",
       "...                           ...      ...      ...      ...      ...   \n",
       "26010  2017-09-04 05:30:00.000000  4462.50  4462.50  4462.50  4462.50   \n",
       "26011  2017-09-04 05:31:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26012  2017-09-04 05:32:00.000000  4466.97  4466.97  4466.97  4466.97   \n",
       "26013  2017-09-04 05:33:00.000000  4433.95  4433.95  4433.94  4433.94   \n",
       "26014  2017-09-04 05:34:00.000000  4434.17  4434.17  4401.45  4401.45   \n",
       "\n",
       "         Volume  index  label    sma_10      macd  ...  \\\n",
       "0      1.775183      0      1       NaN       NaN  ...   \n",
       "1      0.000000      1      1       NaN       NaN  ...   \n",
       "2      0.261074      2      1       NaN       NaN  ...   \n",
       "3      0.012008      3      1       NaN       NaN  ...   \n",
       "4      0.140796      4      1       NaN       NaN  ...   \n",
       "...         ...    ...    ...       ...       ...  ...   \n",
       "26010  0.000000  26010      0  4448.883  7.942480  ...   \n",
       "26011  0.198468  26011      0  4448.980  8.922450  ...   \n",
       "26012  0.000000  26012      0  4449.077  9.592574  ...   \n",
       "26013  0.784977  26013      0  4446.500  7.534293  ...   \n",
       "26014  1.211797  26014      1  4440.674  3.394458  ...   \n",
       "\n",
       "       macd_signal_percentage  macd_hist_percentage  cci_24_percentage  \\\n",
       "0                    0.001840             -0.002316           0.001399   \n",
       "1                    0.001840             -0.002316           0.001399   \n",
       "2                    0.001840             -0.002316           0.001399   \n",
       "3                    0.001840             -0.002316           0.001399   \n",
       "4                    0.001840             -0.002316           0.001399   \n",
       "...                       ...                   ...                ...   \n",
       "26010                0.001675             -0.006731           0.000452   \n",
       "26011                0.002119             -0.017800           0.002359   \n",
       "26012                0.002336              0.002387           0.000632   \n",
       "26013                0.001399             -0.013418          -0.013633   \n",
       "26014               -0.000230              0.018382           0.032995   \n",
       "\n",
       "       mom_10_percentage  roc_10_percentage  rsi_5_percentage  \\\n",
       "0              -0.006634          -0.006591              -1.0   \n",
       "1              -0.006634          -0.006591              -1.0   \n",
       "2              -0.006634          -0.006591              -1.0   \n",
       "3              -0.006634          -0.006591              -1.0   \n",
       "4              -0.006634          -0.006591              -1.0   \n",
       "...                  ...                ...               ...   \n",
       "26010          -0.006640          -0.006597              -1.0   \n",
       "26011          -0.007460          -0.007413              -1.0   \n",
       "26012          -0.006634          -0.006591              -1.0   \n",
       "26013          -0.045152          -0.044959              -1.0   \n",
       "26014          -0.004873          -0.004839              -1.0   \n",
       "\n",
       "       wnr_9_percentage  slowk_percentage  slowd_percentage  adosc_percentage  \n",
       "0             -0.997032          0.000144         -0.001621          0.002114  \n",
       "1             -0.997032          0.000144         -0.001621          0.002114  \n",
       "2             -0.997032          0.000144         -0.001621          0.002114  \n",
       "3             -0.997032          0.000144         -0.001621          0.002114  \n",
       "4             -0.997032          0.000144         -0.001621          0.002114  \n",
       "...                 ...               ...               ...               ...  \n",
       "26010         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26011         -1.000000          0.000144         -0.001621          0.002114  \n",
       "26012         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26013         -0.997032          0.000144         -0.001621          0.002114  \n",
       "26014         -0.994112          0.000144         -0.001621          0.002114  \n",
       "\n",
       "[26015 rows x 45 columns]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_indicators = ['Close', 'Volume', 'sma_10'] + independent_indicators\n",
    "for name, datad in data_sets.items():\n",
    "    data = datad['data']\n",
    "    for indicator in percentage_indicators:\n",
    "        name = indicator + '_percentage'\n",
    "        data[name] = data[indicator].pct_change().replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        mean = data[name].mean()\n",
    "        std = data[name].std()\n",
    "        data[name].loc[data[name] > mean + 3 * std] = mean + 3 * std\n",
    "        data[name].loc[data[name] < mean - 3 * std] = mean - 3 * std\n",
    "        data[name] = (data[name] - mean) / std \n",
    "        data[name] = minmax_scale(data[name], feature_range=(-1, 1))\n",
    "data_sets['BTCUSDT']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macd_min_max</th>\n",
       "      <th>macd_signal_min_max</th>\n",
       "      <th>macd_hist_min_max</th>\n",
       "      <th>cci_24_min_max</th>\n",
       "      <th>mom_10_min_max</th>\n",
       "      <th>roc_10_min_max</th>\n",
       "      <th>rsi_5_min_max</th>\n",
       "      <th>wnr_9_min_max</th>\n",
       "      <th>slowk_min_max</th>\n",
       "      <th>slowd_min_max</th>\n",
       "      <th>...</th>\n",
       "      <th>macd_signal_percentage</th>\n",
       "      <th>macd_hist_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>slowd_percentage</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>0.375231</td>\n",
       "      <td>0.425596</td>\n",
       "      <td>-0.042448</td>\n",
       "      <td>0.218187</td>\n",
       "      <td>0.028397</td>\n",
       "      <td>0.025506</td>\n",
       "      <td>0.245191</td>\n",
       "      <td>0.874826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>-0.006731</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>-0.006597</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>0.421786</td>\n",
       "      <td>0.432460</td>\n",
       "      <td>0.070574</td>\n",
       "      <td>0.236779</td>\n",
       "      <td>0.011126</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>-0.017800</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>0.453621</td>\n",
       "      <td>0.444894</td>\n",
       "      <td>0.127932</td>\n",
       "      <td>0.220684</td>\n",
       "      <td>0.011126</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>0.355839</td>\n",
       "      <td>0.433519</td>\n",
       "      <td>-0.117287</td>\n",
       "      <td>-0.073693</td>\n",
       "      <td>-0.318760</td>\n",
       "      <td>-0.299752</td>\n",
       "      <td>-0.279231</td>\n",
       "      <td>0.080840</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>-0.013418</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>-0.044959</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>0.159170</td>\n",
       "      <td>0.381533</td>\n",
       "      <td>-0.535554</td>\n",
       "      <td>-0.270454</td>\n",
       "      <td>-0.719582</td>\n",
       "      <td>-0.675293</td>\n",
       "      <td>-0.533946</td>\n",
       "      <td>-0.823292</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.004839</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.994112</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       macd_min_max  macd_signal_min_max  macd_hist_min_max  cci_24_min_max  \\\n",
       "0          0.000000             0.000000           0.000000        0.000000   \n",
       "1          0.000000             0.000000           0.000000        0.000000   \n",
       "2          0.000000             0.000000           0.000000        0.000000   \n",
       "3          0.000000             0.000000           0.000000        0.000000   \n",
       "4          0.000000             0.000000           0.000000        0.000000   \n",
       "...             ...                  ...                ...             ...   \n",
       "26010      0.375231             0.425596          -0.042448        0.218187   \n",
       "26011      0.421786             0.432460           0.070574        0.236779   \n",
       "26012      0.453621             0.444894           0.127932        0.220684   \n",
       "26013      0.355839             0.433519          -0.117287       -0.073693   \n",
       "26014      0.159170             0.381533          -0.535554       -0.270454   \n",
       "\n",
       "       mom_10_min_max  roc_10_min_max  rsi_5_min_max  wnr_9_min_max  \\\n",
       "0            0.000000        0.000000       0.000000       0.000000   \n",
       "1            0.000000        0.000000       0.000000       0.000000   \n",
       "2            0.000000        0.000000       0.000000       0.000000   \n",
       "3            0.000000        0.000000       0.000000       0.000000   \n",
       "4            0.000000        0.000000       0.000000       0.000000   \n",
       "...               ...             ...            ...            ...   \n",
       "26010        0.028397        0.025506       0.245191       0.874826   \n",
       "26011        0.011126        0.009311       0.297502       1.000000   \n",
       "26012        0.011126        0.009311       0.297502       1.000000   \n",
       "26013       -0.318760       -0.299752      -0.279231       0.080840   \n",
       "26014       -0.719582       -0.675293      -0.533946      -0.823292   \n",
       "\n",
       "       slowk_min_max  slowd_min_max  ...  macd_signal_percentage  \\\n",
       "0           0.000000       0.000000  ...                0.001840   \n",
       "1           0.000000       0.000000  ...                0.001840   \n",
       "2           0.000000       0.000000  ...                0.001840   \n",
       "3           0.000000       0.000000  ...                0.001840   \n",
       "4           0.000000       0.000000  ...                0.001840   \n",
       "...              ...            ...  ...                     ...   \n",
       "26010       1.000000       0.333333  ...                0.001675   \n",
       "26011       1.000000       0.777778  ...                0.002119   \n",
       "26012       1.000000       1.000000  ...                0.002336   \n",
       "26013       0.333333       0.777778  ...                0.001399   \n",
       "26014      -0.333333       0.333333  ...               -0.000230   \n",
       "\n",
       "       macd_hist_percentage  cci_24_percentage  mom_10_percentage  \\\n",
       "0                 -0.002316           0.001399          -0.006634   \n",
       "1                 -0.002316           0.001399          -0.006634   \n",
       "2                 -0.002316           0.001399          -0.006634   \n",
       "3                 -0.002316           0.001399          -0.006634   \n",
       "4                 -0.002316           0.001399          -0.006634   \n",
       "...                     ...                ...                ...   \n",
       "26010             -0.006731           0.000452          -0.006640   \n",
       "26011             -0.017800           0.002359          -0.007460   \n",
       "26012              0.002387           0.000632          -0.006634   \n",
       "26013             -0.013418          -0.013633          -0.045152   \n",
       "26014              0.018382           0.032995          -0.004873   \n",
       "\n",
       "       roc_10_percentage  rsi_5_percentage  wnr_9_percentage  \\\n",
       "0              -0.006591              -1.0         -0.997032   \n",
       "1              -0.006591              -1.0         -0.997032   \n",
       "2              -0.006591              -1.0         -0.997032   \n",
       "3              -0.006591              -1.0         -0.997032   \n",
       "4              -0.006591              -1.0         -0.997032   \n",
       "...                  ...               ...               ...   \n",
       "26010          -0.006597              -1.0         -0.997032   \n",
       "26011          -0.007413              -1.0         -1.000000   \n",
       "26012          -0.006591              -1.0         -0.997032   \n",
       "26013          -0.044959              -1.0         -0.997032   \n",
       "26014          -0.004839              -1.0         -0.994112   \n",
       "\n",
       "       slowk_percentage  slowd_percentage  adosc_percentage  \n",
       "0              0.000144         -0.001621          0.002114  \n",
       "1              0.000144         -0.001621          0.002114  \n",
       "2              0.000144         -0.001621          0.002114  \n",
       "3              0.000144         -0.001621          0.002114  \n",
       "4              0.000144         -0.001621          0.002114  \n",
       "...                 ...               ...               ...  \n",
       "26010          0.000144         -0.001621          0.002114  \n",
       "26011          0.000144         -0.001621          0.002114  \n",
       "26012          0.000144         -0.001621          0.002114  \n",
       "26013          0.000144         -0.001621          0.002114  \n",
       "26014          0.000144         -0.001621          0.002114  \n",
       "\n",
       "[26015 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "26010    0\n",
       "26011    0\n",
       "26012    0\n",
       "26013    0\n",
       "26014    1\n",
       "Name: label, Length: 26015, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "INVALID_LABEL = 99\n",
    "\n",
    "dependent_indicators = ['Open','High','Low','Close','Volume', 'sma_10']\n",
    "\n",
    "for name, datad in data_sets.items():\n",
    "    data = datad['data']\n",
    "    datad['features'] = data.copy().drop(['index', 'Date', 'label'] + dependent_indicators + independent_indicators, axis=1\n",
    "                               ).fillna(0).replace([np.inf, -np.inf], np.nan).ffill()\n",
    "\n",
    "    display(datad['features'])\n",
    "\n",
    "    datad['labels'] = data['label'].copy().replace([np.inf, -np.inf], np.nan).fillna(INVALID_LABEL)\n",
    "\n",
    "    display(datad['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT\n",
      "[14 11  1 12  6  5 22 23  9 10 15  2  3  7 20  8 16 21 13 18 24 19 17  4\n",
      " 25]\n",
      "[14, 11, 1, 12, 6, 5, 22, 23, 9, 10, 15, 2, 3, 7, 20, 8, 16, 21, 13, 18, 24, 19, 17, 4, 25]\n",
      "\n",
      "[ 1  2 23  5 10  9 22 25  7  8 13  3  4 11 16  6 17 24 18 15 20 14 21 12\n",
      " 19]\n",
      "[15, 13, 24, 17, 16, 14, 44, 48, 16, 18, 28, 5, 7, 18, 36, 14, 33, 45, 31, 33, 44, 33, 38, 16, 44]\n",
      "\n",
      "[ 4  5  7 15  2  1 20 19  9 11 23  6 13  3 16 10 12 14 18 21 24 17 22  8\n",
      " 25]\n",
      "[19, 18, 31, 32, 18, 15, 64, 67, 25, 29, 51, 11, 20, 21, 52, 24, 45, 59, 49, 54, 68, 50, 60, 24, 69]\n",
      "\n",
      "[ 5  6 22 10  3 17 19 18 12 13 21  1 24 20  2 15  4  7 23 16 11 14  9  8\n",
      " 25]\n",
      "[24, 24, 53, 42, 21, 32, 83, 85, 37, 42, 72, 12, 44, 41, 54, 39, 49, 66, 72, 70, 79, 64, 69, 32, 94]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>adosc_min_max</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>rsi_5_min_max</th>\n",
       "      <th>wnr_9_min_max</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006597</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>0.411670</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.245191</td>\n",
       "      <td>0.874826</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>0.336225</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.274284</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>-0.997032</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.044959</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>0.134002</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.279231</td>\n",
       "      <td>0.080840</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>-0.994112</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.004839</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.074237</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.533946</td>\n",
       "      <td>-0.823292</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       wnr_9_percentage  cci_24_percentage  slowk_percentage  \\\n",
       "0             -0.997032           0.001399          0.000144   \n",
       "1             -0.997032           0.001399          0.000144   \n",
       "2             -0.997032           0.001399          0.000144   \n",
       "3             -0.997032           0.001399          0.000144   \n",
       "4             -0.997032           0.001399          0.000144   \n",
       "...                 ...                ...               ...   \n",
       "26010         -0.997032           0.000452          0.000144   \n",
       "26011         -1.000000           0.002359          0.000144   \n",
       "26012         -0.997032           0.000632          0.000144   \n",
       "26013         -0.997032          -0.013633          0.000144   \n",
       "26014         -0.994112           0.032995          0.000144   \n",
       "\n",
       "       roc_10_percentage  mom_10_percentage  adosc_min_max  rsi_5_percentage  \\\n",
       "0              -0.006591          -0.006634       0.000000              -1.0   \n",
       "1              -0.006591          -0.006634       0.000000              -1.0   \n",
       "2              -0.006591          -0.006634       0.000000              -1.0   \n",
       "3              -0.006591          -0.006634       0.000000              -1.0   \n",
       "4              -0.006591          -0.006634       0.000000              -1.0   \n",
       "...                  ...                ...            ...               ...   \n",
       "26010          -0.006597          -0.006640       0.411670              -1.0   \n",
       "26011          -0.007413          -0.007460       0.336225              -1.0   \n",
       "26012          -0.006591          -0.006634       0.274284              -1.0   \n",
       "26013          -0.044959          -0.045152       0.134002              -1.0   \n",
       "26014          -0.004839          -0.004873      -0.074237              -1.0   \n",
       "\n",
       "       rsi_5_min_max  wnr_9_min_max  adosc_percentage  \n",
       "0           0.000000       0.000000          0.002114  \n",
       "1           0.000000       0.000000          0.002114  \n",
       "2           0.000000       0.000000          0.002114  \n",
       "3           0.000000       0.000000          0.002114  \n",
       "4           0.000000       0.000000          0.002114  \n",
       "...              ...            ...               ...  \n",
       "26010       0.245191       0.874826          0.002114  \n",
       "26011       0.297502       1.000000          0.002114  \n",
       "26012       0.297502       1.000000          0.002114  \n",
       "26013      -0.279231       0.080840          0.002114  \n",
       "26014      -0.533946      -0.823292          0.002114  \n",
       "\n",
       "[26015 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_components = 10\n",
    "def get_pca_features(features, pca_components=10):\n",
    "\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    features_pca = pd.DataFrame(pca.fit_transform(features))\n",
    "    return features_pca\n",
    "\n",
    "def rfe(features, labels, sample_size=2000, trials=4, select=10):\n",
    "    estimator = SVR(kernel=\"linear\")\n",
    "    selector = RFE(estimator, n_features_to_select=1)#, step=1)\n",
    "    sums = [0] * len(features.columns)\n",
    "    for trial in range(trials):\n",
    "        samples = features.sample(n=sample_size)\n",
    "        selector = selector.fit(samples, labels.loc[samples.index])\n",
    "        print(selector.ranking_)\n",
    "        sums = [sum(i) for i in zip(sums, selector.ranking_)]\n",
    "        print(sums)\n",
    "        print()\n",
    "    \n",
    "    return features[[features.columns[i] for i in np.argsort(sums)[-select:] ]]\n",
    "\n",
    "for name, datad in data_sets.items():\n",
    "    features = datad['features']\n",
    "    labels = datad['labels']\n",
    "    print(name)\n",
    "    datad['rfe_features'] = rfe(features, labels, sample_size=2000, trials=4, select=10)\n",
    "    display(datad['rfe_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>-0.016525</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>-0.002023</td>\n",
       "      <td>0.002334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>-0.016525</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>-0.002023</td>\n",
       "      <td>0.002334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>-0.016525</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>-0.002023</td>\n",
       "      <td>0.002334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>-0.016525</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>-0.002023</td>\n",
       "      <td>0.002334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>-0.016525</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>-0.002023</td>\n",
       "      <td>0.002334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>-0.911024</td>\n",
       "      <td>-0.248434</td>\n",
       "      <td>0.247482</td>\n",
       "      <td>-0.008016</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>-0.004168</td>\n",
       "      <td>0.001292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>-1.038927</td>\n",
       "      <td>-0.214203</td>\n",
       "      <td>0.166401</td>\n",
       "      <td>-0.007982</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>-0.004251</td>\n",
       "      <td>0.003328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>-1.033032</td>\n",
       "      <td>-0.187945</td>\n",
       "      <td>0.110677</td>\n",
       "      <td>-0.003779</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>-0.003971</td>\n",
       "      <td>0.001680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>0.050001</td>\n",
       "      <td>-0.308196</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>-0.026729</td>\n",
       "      <td>0.049920</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>-0.012371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>0.994709</td>\n",
       "      <td>-0.133002</td>\n",
       "      <td>-0.037839</td>\n",
       "      <td>-0.022068</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>-0.003099</td>\n",
       "      <td>0.033386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6\n",
       "0      0.022396  0.010892  0.010192 -0.016525 -0.002421 -0.002023  0.002334\n",
       "1      0.022396  0.010892  0.010192 -0.016525 -0.002421 -0.002023  0.002334\n",
       "2      0.022396  0.010892  0.010192 -0.016525 -0.002421 -0.002023  0.002334\n",
       "3      0.022396  0.010892  0.010192 -0.016525 -0.002421 -0.002023  0.002334\n",
       "4      0.022396  0.010892  0.010192 -0.016525 -0.002421 -0.002023  0.002334\n",
       "...         ...       ...       ...       ...       ...       ...       ...\n",
       "26010 -0.911024 -0.248434  0.247482 -0.008016 -0.000371 -0.004168  0.001292\n",
       "26011 -1.038927 -0.214203  0.166401 -0.007982  0.000853 -0.004251  0.003328\n",
       "26012 -1.033032 -0.187945  0.110677 -0.003779  0.000768 -0.003971  0.001680\n",
       "26013  0.050001 -0.308196  0.011966 -0.026729  0.049920 -0.001053 -0.012371\n",
       "26014  0.994709 -0.133002 -0.037839 -0.022068 -0.007926 -0.003099  0.033386\n",
       "\n",
       "[26015 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name, datad in data_sets.items():\n",
    "    rfe_features = datad['rfe_features']\n",
    "    datad['rfe_pca_features'] = get_pca_features(rfe_features, pca_components=7)\n",
    "    display(datad['rfe_pca_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>wnr_9_percentage</th>\n",
       "      <th>cci_24_percentage</th>\n",
       "      <th>slowk_percentage</th>\n",
       "      <th>roc_10_percentage</th>\n",
       "      <th>mom_10_percentage</th>\n",
       "      <th>adosc_min_max</th>\n",
       "      <th>rsi_5_percentage</th>\n",
       "      <th>rsi_5_min_max</th>\n",
       "      <th>wnr_9_min_max</th>\n",
       "      <th>adosc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>1.775183</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4280.56</td>\n",
       "      <td>0.261074</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>0.140796</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26010</th>\n",
       "      <td>4462.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006597</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>0.411670</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.245191</td>\n",
       "      <td>0.874826</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26011</th>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.198468</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-0.007460</td>\n",
       "      <td>0.336225</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>4466.97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006634</td>\n",
       "      <td>0.274284</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>4433.94</td>\n",
       "      <td>0.784977</td>\n",
       "      <td>-0.997032</td>\n",
       "      <td>-0.013633</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.044959</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>0.134002</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.279231</td>\n",
       "      <td>0.080840</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>4401.45</td>\n",
       "      <td>1.211797</td>\n",
       "      <td>-0.994112</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.004839</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>-0.074237</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.533946</td>\n",
       "      <td>-0.823292</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26015 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Close    Volume  wnr_9_percentage  cci_24_percentage  \\\n",
       "0      4261.48  1.775183         -0.997032           0.001399   \n",
       "1      4261.48  0.000000         -0.997032           0.001399   \n",
       "2      4280.56  0.261074         -0.997032           0.001399   \n",
       "3      4261.48  0.012008         -0.997032           0.001399   \n",
       "4      4261.48  0.140796         -0.997032           0.001399   \n",
       "...        ...       ...               ...                ...   \n",
       "26010  4462.50  0.000000         -0.997032           0.000452   \n",
       "26011  4466.97  0.198468         -1.000000           0.002359   \n",
       "26012  4466.97  0.000000         -0.997032           0.000632   \n",
       "26013  4433.94  0.784977         -0.997032          -0.013633   \n",
       "26014  4401.45  1.211797         -0.994112           0.032995   \n",
       "\n",
       "       slowk_percentage  roc_10_percentage  mom_10_percentage  adosc_min_max  \\\n",
       "0              0.000144          -0.006591          -0.006634       0.000000   \n",
       "1              0.000144          -0.006591          -0.006634       0.000000   \n",
       "2              0.000144          -0.006591          -0.006634       0.000000   \n",
       "3              0.000144          -0.006591          -0.006634       0.000000   \n",
       "4              0.000144          -0.006591          -0.006634       0.000000   \n",
       "...                 ...                ...                ...            ...   \n",
       "26010          0.000144          -0.006597          -0.006640       0.411670   \n",
       "26011          0.000144          -0.007413          -0.007460       0.336225   \n",
       "26012          0.000144          -0.006591          -0.006634       0.274284   \n",
       "26013          0.000144          -0.044959          -0.045152       0.134002   \n",
       "26014          0.000144          -0.004839          -0.004873      -0.074237   \n",
       "\n",
       "       rsi_5_percentage  rsi_5_min_max  wnr_9_min_max  adosc_percentage  \n",
       "0                  -1.0       0.000000       0.000000          0.002114  \n",
       "1                  -1.0       0.000000       0.000000          0.002114  \n",
       "2                  -1.0       0.000000       0.000000          0.002114  \n",
       "3                  -1.0       0.000000       0.000000          0.002114  \n",
       "4                  -1.0       0.000000       0.000000          0.002114  \n",
       "...                 ...            ...            ...               ...  \n",
       "26010              -1.0       0.245191       0.874826          0.002114  \n",
       "26011              -1.0       0.297502       1.000000          0.002114  \n",
       "26012              -1.0       0.297502       1.000000          0.002114  \n",
       "26013              -1.0      -0.279231       0.080840          0.002114  \n",
       "26014              -1.0      -0.533946      -0.823292          0.002114  \n",
       "\n",
       "[26015 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26015\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[tensor(4113.5898), tensor(0.3663), tensor(-0...</td>\n",
       "      <td>[tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[tensor(4109.9102), tensor(2.2514), tensor(-1...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[tensor(4808.1001), tensor(0.), tensor(-1.000...</td>\n",
       "      <td>[tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[tensor(4014.9900), tensor(0.), tensor(-1.), ...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[tensor(4422.9902), tensor(0.4367), tensor(-0...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15584</th>\n",
       "      <td>[[tensor(4298.8198), tensor(1.3552), tensor(-0...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15585</th>\n",
       "      <td>[[tensor(4166.0200), tensor(0.0482), tensor(-0...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15586</th>\n",
       "      <td>[[tensor(4169.1802), tensor(0.), tensor(-0.997...</td>\n",
       "      <td>[tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15587</th>\n",
       "      <td>[[tensor(3884.9900), tensor(2.6510), tensor(-1...</td>\n",
       "      <td>[tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15588</th>\n",
       "      <td>[[tensor(4291.3799), tensor(0.), tensor(-0.997...</td>\n",
       "      <td>[tensor(2.)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15589 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0             1\n",
       "0      [[tensor(4113.5898), tensor(0.3663), tensor(-0...  [tensor(1.)]\n",
       "1      [[tensor(4109.9102), tensor(2.2514), tensor(-1...  [tensor(0.)]\n",
       "2      [[tensor(4808.1001), tensor(0.), tensor(-1.000...  [tensor(1.)]\n",
       "3      [[tensor(4014.9900), tensor(0.), tensor(-1.), ...  [tensor(0.)]\n",
       "4      [[tensor(4422.9902), tensor(0.4367), tensor(-0...  [tensor(0.)]\n",
       "...                                                  ...           ...\n",
       "15584  [[tensor(4298.8198), tensor(1.3552), tensor(-0...  [tensor(0.)]\n",
       "15585  [[tensor(4166.0200), tensor(0.0482), tensor(-0...  [tensor(0.)]\n",
       "15586  [[tensor(4169.1802), tensor(0.), tensor(-0.997...  [tensor(1.)]\n",
       "15587  [[tensor(3884.9900), tensor(2.6510), tensor(-1...  [tensor(0.)]\n",
       "15588  [[tensor(4291.3799), tensor(0.), tensor(-0.997...  [tensor(2.)]\n",
       "\n",
       "[15589 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_is_copy': None, '_mgr': BlockManager\n",
      "Items: RangeIndex(start=0, stop=2, step=1)\n",
      "Axis 1: RangeIndex(start=0, stop=15589, step=1)\n",
      "ObjectBlock: slice(0, 2, 1), 2 x 15589, dtype: object, '_item_cache': {1: 0        [tensor(1.)]\n",
      "1        [tensor(0.)]\n",
      "2        [tensor(1.)]\n",
      "3        [tensor(0.)]\n",
      "4        [tensor(0.)]\n",
      "             ...     \n",
      "15584    [tensor(0.)]\n",
      "15585    [tensor(0.)]\n",
      "15586    [tensor(1.)]\n",
      "15587    [tensor(0.)]\n",
      "15588    [tensor(2.)]\n",
      "Name: 1, Length: 15589, dtype: object}, '_attrs': {}}\n",
      "0 5608\n",
      "1 7848\n",
      "2 2133\n",
      "\n",
      "0 2133\n",
      "1 2133\n",
      "2 2133\n",
      "\n",
      "6399 3903 6503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4.4230e+03,  4.3666e-01, -9.9767e-01, -2.4153e-03,  1.4409e-04,\n",
       "          -6.9807e-03, -7.0257e-03, -5.4199e-01, -1.0000e+00, -2.3937e-01,\n",
       "          -5.6744e-01,  2.1143e-03],\n",
       "         [ 4.4230e+03,  0.0000e+00, -9.9703e-01,  1.1331e-03,  1.4409e-04,\n",
       "          -6.5912e-03, -6.6341e-03, -5.2106e-01, -1.0000e+00, -2.3937e-01,\n",
       "          -5.6744e-01,  2.1143e-03],\n",
       "         [ 4.4175e+03,  4.3700e-01, -9.9693e-01,  1.1757e-03,  1.4409e-04,\n",
       "          -6.5116e-03, -6.5551e-03, -4.6565e-01, -1.0000e+00, -3.1132e-01,\n",
       "          -6.2237e-01,  2.1143e-03],\n",
       "         [ 4.4176e+03,  1.8284e+00, -9.9704e-01,  8.5315e-04,  1.4409e-04,\n",
       "          -6.5999e-03, -6.6430e-03, -1.9269e-01, -1.0000e+00, -3.0907e-01,\n",
       "          -6.1995e-01,  2.1143e-03],\n",
       "         [ 4.4040e+03,  1.8238e-01, -9.9634e-01,  2.0909e-03,  1.4409e-04,\n",
       "          -6.2017e-03, -6.2426e-03, -6.4026e-02, -1.0000e+00, -4.9346e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.4040e+03,  5.0068e-01, -9.9703e-01,  6.0560e-05,  1.4409e-04,\n",
       "          -7.0473e-03, -7.0970e-03, -6.0799e-03, -1.0000e+00, -4.9346e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.4040e+03,  1.7212e+00, -9.9703e-01, -7.1477e-04,  1.4409e-04,\n",
       "          -5.7438e-03, -5.7694e-03, -1.7857e-01, -1.0000e+00, -4.9346e-01,\n",
       "          -1.0000e+00,  2.1151e-03],\n",
       "         [ 4.4000e+03,  2.4238e+00, -9.9703e-01,  2.2921e-03,  1.4409e-04,\n",
       "          -6.9935e-03, -7.0438e-03, -5.0974e-01, -1.0000e+00, -5.6092e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3848e+03,  4.7832e-01, -9.9703e-01,  4.1214e-03,  1.4409e-04,\n",
       "          -7.3824e-03, -7.4353e-03, -5.9940e-01, -1.0000e+00, -7.3113e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3808e+03,  1.6289e+00, -9.9703e-01, -9.5882e-04,  1.4409e-04,\n",
       "          -6.3192e-03, -6.3608e-03, -7.6780e-01, -1.0000e+00, -7.6125e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.4700e+03,  1.4255e+00, -1.0000e+00, -1.3769e-02, -1.0875e-01,\n",
       "          -1.0667e-02, -1.0743e-02, -6.0488e-01, -1.0000e+00,  5.7574e-01,\n",
       "           1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3753e+03,  6.7334e-01, -9.9703e-01, -5.3213e-02,  1.4409e-04,\n",
       "          -9.3920e-03, -9.4498e-03, -5.6052e-01, -1.0000e+00, -2.1510e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3753e+03,  1.5108e-01, -9.9703e-01,  1.0506e-03,  1.4409e-04,\n",
       "          -6.7503e-03, -6.7956e-03, -4.9193e-01, -1.0000e+00, -2.1502e-01,\n",
       "          -9.9979e-01,  2.1143e-03],\n",
       "         [ 4.3753e+03,  0.0000e+00, -9.9703e-01,  6.8612e-04,  1.4409e-04,\n",
       "          -6.5886e-03, -6.6315e-03, -4.1966e-01, -1.0000e+00, -2.1502e-01,\n",
       "          -9.9979e-01,  2.1143e-03],\n",
       "         [ 4.3753e+03,  0.0000e+00, -9.9703e-01,  6.9245e-04,  1.4409e-04,\n",
       "          -7.0348e-03, -7.0830e-03, -3.5245e-01, -1.0000e+00, -2.1502e-01,\n",
       "          -9.9979e-01,  2.1143e-03],\n",
       "         [ 4.3780e+03,  5.8117e-01, -9.9712e-01,  3.4638e-04,  1.4409e-04,\n",
       "          -6.7206e-03, -6.7643e-03, -2.2717e-01, -1.0000e+00, -1.7442e-01,\n",
       "          -9.4336e-01,  2.1143e-03],\n",
       "         [ 4.3753e+03,  1.1227e+00, -9.9695e-01,  1.3704e-03,  1.4409e-04,\n",
       "          -6.4479e-03, -6.4901e-03, -2.8378e-01, -1.0000e+00, -2.0764e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3753e+03,  0.0000e+00, -9.9703e-01,  9.6559e-04,  1.4409e-04,\n",
       "          -6.7839e-03, -6.8290e-03, -2.8165e-01, -1.0000e+00, -2.0764e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3286e+03,  7.3088e+00, -9.9703e-01,  1.0774e-02,  1.4409e-04,\n",
       "          -4.8079e-03, -4.8525e-03, -1.0000e+00, -1.0000e+00, -6.2180e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 4.3286e+03,  0.0000e+00, -9.9703e-01,  2.6206e-03,  1.4409e-04,\n",
       "          -6.6880e-03, -6.7327e-03, -1.0000e+00, -1.0000e+00, -6.2180e-01,\n",
       "          -1.0000e+00,  2.1143e-03]]), tensor([0.]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch, random\n",
    "def create_inout_sequences(input_data, input_labels, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = torch.FloatTensor(input_data[i:i+tw])\n",
    "        train_label = torch.FloatTensor([input_labels[i+tw - 1]])\n",
    "        if train_label == INVALID_LABEL or torch.isnan(train_seq).any():\n",
    "            continue\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return inout_seq\n",
    "\n",
    "def get_sequenced_train_val_test(features, datad, train_window=20):\n",
    "    data = datad['data']\n",
    "    labels = datad['labels']\n",
    "    all = create_inout_sequences(features.values, list(labels), train_window)\n",
    "    random.shuffle(all)\n",
    "#     display(all)\n",
    "    train = all[:-math.floor(len(features)/2.5)]\n",
    "    validate = all[-math.floor(len(features)/2.5):-math.floor(len(features)/4)]\n",
    "    test = all[-math.floor(len(features)/4):]\n",
    "\n",
    "    tdf = pd.DataFrame(train)\n",
    "    display(tdf)\n",
    "    display(tdf[1].values[0])\n",
    "    print(tdf.__dict__)\n",
    "\n",
    "    \n",
    "\n",
    "    label_rows = {}\n",
    "    for label in sorted(list(data.label.unique())):\n",
    "        print(label, len(tdf.loc[tdf[1] == torch.tensor([label])]))\n",
    "        label_rows[label] = tdf.loc[tdf[1] == torch.tensor([label])]\n",
    "\n",
    "    \n",
    "    min_len = min(len(v) for k, v in label_rows.items())\n",
    "    min_key = [label for label in sorted(list(data.label.unique())) if len(label_rows[label]) == min_len]\n",
    "    print()\n",
    "    for label in sorted(list(data.label.unique())):\n",
    "        to_remove = np.random.choice(label_rows[label].index,size=len(label_rows[label]) - min_len,replace=False)\n",
    "        tdf = tdf.drop(to_remove)\n",
    "        print(label, len(tdf.loc[tdf[1] == torch.tensor([label])]))\n",
    "    print()\n",
    "\n",
    "    train = [tuple(r) for r in tdf.to_numpy()]\n",
    "    print(len(train), len(validate), len(test))\n",
    "    return train, validate, test\n",
    "\n",
    "for name, datad in data_sets.items():\n",
    "    print(name)\n",
    "    data = datad['data']\n",
    "    rfe_features = datad['rfe_features']\n",
    "    labels = datad['labels']\n",
    "    \n",
    "    chosen_indicators = rfe_features #features_pca\n",
    "    chosen_dependent = [ 'Close', 'Volume'] #dependent_indicators #'High', 'Low',\n",
    "    combined = pd.concat([data[chosen_dependent], chosen_indicators], axis=1)\n",
    "    display(combined)\n",
    "    print(len(labels))\n",
    "    train, validate, test = get_sequenced_train_val_test(combined, datad, train_window=train_window)\n",
    "    datad['train'] = train\n",
    "    datad['validate'] = validate\n",
    "    datad['test'] = test\n",
    "    display(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCUSDT\n",
      "6399 train took: 7.579445123672485\n",
      "3903 val took: 4.463438034057617\n",
      "6503 test took: 7.544755220413208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.3537e-01, -8.8051e-01, -9.9767e-01, -2.4153e-03,  1.4409e-04,\n",
       "          -6.9807e-03, -7.0257e-03, -5.4199e-01, -1.0000e+00, -2.3937e-01,\n",
       "          -5.6744e-01,  2.1143e-03],\n",
       "         [ 3.3537e-01, -1.0000e+00, -9.9703e-01,  1.1331e-03,  1.4409e-04,\n",
       "          -6.5912e-03, -6.6341e-03, -5.2106e-01, -1.0000e+00, -2.3937e-01,\n",
       "          -5.6744e-01,  2.1143e-03],\n",
       "         [ 2.5752e-01, -8.8042e-01, -9.9693e-01,  1.1757e-03,  1.4409e-04,\n",
       "          -6.5116e-03, -6.5551e-03, -4.6565e-01, -1.0000e+00, -3.1132e-01,\n",
       "          -6.2237e-01,  2.1143e-03],\n",
       "         [ 2.5865e-01, -4.9966e-01, -9.9704e-01,  8.5315e-04,  1.4409e-04,\n",
       "          -6.5999e-03, -6.6430e-03, -1.9269e-01, -1.0000e+00, -3.0907e-01,\n",
       "          -6.1995e-01,  2.1143e-03],\n",
       "         [ 6.6589e-02, -9.5009e-01, -9.9634e-01,  2.0909e-03,  1.4409e-04,\n",
       "          -6.2017e-03, -6.2426e-03, -6.4026e-02, -1.0000e+00, -4.9346e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 6.6589e-02, -8.6299e-01, -9.9703e-01,  6.0560e-05,  1.4409e-04,\n",
       "          -7.0473e-03, -7.0970e-03, -6.0799e-03, -1.0000e+00, -4.9346e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 6.6589e-02, -5.2901e-01, -9.9703e-01, -7.1477e-04,  1.4409e-04,\n",
       "          -5.7438e-03, -5.7694e-03, -1.7857e-01, -1.0000e+00, -4.9346e-01,\n",
       "          -1.0000e+00,  2.1151e-03],\n",
       "         [ 9.9792e-03, -3.3676e-01, -9.9703e-01,  2.2921e-03,  1.4409e-04,\n",
       "          -6.9935e-03, -7.0438e-03, -5.0974e-01, -1.0000e+00, -5.6092e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [-2.0529e-01, -8.6911e-01, -9.9703e-01,  4.1214e-03,  1.4409e-04,\n",
       "          -7.3824e-03, -7.4353e-03, -5.9940e-01, -1.0000e+00, -7.3113e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [-2.6134e-01, -5.5427e-01, -9.9703e-01, -9.5882e-04,  1.4409e-04,\n",
       "          -6.3192e-03, -6.3608e-03, -7.6780e-01, -1.0000e+00, -7.6125e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [ 1.0000e+00, -6.0991e-01, -1.0000e+00, -1.3769e-02, -1.0875e-01,\n",
       "          -1.0667e-02, -1.0743e-02, -6.0488e-01, -1.0000e+00,  5.7574e-01,\n",
       "           1.0000e+00,  2.1143e-03],\n",
       "         [-3.3933e-01, -8.1575e-01, -9.9703e-01, -5.3213e-02,  1.4409e-04,\n",
       "          -9.3920e-03, -9.4498e-03, -5.6052e-01, -1.0000e+00, -2.1510e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [-3.3918e-01, -9.5866e-01, -9.9703e-01,  1.0506e-03,  1.4409e-04,\n",
       "          -6.7503e-03, -6.7956e-03, -4.9193e-01, -1.0000e+00, -2.1502e-01,\n",
       "          -9.9979e-01,  2.1143e-03],\n",
       "         [-3.3918e-01, -1.0000e+00, -9.9703e-01,  6.8612e-04,  1.4409e-04,\n",
       "          -6.5886e-03, -6.6315e-03, -4.1966e-01, -1.0000e+00, -2.1502e-01,\n",
       "          -9.9979e-01,  2.1143e-03],\n",
       "         [-3.3918e-01, -1.0000e+00, -9.9703e-01,  6.9245e-04,  1.4409e-04,\n",
       "          -7.0348e-03, -7.0830e-03, -3.5245e-01, -1.0000e+00, -2.1502e-01,\n",
       "          -9.9979e-01,  2.1143e-03],\n",
       "         [-3.0140e-01, -8.4097e-01, -9.9712e-01,  3.4638e-04,  1.4409e-04,\n",
       "          -6.7206e-03, -6.7643e-03, -2.2717e-01, -1.0000e+00, -1.7442e-01,\n",
       "          -9.4336e-01,  2.1143e-03],\n",
       "         [-3.3933e-01, -6.9279e-01, -9.9695e-01,  1.3704e-03,  1.4409e-04,\n",
       "          -6.4479e-03, -6.4901e-03, -2.8378e-01, -1.0000e+00, -2.0764e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [-3.3933e-01, -1.0000e+00, -9.9703e-01,  9.6559e-04,  1.4409e-04,\n",
       "          -6.7839e-03, -6.8290e-03, -2.8165e-01, -1.0000e+00, -2.0764e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [-1.0000e+00,  1.0000e+00, -9.9703e-01,  1.0774e-02,  1.4409e-04,\n",
       "          -4.8079e-03, -4.8525e-03, -1.0000e+00, -1.0000e+00, -6.2180e-01,\n",
       "          -1.0000e+00,  2.1143e-03],\n",
       "         [-1.0000e+00, -1.0000e+00, -9.9703e-01,  2.6206e-03,  1.4409e-04,\n",
       "          -6.6880e-03, -6.7327e-03, -1.0000e+00, -1.0000e+00, -6.2180e-01,\n",
       "          -1.0000e+00,  2.1143e-03]]), tensor([0.]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_sequence_columns(sequence_label, range=(-1, 1), indices=list(range(len(chosen_dependent)))):\n",
    "    sequence, label = sequence_label\n",
    "        \n",
    "    input_df = pd.DataFrame(sequence, dtype=np.float32)\n",
    "    for index in indices:\n",
    "        input_df[index] = minmax_scale(input_df[index], feature_range=range)\n",
    "    seq = torch.FloatTensor(input_df.values)\n",
    "    label = torch.FloatTensor([label])\n",
    "    return (seq, label)\n",
    "    \n",
    "\n",
    "# data[indicator + '_percentage'] = ( (data[indicator + '_percentage'] - data[indicator + '_percentage'].min()) / \n",
    "#                                    (data[indicator + '_percentage'].max() - data[indicator + '_percentage'].min()) ) * (1 - -1) + -1 \n",
    "\n",
    "        \n",
    "# df = pd.DataFrame(train)\n",
    "# # display(df[0].tolist()[0:2])\n",
    "# df_t = pd.concat([df.drop(columns=0), pd.DataFrame(df[0].tolist(), index=df.index).add_prefix(0)], \n",
    "#                axis=1)   #pd.DataFrame(df[0].tolist())\n",
    "# display(df_t)\n",
    "# # df[3] = ( (df[0] - min(df[0])) / \n",
    "# #                    (max(df[0]) - min(df[0])) ) * (1 - -1) + -1 \n",
    "# # display(df)\n",
    "\n",
    "n = 2\n",
    "pool = multiprocessing.Pool(multiprocessing.cpu_count() - n)                         # Create a multiprocessing Pool\n",
    "attempts = 3\n",
    "for name, datad in data_sets.items():\n",
    "    print(name)\n",
    "    train = datad['train']\n",
    "    validate = datad['validate']\n",
    "    test = datad['test']\n",
    "    \n",
    "    for a in range(attempts):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            train = pool.map(normalize_sequence_columns, train.copy())\n",
    "            print(len(train), 'train took:', time.time() - start)\n",
    "            break\n",
    "        except RuntimeError:\n",
    "            print('train err ', a)\n",
    "            continue\n",
    "\n",
    "    for a in range(attempts):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            validate = pool.map(normalize_sequence_columns, validate.copy())\n",
    "            print(len(validate), 'val took:', time.time() - start)\n",
    "            break\n",
    "        except RuntimeError:\n",
    "            print('val err ', a)\n",
    "            pass\n",
    "\n",
    "    for a in range(attempts):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            test = pool.map(normalize_sequence_columns, test.copy())\n",
    "            print(len(test), 'test took:', time.time() - start)\n",
    "            break\n",
    "        except RuntimeError:\n",
    "            print('test err ', a)\n",
    "            pass\n",
    "    display(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules to build RunBuilder and RunManager helper classes\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "from IPython.display import clear_output\n",
    "import sklearn\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "global_labels = sorted(data_sets['BTCUSDT']['data'].label.unique())\n",
    "\n",
    "INVALID_LABEL = 99\n",
    "\n",
    "# Read in the hyper-parameters and return a Run namedtuple containing all the \n",
    "# combinations of hyper-parameters\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "\n",
    "        return runs\n",
    "\n",
    "class RunManager():\n",
    "    def __init__(self):\n",
    "        \n",
    "        def view(image):\n",
    "            return image.view(28*28)\n",
    "\n",
    "        compose_transforms = [\n",
    "            transforms.ToTensor(),\n",
    "            view\n",
    "        ]\n",
    "        \n",
    "        # Data sets to choose from\n",
    "#         self.data_sets = {\n",
    "# #             'sp500': {\n",
    "# #                 'train': data_sets['sp500']['train'],\n",
    "# #                 'validate': data_sets['sp500']['validate'],\n",
    "# #                 'test': data_sets['sp500']['test']\n",
    "# #             },\n",
    "#             'BTCUSDT': {\n",
    "#                 'train': data_sets['BTCUSDT']['train'],\n",
    "#                 'validate': data_sets['BTCUSDT']['validate'],\n",
    "#                 'test': data_sets['BTCUSDT']['test']\n",
    "#             }\n",
    "#         }\n",
    "        self.data_sets = {\n",
    "            'sp500': {\n",
    "                'data': pd.read_csv(f'/Users/gabeheim/documents/concatenated_price_data/sp500.csv', index_col=False).drop(['Adj Close'], axis=1)\n",
    "            },\n",
    "            'BTCUSDT': {\n",
    "                'data': pd.read_csv(f'/Users/gabeheim/documents/concatenated_price_data/BTCUSDT.csv', index_col=False)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        data_sets['BTCUSDT']['data'] = data_sets['BTCUSDT']['data'][:math.floor(len(data_sets['BTCUSDT']['data'])/32)]#16)]\n",
    "        \n",
    "        # tracking every epoch count, loss, accuracy, time\n",
    "        self.epoch_count = 0\n",
    "        self.epoch_loss = {'train': 0, 'validate': 0}\n",
    "        self.epoch_num_correct = {'train': {k: 0 for k in global_labels},\n",
    "                                  'validate': {k: 0 for k in global_labels}}\n",
    "        self.epoch_start_time = None\n",
    "\n",
    "        # tracking every run count, run data, hyper-params used, time\n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "        self.runs = pd.DataFrame()\n",
    "#         self.run_plot_statistics = {}\n",
    "        \n",
    "        # testing data\n",
    "        self.test_predictions = []\n",
    "        self.test_labels = []\n",
    "        self.test_correct_count = {k: 0 for k in global_labels}\n",
    "\n",
    "        # record model, loader and TensorBoard \n",
    "        self.network = None\n",
    "        \n",
    "    def get_labels(row, target_percent=.01, stop_loss_percent=.005, mode='since3'):\n",
    "    if mode == 'since':\n",
    "        try:\n",
    "            target_index = data.loc[row[6]:][data.Close >= row[4] * (1 + target_percent)].iloc[0]._name\n",
    "        except IndexError:\n",
    "            target_index = data.index.max() + 5\n",
    "        try:\n",
    "            stop_loss_index = data.loc[row[6]:][data.Close <= row[4] * (1 - stop_loss_percent)].iloc[0]._name\n",
    "        except IndexError:\n",
    "            stop_loss_index = data.index.max() + 5\n",
    "        if target_index > stop_loss_index:\n",
    "            return 0\n",
    "        elif target_index < stop_loss_index:\n",
    "            return 1\n",
    "        return None\n",
    "    if mode == 'average':\n",
    "        mean = data.Close[row[6]:row[6]+10].mean()\n",
    "#         print(mean)\n",
    "        if mean - row[4] > 0: #row[4] * (1 + target_percent):\n",
    "            return 1\n",
    "#         elif mean <= row[4] * (1 - stop_loss_percent):\n",
    "#             return -1\n",
    "        else:\n",
    "            return 0\n",
    "        return None\n",
    "    if mode == 'next':\n",
    "        try:\n",
    "            next = data.Close.values[row[6]+1]\n",
    "        except:\n",
    "            next = 0\n",
    "#         print(next)\n",
    "        if next > row[4]: #row[4] * (1 + target_percent):\n",
    "            return 1\n",
    "#         elif mean <= row[4] * (1 - stop_loss_percent):\n",
    "#             return -1\n",
    "        else:\n",
    "            return 0\n",
    "        return None\n",
    "    if mode == 'since3':\n",
    "#         max_index = data.High[row[6]:row[6]+21].idxmax()\n",
    "#         min_index = data.Low[row[6]:row[6]+21].idxmin()\n",
    "        try:\n",
    "#             target_index = data.High[row[6]:row[6]+train_window+1].loc[data.High >= row[4] * (1 + target_percent)].index[0]\n",
    "            target_index = data.Close[row[6]:row[6]+train_window+1].loc[data.Close >= row[4] * (1 + target_percent)].index[0]\n",
    "        except IndexError:\n",
    "            target_index = data.index.max() + 5\n",
    "        try:\n",
    "#             stop_loss_index = data.Low[row[6]:row[6]+train_window+1].loc[data.Low <= row[4] * (1 - stop_loss_percent)].index[0]\n",
    "            stop_loss_index = data.Close[row[6]:row[6]+train_window+1].loc[data.Close <= row[4] * (1 - stop_loss_percent)].index[0]\n",
    "        except IndexError:\n",
    "            stop_loss_index = data.index.max() + 5\n",
    "        if target_index > stop_loss_index:\n",
    "            return 0\n",
    "        elif target_index < stop_loss_index:\n",
    "            return 2\n",
    "        return 1\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def create_inout_sequences(self, input_data, input_labels, tw):\n",
    "        inout_seq = []\n",
    "        L = len(input_data)\n",
    "        for i in range(L-tw):\n",
    "            train_seq = torch.FloatTensor(input_data[i:i+tw])\n",
    "            train_label = torch.FloatTensor([input_labels[i+tw - 1]])\n",
    "            if train_label == INVALID_LABEL or torch.isnan(train_seq).any():\n",
    "                continue\n",
    "            inout_seq.append((train_seq ,train_label))\n",
    "        return inout_seq\n",
    "\n",
    "    def get_sequenced_train_val_test(self, features, datad, train_window=20):\n",
    "        data = datad['data']\n",
    "        labels = datad['labels']\n",
    "        all = create_inout_sequences(features.values, list(labels), train_window)\n",
    "        random.shuffle(all)\n",
    "    #     display(all)\n",
    "        train = all[:-math.floor(len(features)/2.5)]\n",
    "        validate = all[-math.floor(len(features)/2.5):-math.floor(len(features)/4)]\n",
    "        test = all[-math.floor(len(features)/4):]\n",
    "\n",
    "        tdf = pd.DataFrame(train)\n",
    "        display(tdf)\n",
    "        display(tdf[1].values[0])\n",
    "        print(tdf.__dict__)\n",
    "\n",
    "\n",
    "\n",
    "        label_rows = {}\n",
    "        for label in sorted(list(data.label.unique())):\n",
    "            print(label, len(tdf.loc[tdf[1] == torch.tensor([label])]))\n",
    "            label_rows[label] = tdf.loc[tdf[1] == torch.tensor([label])]\n",
    "\n",
    "\n",
    "        min_len = min(len(v) for k, v in label_rows.items())\n",
    "        min_key = [label for label in sorted(list(data.label.unique())) if len(label_rows[label]) == min_len]\n",
    "        print()\n",
    "        for label in sorted(list(data.label.unique())):\n",
    "            to_remove = np.random.choice(label_rows[label].index,size=len(label_rows[label]) - min_len,replace=False)\n",
    "            tdf = tdf.drop(to_remove)\n",
    "            print(label, len(tdf.loc[tdf[1] == torch.tensor([label])]))\n",
    "        print()\n",
    "\n",
    "        train = [tuple(r) for r in tdf.to_numpy()]\n",
    "        print(len(train), len(validate), len(test))\n",
    "        return train, validate, test\n",
    "    \n",
    "    \n",
    "    def get_pca_features(features, pca_components=10):\n",
    "\n",
    "        pca = PCA(n_components=pca_components)\n",
    "        features_pca = pd.DataFrame(pca.fit_transform(features))\n",
    "        return features_pca\n",
    "\n",
    "    def rfe(features, labels, sample_size=2000, trials=4, select=10):\n",
    "        estimator = SVR(kernel=\"linear\")\n",
    "        selector = RFE(estimator, n_features_to_select=1)#, step=1)\n",
    "        sums = [0] * len(features.columns)\n",
    "        for trial in range(trials):\n",
    "            samples = features.sample(n=sample_size)\n",
    "            selector = selector.fit(samples, labels.loc[samples.index])\n",
    "            print(selector.ranking_)\n",
    "            sums = [sum(i) for i in zip(sums, selector.ranking_)]\n",
    "            print(sums)\n",
    "            print()\n",
    "\n",
    "        return features[[features.columns[i] for i in np.argsort(sums)[-select:] ]]\n",
    "    \n",
    "    def normalize_sequence_columns(sequence_label, range=(-1, 1), indices=list(range(len(chosen_dependent)))):\n",
    "        sequence, label = sequence_label\n",
    "\n",
    "        input_df = pd.DataFrame(sequence, dtype=np.float32)\n",
    "        for index in indices:\n",
    "            input_df[index] = minmax_scale(input_df[index], feature_range=range)\n",
    "        seq = torch.FloatTensor(input_df.values)\n",
    "        label = torch.FloatTensor([label])\n",
    "        return (seq, label)\n",
    "\n",
    "    \n",
    "    def prepare_data(self, run):\n",
    "        \n",
    "        name = run.data_set\n",
    "        datad = self.data_sets[name]\n",
    "        data = datad['data']\n",
    "        train_window = run.train_window\n",
    "        pool = multiprocessing.Pool(multiprocessing.cpu_count() - n) \n",
    "        pca_components = run.pca_components\n",
    "        label_mode = run.label_mode\n",
    "        \n",
    "        # labels\n",
    "        print('\\n Getting labels \\n')\n",
    "        \n",
    "        data['index'] = data.index\n",
    "        # too volatile class?\n",
    "        n = 0\n",
    "\n",
    "        start = time.time()\n",
    "        data['label'] = pool.map(partial(self.get_labels, mode=label_mode), [tuple(r) for r in data.to_numpy()])  # process data_inputs iterable with pool\n",
    "        print(name, 'pool label took: ', time.time() - start)\n",
    "            \n",
    "        # log class distribution\n",
    "        print('\\n Class distribution: \\n')\n",
    "        \n",
    "        for label in sorted(data.label.unique()):\n",
    "            print(name, label, len(data.loc[data.label == label]))\n",
    "                \n",
    "        # get indicators\n",
    "        print('\\n Getting indicators \\n')\n",
    "        \n",
    "        data['sma_10'] = ta.SMA(data.Close, timeperiod=10)\n",
    "        macd, macd_signal, macd_hist = ta.MACDFIX(data.Close, signalperiod=9)\n",
    "        data['macd'] = macd\n",
    "        data['macd_signal'] = macd_signal\n",
    "        data['macd_hist'] = macd_hist\n",
    "        data['cci_24'] = ta.CCI(data.High, data.Low, data.Close, timeperiod=24)\n",
    "        data['mom_10'] = ta.MOM(data.Close, timeperiod=10)\n",
    "        data['roc_10'] = ta.ROC(data.Close, timeperiod=10)\n",
    "        data['rsi_5'] = ta.RSI(data.Close, timeperiod=5)\n",
    "        data['wnr_9'] = ta.WILLR(data.High, data.Low, data.Close, timeperiod=9)\n",
    "        slowk, slowd = ta.STOCH(data.High, data.Low, data.Close, fastk_period=5, \n",
    "                                slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "        data['slowk'] = slowk\n",
    "        data['slowd'] = slowd\n",
    "        data['adosc'] = ta.ADOSC(data.High, data.Low, data.Close, data.Volume, fastperiod=3, slowperiod=10)\n",
    "        data = data[30:].reset_index()\n",
    "        data = data.drop(['level_0'], axis=1)\n",
    "        data['index'] = data.index\n",
    "            \n",
    "        # min max them\n",
    "        print('\\n Min-max scaling indicators \\n')\n",
    "        independent_indicators = ['macd', 'macd_signal', 'macd_hist', 'cci_24', 'mom_10', 'roc_10','rsi_5','wnr_9','slowk','slowd','adosc']\n",
    "        for indicator in independent_indicators:\n",
    "            name = indicator + '_min_max'\n",
    "            mean = data[indicator].mean()\n",
    "            std = data[indicator].std()\n",
    "            data[indicator].loc[data[indicator] > mean + 3 * std] = mean + 3 * std\n",
    "            data[indicator].loc[data[indicator] < mean - 3 * std] = mean - 3 * std\n",
    "            data[name] = (data[indicator] - mean) / std \n",
    "            data[name] = minmax_scale(data[indicator], feature_range=(-1,1))\n",
    "                \n",
    "        # percentage them \n",
    "        print('\\n Getting percentage fluctuation of indicators \\n')\n",
    "        percentage_indicators = ['Close', 'Volume', 'sma_10'] + independent_indicators\n",
    "        for indicator in percentage_indicators:\n",
    "            name = indicator + '_percentage'\n",
    "            data[name] = data[indicator].pct_change().replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "            mean = data[name].mean()\n",
    "            std = data[name].std()\n",
    "            data[name].loc[data[name] > mean + 3 * std] = mean + 3 * std\n",
    "            data[name].loc[data[name] < mean - 3 * std] = mean - 3 * std\n",
    "            data[name] = (data[name] - mean) / std \n",
    "            data[name] = minmax_scale(data[name], feature_range=(-1, 1))\n",
    "                \n",
    "        # isolate features / labels, fillnas\n",
    "        print('\\n Isolating features and labels, filling their nas \\n')\n",
    "        dependent_indicators = ['Open','High','Low','Close','Volume', 'sma_10']\n",
    "\n",
    "        datad['features'] = data.copy().drop(['index', 'Date', 'label'] + dependent_indicators + independent_indicators, axis=1\n",
    "                                   ).fillna(0).replace([np.inf, -np.inf], np.nan).ffill()\n",
    "\n",
    "        display(datad['features'])\n",
    "\n",
    "        datad['labels'] = data['label'].copy().replace([np.inf, -np.inf], np.nan).fillna(INVALID_LABEL)\n",
    "\n",
    "        display(datad['labels'])\n",
    "            \n",
    "        # rfe\n",
    "        print('\\n Performing RFE \\n')\n",
    "        features = datad['features']\n",
    "        labels = datad['labels']\n",
    "        datad['rfe_features'] = rfe(features, labels, sample_size=2000, trials=4, select=10)\n",
    "        display(datad['rfe_features'])\n",
    "        rfe_features = datad['rfe_features']\n",
    "            \n",
    "        # pca\n",
    "        if pca_components:\n",
    "            print('\\n Performing PCA \\n')\n",
    "            datad['rfe_pca_features'] = get_pca_features(rfe_features, pca_components=7)\n",
    "            display(datad['rfe_pca_features'])\n",
    "            \n",
    "        # sequence and split to train val test\n",
    "        print('\\n Building sequences and splitting to train, val, and test sets \\n')\n",
    "        chosen_indicators = rfe_features #features_pca\n",
    "        chosen_dependent = [ 'Close', 'Volume'] #dependent_indicators #'High', 'Low',\n",
    "        combined = pd.concat([data[chosen_dependent], chosen_indicators], axis=1)\n",
    "        display(combined)\n",
    "        print(len(labels))\n",
    "        train, validate, test = get_sequenced_train_val_test(combined, datad, train_window=train_window)\n",
    "        datad['train'] = train\n",
    "        datad['validate'] = validate\n",
    "        datad['test'] = test\n",
    "        display(train[0])\n",
    "        \n",
    "        # normalize certain features within sequence\n",
    "        print('\\n Normalizing certain features within sequences \\n')\n",
    "        attempts = 3\n",
    "        train = datad['train']\n",
    "        validate = datad['validate']\n",
    "        test = datad['test']\n",
    "\n",
    "        for a in range(attempts):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                train = pool.map(normalize_sequence_columns, train.copy())\n",
    "                print(len(train), 'train took:', time.time() - start)\n",
    "                break\n",
    "            except RuntimeError:\n",
    "                print('train err ', a)\n",
    "                pass\n",
    "\n",
    "        for a in range(attempts):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                validate = pool.map(normalize_sequence_columns, validate.copy())\n",
    "                print(len(validate), 'val took:', time.time() - start)\n",
    "                break\n",
    "            except RuntimeError:\n",
    "                print('val err ', a)\n",
    "                pass\n",
    "\n",
    "        for a in range(attempts):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                test = pool.map(normalize_sequence_columns, test.copy())\n",
    "                print(len(test), 'test took:', time.time() - start)\n",
    "                break\n",
    "            except RuntimeError:\n",
    "                print('test err ', a)\n",
    "                pass\n",
    "        display(train[0])\n",
    "            \n",
    "        pool.close()\n",
    "        \n",
    "    # record the count, hyper-param, model, loader of each run\n",
    "    # record sample images and network graph to TensorBoard    \n",
    "    def begin_run(self, run, network):\n",
    "\n",
    "        self.run_start_time = time.time()\n",
    "\n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "#         self.run_plot_statistics[self.run_count] = {}\n",
    "    \n",
    "        self.network = network\n",
    "        \n",
    "        self.prepare_data(run)\n",
    "\n",
    "    # when run ends, close TensorBoard, zero epoch count\n",
    "    def end_run(self, net):\n",
    "        self.epoch_count = 0\n",
    "        \n",
    "        self.run_data[-1]['net'] = net\n",
    "        \n",
    "        test_accuracy = sum([v for k, v in self.test_correct_count.items()]) / (len(self.data_sets[self.run_params.data_set][phase]))\n",
    "        self.run_data[-1]['test_accuracy'] = test_accuracy\n",
    "        \n",
    "        cnf_matrix = sklearn.metrics.confusion_matrix(self.test_labels, self.test_predictions)\n",
    "        self.run_data[-1]['confusion_matrix'] = cnf_matrix\n",
    "        \n",
    "        self.runs.append(self.run_data)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Plot normalized confusion matrix\n",
    "#         fig = plt.figure()\n",
    "#         fig.set_size_inches(7, 6, forward=True)\n",
    "        #fig.align_labels()\n",
    "\n",
    "        # fig.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
    "#         self.plot_confusion_matrix(cnf_matrix, classes=self.run_params.label_subset, normalize=True,\n",
    "#                               title='Normalized confusion matrix')\n",
    "        \n",
    "\n",
    "    # zero epoch count, loss, accuracy, \n",
    "    def begin_epoch(self, epoch_number):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "        self.epoch_count = epoch_number\n",
    "#         self.run_plot_statistics[self.run_count][self.epoch_count] = {\n",
    "#             'loss': {phase: [] for phase in self.loaders.keys()},\n",
    "#             'accuracy': {phase: [] for phase in self.loaders.keys()}\n",
    "#         }\n",
    "        self.epoch_loss = {'train': 0, 'validate': 0}\n",
    "        self.epoch_num_correct = {'train': {k: 0 for k in global_labels},\n",
    "                                  'validate': {k: 0 for k in global_labels}}\n",
    "\n",
    "    def end_epoch(self):\n",
    "        # calculate epoch duration and run duration(accumulate)\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "\n",
    "        # record epoch loss and accuracy\n",
    "        def get_all_correct(dict):\n",
    "            return sum([v for k, v in dict.items()])\n",
    "        loss = {phase: self.epoch_loss[phase] / (len(self.data_sets[self.run_params.data_set][phase])) for phase in ['train', 'validate']}\n",
    "        accuracy = {phase: get_all_correct(self.epoch_num_correct[phase]) / (len(self.data_sets[self.run_params.data_set][phase])) for phase in ['train', 'validate']}\n",
    "        \n",
    "        # Write into 'results' (OrderedDict) for all run related data\n",
    "        results = OrderedDict()\n",
    "        results['run'] = self.run_count\n",
    "        results['epoch'] = self.epoch_count\n",
    "        results['train loss'] = loss['train']\n",
    "        results['validate loss'] = loss['validate']\n",
    "        results['train accuracy'] = accuracy['train']\n",
    "        results['validate accuracy'] = accuracy['validate']\n",
    "        results['epoch duration'] = epoch_duration\n",
    "        results['run duration'] = run_duration\n",
    "\n",
    "        # Record hyper-params into 'results'\n",
    "        for parameter, value in self.run_params._asdict().items(): \n",
    "            if type(value) == dict:\n",
    "                for true_parameter, true_value in value.items():\n",
    "                    results[true_parameter] = true_value\n",
    "                continue\n",
    "                \n",
    "            results[parameter] = value\n",
    "            \n",
    "        self.run_data.append(results)\n",
    "\n",
    "#         print(results)\n",
    "\n",
    "    # accumulate loss of batch into entire epoch loss\n",
    "    def track_loss(self, phase, raw_loss):\n",
    "        loss = raw_loss.item() \n",
    "        self.epoch_loss[phase] += loss\n",
    "        \n",
    "#         self.run_plot_statistics[self.run_count][self.epoch_count]['loss'][phase].append(loss)\n",
    "\n",
    "    # accumulate number of corrects of batch into entire epoch num_correct\n",
    "    def track_num_correct(self, phase, outputs, labels):\n",
    "        try:\n",
    "            l_i = int(labels.item())\n",
    "        except:\n",
    "            l_i = 0\n",
    "        self.epoch_num_correct[phase][l_i] += self._get_num_correct(outputs, labels)\n",
    "#         try:\n",
    "#             self.run_plot_statistics[self.run_count][self.epoch_count]['accuracy'][phase].append(self.epoch_num_correct[phase] / \\\n",
    "#                                                         len(self.run_plot_statistics[self.run_count][self.epoch_count]['accuracy']))\n",
    "#         except: # if first image\n",
    "#             self.run_plot_statistics[self.run_count][self.epoch_count]['accuracy'][phase].append(self.epoch_num_correct[phase])\n",
    "        \n",
    "    def track_test_predictions(self, prediction, label):\n",
    "        self.test_predictions.append(prediction)\n",
    "        self.test_labels.append(label)\n",
    "        try:\n",
    "            l_i = int(labels.item())\n",
    "        except:\n",
    "            l_i = 0\n",
    "        self.test_correct_count[l_i] += 1 if prediction == label else 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_num_correct(self, output, label):\n",
    "        try:\n",
    "            l_i = int(labels.item())\n",
    "        except:\n",
    "            l_i = 0\n",
    "        return 1 if int(torch.argmax(output)) == l_i else 0\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm, classes, variables, normalize=False, cmap=plt.cm.Blues):\n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \"\"\"\n",
    "                \n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            print(f\"Normalized Confusion Matrix (Run #{len(self.runs) - 1})\")\n",
    "        else:\n",
    "            print('Confusion matrix, without normalization')        \n",
    "\n",
    "        ax = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(f'{df_row.data_set}: Confusion matrix')\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=90)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        \n",
    "        x_label = \"Predicted label\"\n",
    "        for variable in variables:\n",
    "            x_label += f\"\\n{variable} = {run_data[variable].values[0]}\"\n",
    "        ax.set_xlabel(x_label)\n",
    "        plt.show()\n",
    "    \n",
    "    # save end results of all runs into json for further analysis\n",
    "    def results(self, fileName):\n",
    "        return\n",
    "        \n",
    "#         result_df = pd.DataFrame.from_dict(\n",
    "#                 self.run_data, \n",
    "#                 orient = 'columns',\n",
    "#         )\n",
    "#         display(result_df)\n",
    "        \n",
    "\n",
    "#         with open(f'results/{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "#             json.dump(self.run_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, weight_init={'function':torch.nn.init.xavier_uniform}, hidden_neurons=128, output_neurons=len(global_labels), \n",
    "                 hidden_activation=functional.relu, output_activation=torch.nn.Softmax(dim=2), input_size=len(train[0][0][0])):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        # hyper parameters\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_neurons)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        \n",
    "        self.hidden_cell = (torch.zeros(1,1,hidden_neurons),\n",
    "                            torch.zeros(1,1,hidden_neurons))\n",
    "        \n",
    "        self.cnn = [#nn.Sequential(\n",
    "            nn.Conv1d(input_size, 128, 1),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=8, stride=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 128, kernel_size=5, stride=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()#,\n",
    "#             nn.AvgPool1d(kernel_size=7, stride=1,padding=0) #(Lin+2*p-k)/s+1\n",
    "        ]#)\n",
    "            \n",
    "        self.out = nn.Linear(in_features=hidden_neurons, out_features=output_neurons) # hidden layer to output\n",
    "        if weight_init:\n",
    "            weight_init['function'](self.out.weight)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print('raw input size', x.size())\n",
    "#         print('lstm altered size', x.view(len(x) ,1, self.input_size).size())\n",
    "\n",
    "        lstm_input = x.view(len(x) ,1, self.input_size)\n",
    "        lstm_out, self.hidden_cell = self.lstm(lstm_input, self.hidden_cell)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "#         print('lstm out:', lstm_out.size())\n",
    "#         print('lstm out out:', self.out(lstm_out).size())\n",
    "#         print('lstm activation:', self.output_activation(self.out(lstm_out)).size(), self.output_activation(self.out(lstm_out))[-1])\n",
    "#         predictions = self.output_activation(self.out(lstm_out))\n",
    "#         return predictions[-1]\n",
    "\n",
    "# #         print('cnn altered size', x.reshape(1, self.input_size, len(x)).size())\n",
    "#         cnn_out = x.reshape(1, self.input_size, len(x))\n",
    "#         for i, layer in enumerate(self.cnn):\n",
    "# #             print(i, '\\nx: ', cnn_out.size())\n",
    "#             cnn_out = layer(cnn_out)\n",
    "# #             print('result: ', cnn_out.size(), '\\n')\n",
    "#         cnn_out = cnn_out.reshape(-1, 1, 128)\n",
    "    \n",
    "    \n",
    "#         print(lstm_out.size(), cnn_out.size())\n",
    "        features = lstm_out#torch.cat((lstm_out, cnn_out))\n",
    "#         print(features.size())\n",
    "        result = self.out(features)\n",
    "#         print('res', result.size())\n",
    "        result = self.output_activation(result)\n",
    "#         print('res act', result.size())\n",
    "#         print(result[-1])\n",
    "        return result[-1]\n",
    "        \n",
    "\n",
    "def sum_squared_error(out, label):\n",
    "#     print(label, out)\n",
    "#     print(label - out)\n",
    "#     print()\n",
    "    return ((label - out) ** 2).sum()\n",
    "\n",
    "def mean_squared_error(outputs, labels):\n",
    "    return sum_squared_error(outputs, labels) / len(outputs)\n",
    "\n",
    "def cross_entropy(outputs, labels):\n",
    "    return -1 * (torch.log(outputs) * labels + (torch.log(1 - outputs)) * (1 - labels)).sum()\n",
    "\n",
    "def dummy_activation(x):\n",
    "    return x\n",
    "\n",
    "# put all hyper params into a OrderedDict, easily expandable\n",
    "params = OrderedDict(\n",
    "    data_set = ['BTCUSDT'], #'sp500', \n",
    "    hidden_neurons = [100],#, 100, 5], #1\n",
    "    \n",
    "    batch_size = [1],\n",
    "    \n",
    "    weight_init = [{\n",
    "        'function': torch.nn.init.xavier_uniform,\n",
    "        'name': \"Xavier Uniform\"\n",
    "    }],\n",
    "    \n",
    "    hidden_activation = [torch.relu],#, torch.tanh, torch.relu],\n",
    "    loss_output = [\n",
    "        {\n",
    "        'criterion': sum_squared_error,\n",
    "        'output_activation': torch.nn.Softmax(dim=2)\n",
    "    },  \n",
    "\n",
    "    ],\n",
    "    \n",
    "    learning_rate = [0.01],#, .001],\n",
    "    momentum = [0.1],#, 0],\n",
    "    \n",
    "    optimizer = [optim.SGD],#, optim.Adam], #optim.Adam(network.parameters(), lr=run.lr)\n",
    "    validation_split = [0.1],\n",
    "    train_window = [20],\n",
    "    pca_components = [None],#10\n",
    "    label_mode = ['since3']\n",
    ")\n",
    "\n",
    "\n",
    "def negative_one(x):\n",
    "    return -1\n",
    "\n",
    "def zero(x):\n",
    "    return 0\n",
    "\n",
    "def one(x):\n",
    "    return 1\n",
    "\n",
    "def argmax(x):\n",
    "    return x[0][0][torch.argmax(x)].item()\n",
    "\n",
    "error_encoding_map = {\n",
    "    torch.sigmoid: {\n",
    "        'cold': zero,\n",
    "        'hot': one\n",
    "    },\n",
    "    torch.relu: {\n",
    "        'cold': zero,\n",
    "        'hot': one\n",
    "    },\n",
    "    torch.tanh: {\n",
    "        'cold': negative_one,\n",
    "        'hot': one\n",
    "    },\n",
    "    torch.nn.Softmax(dim=2): {\n",
    "        'cold': zero,\n",
    "        'hot': one\n",
    "    },\n",
    "    dummy_activation: {\n",
    "        'cold': zero,\n",
    "        'hot': argmax\n",
    "    }\n",
    "}\n",
    "\n",
    "m = RunManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(data_set='BTCUSDT', hidden_neurons=100, batch_size=1, weight_init={'function': <function _make_deprecate.<locals>.deprecated_init at 0x1246c96a8>, 'name': 'Xavier Uniform'}, hidden_activation=<built-in method relu of type object at 0x13f1c0c20>, loss_output={'criterion': <function sum_squared_error at 0x166e697b8>, 'output_activation': Softmax(dim=2)}, learning_rate=0.01, momentum=0.1, optimizer=<class 'torch.optim.sgd.SGD'>, validation_split=0.1)\n",
      "sample #0 train 0.0 {0: 0, 1: 0, 2: 0}\n",
      "sample #1000 train 0.7116110920906067 {0: 117, 1: 116, 2: 124}\n",
      "sample #2000 train 0.7167611122131348 {0: 227, 1: 242, 2: 199}\n",
      "sample #3000 train 0.7093368768692017 {0: 354, 1: 336, 2: 313}\n",
      "sample #4000 train 0.7156962156295776 {0: 443, 1: 471, 2: 410}\n",
      "sample #5000 train 0.7116676568984985 {0: 597, 1: 555, 2: 506}\n",
      "sample #6000 train 0.7018457055091858 {0: 694, 1: 669, 2: 629}\n",
      "sample #0 validate 0.28592172265052795 {0: 0, 1: 0, 2: 0}\n",
      "sample #1000 validate 0.7054585218429565 {0: 0, 1: 0, 2: 138}\n",
      "sample #2000 validate 0.7052972912788391 {0: 0, 1: 0, 2: 275}\n",
      "sample #3000 validate 0.7074452042579651 {0: 0, 1: 0, 2: 400}\n",
      "sample #0 train 0.6384454369544983 {0: 0, 1: 0, 2: 0}\n",
      "sample #1000 train 0.7099899649620056 {0: 106, 1: 112, 2: 118}\n",
      "sample #2000 train 0.7065271139144897 {0: 224, 1: 208, 2: 224}\n",
      "sample #3000 train 0.7076906561851501 {0: 319, 1: 324, 2: 339}\n",
      "sample #4000 train 0.7091895341873169 {0: 433, 1: 455, 2: 435}\n",
      "sample #5000 train 0.7109392881393433 {0: 577, 1: 547, 2: 525}\n",
      "sample #6000 train 0.7009389996528625 {0: 696, 1: 654, 2: 647}\n",
      "sample #0 validate 0.29201382398605347 {0: 0, 1: 0, 2: 0}\n",
      "sample #1000 validate 0.6873920559883118 {0: 0, 1: 0, 2: 138}\n",
      "sample #2000 validate 0.6869664788246155 {0: 0, 1: 0, 2: 275}\n",
      "sample #3000 validate 0.6882396936416626 {0: 0, 1: 0, 2: 400}\n",
      "sample #0 train 0.6210854649543762 {0: 0, 1: 0, 2: 0}\n",
      "sample #1000 train 0.7061294317245483 {0: 109, 1: 95, 2: 131}\n",
      "sample #2000 train 0.7088516354560852 {0: 210, 1: 211, 2: 220}\n",
      "sample #3000 train 0.7137220501899719 {0: 307, 1: 322, 2: 348}\n",
      "sample #4000 train 0.7118453979492188 {0: 403, 1: 464, 2: 439}\n",
      "sample #5000 train 0.7042570114135742 {0: 558, 1: 564, 2: 548}\n",
      "sample #6000 train 0.7017548680305481 {0: 661, 1: 681, 2: 672}\n",
      "sample #0 validate 0.2857041358947754 {0: 0, 1: 0, 2: 0}\n",
      "sample #1000 validate 0.6814414858818054 {0: 0, 1: 0, 2: 138}\n",
      "sample #2000 validate 0.6809554696083069 {0: 0, 1: 0, 2: 275}\n",
      "sample #3000 validate 0.6819096803665161 {0: 0, 1: 0, 2: 400}\n",
      "sample #0 train 0.6153814792633057 {0: 0, 1: 0, 2: 0}\n",
      "sample #1000 train 0.6977037787437439 {0: 109, 1: 115, 2: 135}\n",
      "sample #2000 train 0.7152540683746338 {0: 222, 1: 233, 2: 231}\n",
      "sample #3000 train 0.7137125730514526 {0: 323, 1: 328, 2: 351}\n",
      "sample #4000 train 0.7173476219177246 {0: 423, 1: 460, 2: 441}\n",
      "sample #5000 train 0.7026416659355164 {0: 578, 1: 555, 2: 529}\n",
      "sample #6000 train 0.7100049257278442 {0: 676, 1: 669, 2: 652}\n",
      "sample #0 validate 0.2839178442955017 {0: 0, 1: 0, 2: 0}\n",
      "sample #1000 validate 0.7198029160499573 {0: 0, 1: 0, 2: 138}\n",
      "sample #2000 validate 0.7183294892311096 {0: 0, 1: 0, 2: 275}\n",
      "sample #3000 validate 0.7214219570159912 {0: 0, 1: 0, 2: 400}\n",
      "sample #0 train 0.6502604484558105 {0: 0, 1: 0, 2: 0}\n",
      "sample #1000 train 0.7070382833480835 {0: 109, 1: 94, 2: 132}\n",
      "sample #2000 train 0.7071042060852051 {0: 233, 1: 210, 2: 238}\n",
      "sample #3000 train 0.7073191404342651 {0: 336, 1: 315, 2: 367}\n",
      "sample #4000 train 0.7157653570175171 {0: 436, 1: 455, 2: 453}\n",
      "sample #5000 train 0.704751193523407 {0: 577, 1: 547, 2: 552}\n",
      "sample #6000 train 0.7031834125518799 {0: 685, 1: 656, 2: 678}\n",
      "sample #0 validate 0.28760653734207153 {0: 0, 1: 0, 2: 0}\n",
      "sample #1000 validate 0.7165768146514893 {0: 0, 1: 0, 2: 138}\n",
      "sample #2000 validate 0.7155102491378784 {0: 0, 1: 0, 2: 275}\n",
      "sample #3000 validate 0.7183729410171509 {0: 0, 1: 0, 2: 400}\n",
      "sample #0 train 0.6477473974227905 {0: 0, 1: 0, 2: 0}\n",
      "sample #1000 train 0.7064035534858704 {0: 94, 1: 102, 2: 131}\n",
      "sample #2000 train 0.7055913209915161 {0: 215, 1: 232, 2: 225}\n",
      "sample #3000 train 0.7127421498298645 {0: 328, 1: 326, 2: 354}\n",
      "sample #4000 train 0.7065697312355042 {0: 426, 1: 461, 2: 446}\n",
      "sample #5000 train 0.7054811120033264 {0: 578, 1: 556, 2: 530}\n",
      "sample #6000 train 0.6975497603416443 {0: 692, 1: 676, 2: 649}\n",
      "sample #0 validate 0.2908233404159546 {0: 0, 1: 0, 2: 0}\n",
      "sample #1000 validate 0.6673468351364136 {0: 0, 1: 532, 2: 0}\n",
      "sample #2000 validate 0.6699540019035339 {0: 0, 1: 1041, 2: 0}\n",
      "sample #3000 validate 0.6694663763046265 {0: 0, 1: 1566, 2: 0}\n",
      "sample #0: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #250: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #500: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #750: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #1000: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([0.]) 1\n",
      "sample #1250: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #1500: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #1750: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #2000: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([0.]) 1\n",
      "sample #2250: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #2500: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #2750: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #3000: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([0.]) 1\n",
      "sample #3250: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([2.]) 1\n",
      "sample #3500: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([0.]) 1\n",
      "sample #3750: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #4000: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #4250: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([0.]) 1\n",
      "sample #4500: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #4750: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #5000: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #5250: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #5500: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([0.]) 1\n",
      "sample #5750: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #6000: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #6250: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([1.]) 1\n",
      "sample #6500: tensor([[0.2978, 0.3522, 0.3500]], grad_fn=<SelectBackward>) tensor([2.]) 1\n"
     ]
    }
   ],
   "source": [
    "import time, copy, json\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "epochs = 6\n",
    "# get all runs from params using RunBuilder class\n",
    "# print(f\"Runs: {RunBuilder.get_runs(params)}\")\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    print(run)\n",
    "    # if params changes, following line of code should reflect the changes too\n",
    "    net = NeuralNet()\n",
    "#         hidden_neurons=run.hidden_neurons,\n",
    "#         hidden_activation=run.hidden_activation, output_activation=run.loss_output['output_activation'])\n",
    "    optimizer = run.optimizer(net.parameters(), lr=run.learning_rate, momentum=run.momentum)#copy.deepcopy(run.optimizer)\n",
    "    \n",
    "    sum_loss = 0\n",
    "    criterion = run.loss_output['criterion']\n",
    "\n",
    "    m.begin_run(run, net)\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        m.begin_epoch(epoch + 1)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'validate']:\n",
    "            count = 0\n",
    "            if phase == 'train':\n",
    "                net.train()  # Set model to training mode\n",
    "            else:\n",
    "                net.eval()   # Set model to evaluate mode\n",
    "                \n",
    "            # Iterate over data.\n",
    "            for images, labels in m.data_sets[m.run_params.data_set][phase]:\n",
    "\n",
    "                if count % 1000 == 0:\n",
    "                    print(f'sample #{count} {phase} {sum_loss / 1000} {m.epoch_num_correct[phase]}')\n",
    "                    sum_loss = 0\n",
    "                    \n",
    "                net.hidden_cell = (torch.zeros(1, 1, net.hidden_neurons),\n",
    "                    torch.zeros(1, 1, net.hidden_neurons))\n",
    "\n",
    "                X = Variable(images)#.reshape(1, 784, 1).squeeze(0)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(X)         \n",
    "                    Y = np.zeros(len(global_labels))\n",
    "                    try:\n",
    "                        l_i = int(labels.item())\n",
    "                    except:\n",
    "                        l_i = 0\n",
    "                    try:\n",
    "                        Y[l_i] = 1 \n",
    "                    except:\n",
    "                        print('bad label')\n",
    "                        Y[0] = 1\n",
    "                    Y = Variable(torch.from_numpy(Y).long()).unsqueeze(0)\n",
    "                    loss = criterion(outputs, Y)\n",
    "#                     if math.isnan(loss):\n",
    "#                         print(outputs, Y)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                sum_loss += loss\n",
    "                m.track_loss(phase, loss)\n",
    "                m.track_num_correct(phase, outputs, labels)\n",
    "                \n",
    "#                 print('\\n')\n",
    "                count += 1\n",
    "#                 if count > 100:\n",
    "#                     break\n",
    "                \n",
    "                    \n",
    "        m.end_epoch()\n",
    "    \n",
    "    # Testing\n",
    "    y_true = []\n",
    "    y_predict = []\n",
    "    phase = 'test'\n",
    "    count = 0\n",
    "    net.eval()\n",
    "    for images, labels in m.data_sets[m.run_params.data_set]['test']:\n",
    "\n",
    "        X = Variable(images)#.unsqueeze(2)\n",
    "        Y = Variable(labels)\n",
    "\n",
    "        outputs = net(X)\n",
    "        predicted_class = int(torch.argmax(outputs))\n",
    "        \n",
    "        m.track_test_predictions(predicted_class, labels.item())\n",
    "\n",
    "        if count % 250 == 0:\n",
    "            print(f'sample #{count}: {outputs} {labels} {predicted_class}')\n",
    "        count += 1\n",
    "\n",
    "#         if count > 40:\n",
    "#             break\n",
    "\n",
    "    m.end_run(net)\n",
    "\n",
    "# when all runs are done, show results\n",
    "m.results('trial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('run', 1),\n",
       "              ('epoch', 1),\n",
       "              ('train loss', 0.5054013383072672),\n",
       "              ('validate loss', 0.4940976263252127),\n",
       "              ('train accuracy', 0.4952726473175022),\n",
       "              ('validate accuracy', 0.565282454171343),\n",
       "              ('epoch duration', 51.0260648727417),\n",
       "              ('run duration', 51.02607083320618),\n",
       "              ('data_set', 'xrp_btc'),\n",
       "              ('hidden_neurons', 100),\n",
       "              ('batch_size', 1),\n",
       "              ('function',\n",
       "               <function torch.nn.init._make_deprecate.<locals>.deprecated_init(*args, **kwargs)>),\n",
       "              ('name', 'Xavier Uniform'),\n",
       "              ('hidden_activation', <function _VariableFunctionsClass.relu>),\n",
       "              ('criterion', <function __main__.sum_squared_error(out, label)>),\n",
       "              ('output_activation', Softmax(dim=2)),\n",
       "              ('learning_rate', 0.01),\n",
       "              ('momentum', 0.1),\n",
       "              ('optimizer', torch.optim.sgd.SGD),\n",
       "              ('validation_split', 0.1)]),\n",
       " OrderedDict([('run', 1),\n",
       "              ('epoch', 2),\n",
       "              ('train loss', 0.5030166666017699),\n",
       "              ('validate loss', 0.49491386166280726),\n",
       "              ('train accuracy', 0.5049472295514512),\n",
       "              ('validate accuracy', 0.5712682379349046),\n",
       "              ('epoch duration', 52.33416700363159),\n",
       "              ('run duration', 103.36028099060059),\n",
       "              ('data_set', 'xrp_btc'),\n",
       "              ('hidden_neurons', 100),\n",
       "              ('batch_size', 1),\n",
       "              ('function',\n",
       "               <function torch.nn.init._make_deprecate.<locals>.deprecated_init(*args, **kwargs)>),\n",
       "              ('name', 'Xavier Uniform'),\n",
       "              ('hidden_activation', <function _VariableFunctionsClass.relu>),\n",
       "              ('criterion', <function __main__.sum_squared_error(out, label)>),\n",
       "              ('output_activation', Softmax(dim=2)),\n",
       "              ('learning_rate', 0.01),\n",
       "              ('momentum', 0.1),\n",
       "              ('optimizer', torch.optim.sgd.SGD),\n",
       "              ('validation_split', 0.1)]),\n",
       " OrderedDict([('run', 1),\n",
       "              ('epoch', 3),\n",
       "              ('train loss', 0.5021216979048811),\n",
       "              ('validate loss', 0.49541631800335445),\n",
       "              ('train accuracy', 0.5019788918205804),\n",
       "              ('validate accuracy', 0.5679012345679012),\n",
       "              ('epoch duration', 52.470519065856934),\n",
       "              ('run duration', 155.83084201812744),\n",
       "              ('data_set', 'xrp_btc'),\n",
       "              ('hidden_neurons', 100),\n",
       "              ('batch_size', 1),\n",
       "              ('function',\n",
       "               <function torch.nn.init._make_deprecate.<locals>.deprecated_init(*args, **kwargs)>),\n",
       "              ('name', 'Xavier Uniform'),\n",
       "              ('hidden_activation', <function _VariableFunctionsClass.relu>),\n",
       "              ('criterion', <function __main__.sum_squared_error(out, label)>),\n",
       "              ('output_activation', Softmax(dim=2)),\n",
       "              ('learning_rate', 0.01),\n",
       "              ('momentum', 0.1),\n",
       "              ('optimizer', torch.optim.sgd.SGD),\n",
       "              ('validation_split', 0.1)]),\n",
       " OrderedDict([('run', 1),\n",
       "              ('epoch', 4),\n",
       "              ('train loss', 0.5018801802315519),\n",
       "              ('validate loss', 0.49581602228031235),\n",
       "              ('train accuracy', 0.5042875989445911),\n",
       "              ('validate accuracy', 0.5615413393191171),\n",
       "              ('epoch duration', 53.39913988113403),\n",
       "              ('run duration', 209.23002576828003),\n",
       "              ('data_set', 'xrp_btc'),\n",
       "              ('hidden_neurons', 100),\n",
       "              ('batch_size', 1),\n",
       "              ('function',\n",
       "               <function torch.nn.init._make_deprecate.<locals>.deprecated_init(*args, **kwargs)>),\n",
       "              ('name', 'Xavier Uniform'),\n",
       "              ('hidden_activation', <function _VariableFunctionsClass.relu>),\n",
       "              ('criterion', <function __main__.sum_squared_error(out, label)>),\n",
       "              ('output_activation', Softmax(dim=2)),\n",
       "              ('learning_rate', 0.01),\n",
       "              ('momentum', 0.1),\n",
       "              ('optimizer', torch.optim.sgd.SGD),\n",
       "              ('validation_split', 0.1)]),\n",
       " OrderedDict([('run', 1),\n",
       "              ('epoch', 5),\n",
       "              ('train loss', 0.5017599487151351),\n",
       "              ('validate loss', 0.495741448819838),\n",
       "              ('train accuracy', 0.5038478452066842),\n",
       "              ('validate accuracy', 0.5611672278338945),\n",
       "              ('epoch duration', 51.936245918273926),\n",
       "              ('run duration', 261.16631174087524),\n",
       "              ('data_set', 'xrp_btc'),\n",
       "              ('hidden_neurons', 100),\n",
       "              ('batch_size', 1),\n",
       "              ('function',\n",
       "               <function torch.nn.init._make_deprecate.<locals>.deprecated_init(*args, **kwargs)>),\n",
       "              ('name', 'Xavier Uniform'),\n",
       "              ('hidden_activation', <function _VariableFunctionsClass.relu>),\n",
       "              ('criterion', <function __main__.sum_squared_error(out, label)>),\n",
       "              ('output_activation', Softmax(dim=2)),\n",
       "              ('learning_rate', 0.01),\n",
       "              ('momentum', 0.1),\n",
       "              ('optimizer', torch.optim.sgd.SGD),\n",
       "              ('validation_split', 0.1)]),\n",
       " OrderedDict([('run', 1),\n",
       "              ('epoch', 6),\n",
       "              ('train loss', 0.5016201729529923),\n",
       "              ('validate loss', 0.4960048052642928),\n",
       "              ('train accuracy', 0.5046174142480211),\n",
       "              ('validate accuracy', 0.5548073325851104),\n",
       "              ('epoch duration', 53.67730498313904),\n",
       "              ('run duration', 314.84365797042847),\n",
       "              ('data_set', 'xrp_btc'),\n",
       "              ('hidden_neurons', 100),\n",
       "              ('batch_size', 1),\n",
       "              ('function',\n",
       "               <function torch.nn.init._make_deprecate.<locals>.deprecated_init(*args, **kwargs)>),\n",
       "              ('name', 'Xavier Uniform'),\n",
       "              ('hidden_activation', <function _VariableFunctionsClass.relu>),\n",
       "              ('criterion', <function __main__.sum_squared_error(out, label)>),\n",
       "              ('output_activation', Softmax(dim=2)),\n",
       "              ('learning_rate', 0.01),\n",
       "              ('momentum', 0.1),\n",
       "              ('optimizer', torch.optim.sgd.SGD),\n",
       "              ('validation_split', 0.1),\n",
       "              ('net', NeuralNet(\n",
       "                 (output_activation): Softmax(dim=2)\n",
       "                 (lstm): LSTM(12, 128)\n",
       "                 (dropout): Dropout(p=0.4, inplace=False)\n",
       "                 (out): Linear(in_features=128, out_features=2, bias=True)\n",
       "               )),\n",
       "              ('test_accuracy', 0.5637342908438061),\n",
       "              ('confusion_matrix', array([[ 340, 1572],\n",
       "                      [ 372, 2172]]))])]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cnf_matrix = sklearn.metrics.confusion_matrix(pd.DataFrame(m.test_labels).fillna(0).values, m.test_predictions)\n",
    "# m.run_data[-1]['confusion_matrix'] = cnf_matrix\n",
    "\n",
    "# test_accuracy = sum([v for k, v in m.test_correct_count.items()]) / (len(m.data_sets['xrp_btc'][phase]))\n",
    "# m.run_data[-1]['test_accuracy'] = test_accuracy\n",
    "\n",
    "# m.runs.append(m.run_data)\n",
    "m.run_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train loss</th>\n",
       "      <th>validate loss</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>validate accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>data_set</th>\n",
       "      <th>hidden_neurons</th>\n",
       "      <th>...</th>\n",
       "      <th>hidden_activation</th>\n",
       "      <th>criterion</th>\n",
       "      <th>output_activation</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>momentum</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>validation_split</th>\n",
       "      <th>net</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.505401</td>\n",
       "      <td>0.494098</td>\n",
       "      <td>0.495273</td>\n",
       "      <td>0.565282</td>\n",
       "      <td>51.026065</td>\n",
       "      <td>51.026071</td>\n",
       "      <td>xrp_btc</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;built-in method relu of type object at 0x13f1...</td>\n",
       "      <td>&lt;function sum_squared_error at 0x17138ad90&gt;</td>\n",
       "      <td>Softmax(dim=2)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.503017</td>\n",
       "      <td>0.494914</td>\n",
       "      <td>0.504947</td>\n",
       "      <td>0.571268</td>\n",
       "      <td>52.334167</td>\n",
       "      <td>103.360281</td>\n",
       "      <td>xrp_btc</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;built-in method relu of type object at 0x13f1...</td>\n",
       "      <td>&lt;function sum_squared_error at 0x17138ad90&gt;</td>\n",
       "      <td>Softmax(dim=2)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.502122</td>\n",
       "      <td>0.495416</td>\n",
       "      <td>0.501979</td>\n",
       "      <td>0.567901</td>\n",
       "      <td>52.470519</td>\n",
       "      <td>155.830842</td>\n",
       "      <td>xrp_btc</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;built-in method relu of type object at 0x13f1...</td>\n",
       "      <td>&lt;function sum_squared_error at 0x17138ad90&gt;</td>\n",
       "      <td>Softmax(dim=2)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.501880</td>\n",
       "      <td>0.495816</td>\n",
       "      <td>0.504288</td>\n",
       "      <td>0.561541</td>\n",
       "      <td>53.399140</td>\n",
       "      <td>209.230026</td>\n",
       "      <td>xrp_btc</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;built-in method relu of type object at 0x13f1...</td>\n",
       "      <td>&lt;function sum_squared_error at 0x17138ad90&gt;</td>\n",
       "      <td>Softmax(dim=2)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.501760</td>\n",
       "      <td>0.495741</td>\n",
       "      <td>0.503848</td>\n",
       "      <td>0.561167</td>\n",
       "      <td>51.936246</td>\n",
       "      <td>261.166312</td>\n",
       "      <td>xrp_btc</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;built-in method relu of type object at 0x13f1...</td>\n",
       "      <td>&lt;function sum_squared_error at 0x17138ad90&gt;</td>\n",
       "      <td>Softmax(dim=2)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.501620</td>\n",
       "      <td>0.496005</td>\n",
       "      <td>0.504617</td>\n",
       "      <td>0.554807</td>\n",
       "      <td>53.677305</td>\n",
       "      <td>314.843658</td>\n",
       "      <td>xrp_btc</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;built-in method relu of type object at 0x13f1...</td>\n",
       "      <td>&lt;function sum_squared_error at 0x17138ad90&gt;</td>\n",
       "      <td>Softmax(dim=2)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NeuralNet(\\n  (output_activation): Softmax(dim...</td>\n",
       "      <td>0.563734</td>\n",
       "      <td>[[340, 1572], [372, 2172]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   run  epoch  train loss  validate loss  train accuracy  validate accuracy  \\\n",
       "0    1      1    0.505401       0.494098        0.495273           0.565282   \n",
       "1    1      2    0.503017       0.494914        0.504947           0.571268   \n",
       "2    1      3    0.502122       0.495416        0.501979           0.567901   \n",
       "3    1      4    0.501880       0.495816        0.504288           0.561541   \n",
       "4    1      5    0.501760       0.495741        0.503848           0.561167   \n",
       "5    1      6    0.501620       0.496005        0.504617           0.554807   \n",
       "\n",
       "   epoch duration  run duration data_set  hidden_neurons  ...  \\\n",
       "0       51.026065     51.026071  xrp_btc             100  ...   \n",
       "1       52.334167    103.360281  xrp_btc             100  ...   \n",
       "2       52.470519    155.830842  xrp_btc             100  ...   \n",
       "3       53.399140    209.230026  xrp_btc             100  ...   \n",
       "4       51.936246    261.166312  xrp_btc             100  ...   \n",
       "5       53.677305    314.843658  xrp_btc             100  ...   \n",
       "\n",
       "                                   hidden_activation  \\\n",
       "0  <built-in method relu of type object at 0x13f1...   \n",
       "1  <built-in method relu of type object at 0x13f1...   \n",
       "2  <built-in method relu of type object at 0x13f1...   \n",
       "3  <built-in method relu of type object at 0x13f1...   \n",
       "4  <built-in method relu of type object at 0x13f1...   \n",
       "5  <built-in method relu of type object at 0x13f1...   \n",
       "\n",
       "                                     criterion output_activation  \\\n",
       "0  <function sum_squared_error at 0x17138ad90>    Softmax(dim=2)   \n",
       "1  <function sum_squared_error at 0x17138ad90>    Softmax(dim=2)   \n",
       "2  <function sum_squared_error at 0x17138ad90>    Softmax(dim=2)   \n",
       "3  <function sum_squared_error at 0x17138ad90>    Softmax(dim=2)   \n",
       "4  <function sum_squared_error at 0x17138ad90>    Softmax(dim=2)   \n",
       "5  <function sum_squared_error at 0x17138ad90>    Softmax(dim=2)   \n",
       "\n",
       "  learning_rate momentum                      optimizer  validation_split  \\\n",
       "0          0.01      0.1  <class 'torch.optim.sgd.SGD'>               0.1   \n",
       "1          0.01      0.1  <class 'torch.optim.sgd.SGD'>               0.1   \n",
       "2          0.01      0.1  <class 'torch.optim.sgd.SGD'>               0.1   \n",
       "3          0.01      0.1  <class 'torch.optim.sgd.SGD'>               0.1   \n",
       "4          0.01      0.1  <class 'torch.optim.sgd.SGD'>               0.1   \n",
       "5          0.01      0.1  <class 'torch.optim.sgd.SGD'>               0.1   \n",
       "\n",
       "                                                 net test_accuracy  \\\n",
       "0                                                NaN           NaN   \n",
       "1                                                NaN           NaN   \n",
       "2                                                NaN           NaN   \n",
       "3                                                NaN           NaN   \n",
       "4                                                NaN           NaN   \n",
       "5  NeuralNet(\\n  (output_activation): Softmax(dim...      0.563734   \n",
       "\n",
       "             confusion_matrix  \n",
       "0                         NaN  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3                         NaN  \n",
       "4                         NaN  \n",
       "5  [[340, 1572], [372, 2172]]  \n",
       "\n",
       "[6 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'xrp_btc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-98a6fe795866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# record record\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'validate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             record[data_set].append({\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0;34m'max_accuracy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'xrp_btc'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas.plotting import table \n",
    "source_df = pd.DataFrame(m.run_data)\n",
    "display(source_df)\n",
    "record = {\n",
    "    'sp500': [],\n",
    "    'BTCUSDT': []\n",
    "}\n",
    "    \n",
    "df = source_df#.loc[source_df.data_set == data_set]\n",
    "\n",
    "for run_i in df['run'].unique():\n",
    "    run_data = df.loc[df.run == run_i]\n",
    "    epochs = run_data.epoch.values\n",
    "\n",
    "    # Accuracy 1st y-axis\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Loss 2nd y-axis\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    colors = ['red', 'green', 'blue']\n",
    "\n",
    "    for phase_i, phase in enumerate(['train', 'validate']):\n",
    "\n",
    "        accuracy = run_data[f'{phase} accuracy'].values\n",
    "\n",
    "        # record record\n",
    "        if phase == 'validate':\n",
    "            record[data_set].append({\n",
    "                'max_accuracy': np.max(accuracy).round(3),\n",
    "                'epoch': np.where(accuracy == np.max(accuracy))[0] + 1,\n",
    "\n",
    "#                 'run': run_i\n",
    "            })\n",
    "#             for variable in variables:\n",
    "#                 record[data_set][-1][variable] = run_data[variable].values[0]\n",
    "\n",
    "        loss = run_data[f'{phase} loss'].values\n",
    "        phase_accuracy, = ax1.plot(epochs, accuracy, \n",
    "             color=colors[phase_i],   \n",
    "             linewidth=1.0\n",
    "        )\n",
    "        phase_accuracy.set_label(f\"{phase.capitalize()} Accuracy\")\n",
    "\n",
    "        phase_loss, = ax2.plot(epochs, loss, \n",
    "             color=colors[phase_i],   \n",
    "             linewidth=1.0,\n",
    "             linestyle='--' \n",
    "        )\n",
    "        phase_loss.set_label(f\"{phase.capitalize()} Loss\")\n",
    "\n",
    "    ax1.legend(loc='lower left')\n",
    "    ax2.legend(loc='lower right')\n",
    "\n",
    "    x_label = \"Epoch\"\n",
    "#     for variable in variables:\n",
    "#         x_label += f\"\\n{variable} = {run_data[variable].values[0]}\"\n",
    "    ax1.set_xlabel(x_label)\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "\n",
    "    plt.title(f\"{run_data['data_set'].values[0]}: Epoch vs. Accuracy and Loss\")\n",
    "\n",
    "    ax1.set_ybound(lower=0.45, upper=.55)\n",
    "\n",
    "    save_string = \"sigma_relationship.png\"\n",
    "#     for variable in variables:\n",
    "#         save_string = f\"{data_set}_{variable}_{run_data[variable].values[0]}_\" + save_string\n",
    "#     plt.savefig(f\"./{variables[0]}/\" + save_string, bbox_inches='tight')\n",
    "    plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xrp_btc\n",
      "[array([[ 340, 1572],\n",
      "       [ 372, 2172]])]\n",
      "[[ 340 1572]\n",
      " [ 372 2172]]\n",
      "Normalized Confusion Matrix (Run #1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAEmCAYAAAC50k0UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZxWdd3/8dd7QMA9FTQZQFBRweVWRHApo9wwXPLODJdu9c5MCy3Nu+hXLpF2lxZlpXe5l95JmhsqineWOyqIuIAbgsqiCQi4y/b5/XHO4MU4c82Z4drmzPvJ4zy4zjnf63s+18w85jPf5XyPIgIzM7NKqKt2AGZm1nE46ZiZWcU46ZiZWcU46ZiZWcU46ZiZWcU46ZiZWcU46VibSOorKSR1rnYsa0vSEZLmSHpX0m5rUc90ScNKGFrVSLpL0vHVjsPyR75Px9pCUl9gNrBORKxo4vx5wLYRcVyJrrclcD7wRWADYB7wV+DCiHhvLet+GTgzIm5b60BrXKm/L2at5ZZOB6ZEzf8MSNoUmASsC+wVERsCBwCfArYpwSW2AqaXoJ52r738TFg7FhHecrSR/BJ+CxiU7vcEFgDD0v37gAuAh4EPgG3TY/8NPA68DdwGbNrCdfoCAZwMzAdeB85Kzw0HlgHLgXeBp9LjmwJXp+UXA7dm/EznA88AdUXK7A1MBpam/+9dcO4+4KfpZ34HuAfoDnRN4wvgPeDltHyQtAYa3n8NcH76ujtwB7Ak/To/2BAX8Aqwf/q6K/Cb9LPOT193Tc8NA+YC3wPeTL92Jxb5bPelX4NH0nhvBzYD/jf9fk0G+haUvxiYk557AvhsC9+X5n4mTkrP/w9wU0H9vwDuJe0p8eatNVvVA/BWhm8qfAOYAawHTAR+WXDuPuA1YEegM7BOemwesBOwPnATcF0L1+ib/nK+Pn3PziTJreGX7nmN6wDuJOkS2yS97ucKzi0BPtPMtR4FflIklk1JktjX0s90dLq/WcFnfhnYjqS1dB/w84L3N04yxZLOfwN/SONfB/gsH3dTv1Lw+cekcW8O9EgTxk/Tc8OAFWmZdUi6DN8HNmnm890HzCT5g2Lj9Hv7IrB/+nn/DFxdUP44kqTUmSSxvQF0K/J9ae5noiHprJde74T08y4EelX759xb+9zcjM6hiLic5JfUY8CWwI8aFbkmIqZHxIqIWJ4euzYino1kfORs4ChJnTJc7icR8V5EPEPSijm6qULpmMzBwCkRsTgilkfE/QUxfyoiHmrmGpuRtAaaMwJ4KSKuTT/T9cDzwKEFZa6OiBcj4gPgBmDXDJ+tKctJvqZbpZ/hwYhoamD0WGBMRLwZEQuAn5AkxcJ6xqR1TCBpeWxf5LpXR8TLEbEUuIukVfb3SMbTbgRWT4CIiOsiYlH6tfgVSaurWN3Q9M9EQ33vp7GPBa4DTouIuS3UZ9YkJ538upyk5fK7iPio0bk5TZQvPPYqyV+73TNcp/H7ejZTrjfwVkQszlBnY4tIftE3p2d67UKvAvUF+28UvH6fZDJCW1xEktDvkTRL0uiMMTX+2iyKNSdgtBTTvwpef9DE/ur3SjpL0nOSlkpaQtI6aul72dTPxGoR8RgwCxBJ0jZrEyedHJK0AckYwpXAeelAfKGm/jLvXfC6D8lf4gszXK7x++Y3c405wKaSPpWhzsb+DhxRZIB7PslkgEJ9SLoM2+J9ki6lBp9ueBER70TE9yJia+Aw4ExJ+2WIqfBrUzaSPgt8HziKpLvuUyTjXEqLNDddteg0VknfJmkxzU/rN2sTJ518uhiYEhEnkYyj/CHDe46TNFDSeiRjDX+LiJUZ3ne2pPUk7QicSDJmA8lf4n0bEkVEvE7SLXSppE0krSNp34yfZyywEfAnSVsBSKqXNFbSLsAEYDtJx0jqLOmrwECSAf+2mAYcI6mTpOHA5xpOSDpE0raSRPLLfCWwqok6rgd+LKmHpO7AOSRdU+W2Icl40QKgs6RzSL52Ddb4vmQhaTuSiQzHkXSzfV9SW7snrYNz0skZSYeTzFI6NT10JjBI0rEtvPVakgHzN4BuwOkZL3k/SXfTvSQTFu5Jj9+Y/r9I0tT09ddIWlDPk8za+m5B3O+mf6V/QkS8RTI7bTnwmKR30ustBWZGxCLgEJJB80Ukf4kfEhFZWmpN+Q7JeNASkrGZWwvO9Sdpeb1LMo370oj4ZxN1nA9MAZ4mmXk3NT1WbhOBu0kG/l8FPmTNrrOmvi/NSm/+vQ74RUQ8FREvAf8PuFZS15JGbh2Cbw41JN1HMqPpimrHYmb55paOmZlVjJOONUvSsWm3V+PNd++bWZu4e83MzCrGLR0zM6uYmlqWfrPu3aNPn77VDsM6oKdebOstPWZtFx8tJZa/r5ZLZtNpo60iVnyQ/fofLJgYEcNLdf0sairp9OnTl3889Fi1w7AOqOeB51Y7BOuAPpp2ZUnrixUf0HX7ozKX/3DaJVlWHSmpmko6Zma2NgQ1/mQKJx0zs7wQoJL11pWFk46ZWZ64pWNmZhXjlo6ZmVWGoC7LY7Cqx0nHzCwvhLvXzMysUuTuNTMzqyC3dMzMrGLc0jEzs8rwzaFmZlYpvjnUzMwqyi0dMzOrjNrvXqvt6MzMrHXqlH3LQNJwSS9ImilpdBPn+0j6p6QnJT0t6YtFw2vjxzIzs1rTcHNo1q2l6qROwCXAwcBA4GhJAxsV+zFwQ0TsBowELi1Wp5OOmVmeSNm3lg0BZkbErIhYBowDDm9UJoCN0tcbA/OLVegxHTOz3Gj12mvdJU0p2L8sIi4r2K8H5hTszwWGNqrjPOAeSacB6wP7F7ugk46ZWZ60biLBwogYvJZXPBq4JiJ+JWkv4FpJO0XEqqYKO+mYmeVF9m6zrOYBvQv2e6XHCn0dGA4QEZMkdQO6A282VaHHdMzM8qSEEwmAyUB/Sf0kdSGZKDC+UZnXgP0AJA0AugELmqvQLR0zszwpYUsnIlZIGgVMBDoBV0XEdEljgCkRMR74HnC5pDNIJhWcEBHRXJ1OOmZmuVH6m0MjYgIwodGxcwpezwD2yVqfk46ZWZ547TUzM6sIPznUzMwqp/bXXnPSMTPLE3evmZlZxbilY2ZmFeOWjpmZVYQ8pmNmZhWkOicdMzOrAAFy95qZmVWE0q2GOemYmeWG3NIxM7PKcdIxM7OKcdIxM7OKcdIxM7PK8EQCMzOrFHkigZmZVZKTjpmZVYyTjpmZVYyTjpmZVYZAdU46ZmZWAZ5IYGZmFeWkY2ZmlVPbOcdJx8wsN+SWjpmZVVCtJ53afsScmZm1iqTMW8b6hkt6QdJMSaObOP9rSdPS7UVJS4rV55aOmVlOlHr2mqROwCXAAcBcYLKk8RExo6FMRJxRUP40YLdidbqlY2aWJ2rF1rIhwMyImBURy4BxwOFFyh8NXF+sQiedduDv99zNkF0HsvvO2/ObX/7iE+cfeegBhu29Bz026sptt9y0xrlzf/QD9hq8C0MH7cTos75LRFQqbGvnDhjan6f+8h2eHXcGZx237yfO995iY+7+7X8y6apv8fg1ozhoz+0A+MLgbXj4ylOZ/KdRPHzlqXxu0NaVDr3jUqu717pLmlKwndyoxnpgTsH+3PTYJy8tbQX0A/5RLER3r9W4lStX8v0zT+fm2++mZ30v9vvsngwfcSg7DBi4ukyv3n245I9X8vuLx67x3scefYTHHn2Ehx57EoCD99+Xhx+8n8/sO6ySH8Haobo68ZszD2XEGVcz7823eeiKU7jjoed4/pUFq8v84Phh3PSPZ7n81sfZoW8Pbr3oP9jhK79i0dL3OfL71/H6oncY2G9zbh97AtsccWEVP03H0srutYURMbhElx4J/C0iVhYr5KRT456Y8jj9tt6Gvv2Svxb//cijuOuO8WsknT5b9QWgrm7NhqskPvrwI5YtW0ZEsGL5CnpsvkXFYrf2a48BvXh57iJemb8YgBv//gyHfGbAGkknAjZavysAG6/fjdcXvgPAUy+9vrrMjNlv0q1rZ7qs04lly4v+LrISKfHstXlA74L9XumxpowEvt1ShU46Ne71+fOp7/Xx97xnfS+emPJ4pvcOGboXn9n3cwzYphcRwTe++S2232FAuUK1HOnZYyPmvrl09f68BW8zZGCvNcpccNW93D72BE798p6st24XRnz36k/Uc8SwHZn24utOOJVU2hnTk4H+kvqRJJuRwDGfuKS0A7AJMKmlCss6ptPSVDsrr1kvz+TFF57n2RdfZfpLr/HA/f9k0sMPVjssy4mj9t+F6+56km3//SKOOOvPXPnjI9f4K3tAv805/9SDGHXhbVWMsmORRF1dXeatJRGxAhgFTASeA26IiOmSxkg6rKDoSGBcZBg0LltLJ8tUO2vZlj17Mm/ux+N48+fNZcste2Z67x3jb2XwkKFssMEGAOx/4HAmP/Yoe+3z2bLEavkxf8Hb9Np849X79T02Yt6Ct9coc/whu3P49/4MwGPT59Cta2e6b7weC5a8R32Pjfjrz47hpPP/xuz5b1U09o6u1DeHRsQEYEKjY+c02j8va33lbOm0dqqdNWHQ7nsw6+WZvPrKbJYtW8bNf7uB4SMOzfTeXr378MiDD7BixQqWL1/OIw8+wHY77FDmiC0Ppjw/j217b8ZWW27COp078ZX9d+bOh59fo8ycfy1l2O7JWOP2W/WgW5fOLFjyHhtv0I2bL/oaZ//PPUx65rVqhN+hlfrm0FIr55hOU1PthpbxernUuXNnLvzVxRx5+BdZuXIlx/7HCQwYuCM/++m57DZoMAePOJSpT0zmayOPZOmSxdx91x38/IKfMGnK0xx+xJd58P5/ss+QXZHEfvsfyPAvZktY1rGtXLmKM8bewe1jj6dTXR1/uvMJnpv9Jmd/fT+mPj+POx9+ntG/v4tLv/8lTvvq3kTANy64GYBTvrwn29Rvxg9P/Dw/PPHzABx6xjUsWPJeNT9Sx1Hbq+Cgct23IelIYHhEnJTufw0YGhGjGpU7GTgZoFfvPrs//fysssRjVkzPA8+tdgjWAX007UpWvft6ydJE1y36R/2xF2cuP/vXI54o4ZTpTMrZvZZpql1EXBYRgyNicPfuPcoYjplZzrX+5tCKK2fSWT3VTlIXktkN48t4PTOzDk2AlH2rhrKN6UTECkkNU+06AVdFxPRyXc/MzDr446qbmmpnZmblU+M5xysSmJnlSYdu6ZiZWQVVcawmKycdM7OcEMkK4bXMScfMLEecdMzMrDLcvWZmZpWS3KdT21nHScfMLDc6+H06ZmZWWTWec5x0zMzyxC0dMzOrDE8kMDOzSvFEAjMzq6gazzlOOmZmeeKWjpmZVUyN5xwnHTOz3JBbOmZmViENTw6tZU46Zma5oZpf8LOu2gGYmVnpSMq8ZaxvuKQXJM2UNLqZMkdJmiFpuqS/FKvPLR0zs7wo8c2hkjoBlwAHAHOByZLGR8SMgjL9gR8C+0TEYkmbF6vTLR0zs5xouDm0hC2dIcDMiJgVEcuAccDhjcp8A7gkIhYDRMSbxSp00jEzy5FWJp3ukqYUbCc3qq4emFOwPzc9Vmg7YDtJD0t6VNLwYvG5e83MLEda2b22MCIGr+UlOwP9gWFAL+ABSTtHxJKmCrulY2aWIyXuXpsH9C7Y75UeKzQXGB8RyyNiNvAiSRJqkpOOmVlepBMJsm4ZTAb6S+onqQswEhjfqMytJK0cJHUn6W6b1VyF7l4zM8sJlfjJoRGxQtIoYCLQCbgqIqZLGgNMiYjx6bkDJc0AVgL/FRGLmqvTScfMLEdKvSJBREwAJjQ6dk7B6wDOTLcWOemYmeVIXY2vg+OkY2aWIzWec5x0zMzyQoJONb72mpOOmVmO+NEGZmZWMTWec5x0zMzyQiTTpmtZs0lH0u+AaO58RJxelojMzKzNanxIp2hLZ0rFojAzs7XXiufkVEuzSSci/lS4L2m9iHi//CGZmVlb1XjOaXntNUl7pcsbPJ/u/5ukS8semZmZtYpIbg7NulVDlgU/fwMcBCwCiIingH3LGZSZmbVNiRf8LLlMs9ciYk6jfsKV5QnHzMzWRrsd0ykwR9LeQEhaB/gO8Fx5wzIzs9aqZgsmqyxJ5xTgYpJHlM4nWcb62+UMyszM2qbdL/gZEQuBYysQi5mZraXaTjnZZq9tLel2SQskvSnpNklbVyI4MzPLTiQLfmbdqiHL7LW/ADcAWwI9gRuB68sZlJmZtUF6c2jWrRqyJJ31IuLaiFiRbtcB3codmJmZtV67nTItadP05V2SRgPjSNZi+yqNHl1qZma1oT1PmX6CJMk0fIJvFpwL4IflCsrMzFovWZGg2lEUV2zttX6VDMTMzNZee27prCZpJ2AgBWM5EfHncgVlZmZtU9spJ0PSkXQuMIwk6UwADgYeApx0zMxqiFT7N4dmmb12JLAf8EZEnAj8G7BxWaMyM7M2abez1wp8EBGrJK2QtBHwJtC7zHGZmVkb1PqYTpaWzhRJnwIuJ5nRNhWYVNaozMysTUrd0pE0XNILkmamt880Pn9CumLNtHQ7qVh9WdZe+1b68g+S7gY2ioins4VrZmaVIkr7cDZJnYBLgAOAucBkSeMjYkajon+NiFFZ6ix2c+igYuciYmqWC7RGnWDdLp1KXa1Zy95bUu0IrCNaVeJHkwnqSnujzhBgZkTMApA0DjgcaJx0MivW0vlVkXMBfKGtFzUzs/LIMmZSoLukKQX7l0XEZQX79cCcgv25wNAm6vmypH2BF4EzImJOE2WA4jeHfj5bzGZmVgtEqycSLIyIwWt52duB6yPiI0nfBP5EkUZJK5OimZnVsjpl3zKYx5qzlXulx1aLiEUR8VG6ewWwe9H4sn8UMzOrdSVOOpOB/pL6SeoCjATGFxaQtGXB7mHAc8UqzLQMjpmZ1b5kKnTpJhJExApJo4CJQCfgqoiYLmkMMCUixgOnSzoMWAG8BZxQrM4sy+CI5HHVW0fEGEl9gE9HxONr93HMzKzUSr3KdERMoNHjbCLinILXP6QVTx3I0r12KbAXcHS6/w7JvG0zM6sxeVgGZ2hEDJL0JEBELE779szMrIYkz9Op7WVwsiSd5eldqQEgqQewqqxRmZlZm9T67LAs8f0WuAXYXNIFJI81+FlZozIzszZp991rEfG/kp4gebyBgC9FRNEpcWZmVnlSaddeK4css9f6AO+T3HW6+lhEvFbOwMzMrPVqPOdkGtO5k2Q8RySPq+4HvADsWMa4zMyslQR0LvWc6RLL0r22c+F+uvr0t5opbmZmVZSHls4aImKqpKZWGTUzs2rKvrxN1WQZ0zmzYLcOGATML1tEZmbWZqK2s06Wls6GBa9XkIzx3FSecMzMrK2Sm0OrHUVxRZNOelPohhFxVoXiMTOztdBuk46kzukKo/tUMiAzM2u7Uq4yXQ7FWjqPk4zfTJM0HrgReK/hZETcXObYzMysFdp991qqG7CI5PGjDffrBOCkY2ZWS6q4vE1WxZLO5unMtWf5ONk0iLJGZWZmbdKel8HpBGwATc6/c9IxM6sx7b177fWIGFOxSMzMbK3VeEOnaNKp8dDNzKyQEJ1qPOsUSzr7VSwKMzNbe+15GZyIeKuSgZiZ2dprzxMJzMysHRHte0zHzMzaGbd0zMysYmo851BX7QDMzKw0RPJLPeuWqU5puKQXJM2UNLpIuS9LCkmDi9Xnlo6ZWV6otAt+pk8auAQ4AJgLTJY0PiJmNCq3IfAd4LGW6nRLx8wsR9SKLYMhwMyImBURy4BxwOFNlPsp8Avgw5YqdNIxM8uJZBkcZd6A7pKmFGwnN6qyHphTsD83PfbxNaVBQO+IuDNLjO5eMzPLkVZ2ri2MiKJjMEWvJdUBY4ETsr7HScfMLEdKPHttHtC7YL9XeqzBhsBOwH3pWNKngfGSDouIKU1V6KRjZpYbKvWTQycD/SX1I0k2I4FjGk5GxFKg++qrS/cBZzWXcMBJx8wsNwQlXfAzIlZIGgVMJHnczVURMV3SGGBKRIxvbZ1OOmZmOVLqe0MjYgIwodGxc5opO6yl+px0zMzyosT36ZSDk46ZWU40rEhQy5x0zMxyxC0dMzOrmNpOOU46Zma5UuMNHScdM7O8SMZ0ajvrOOmYmeWIWzpmZlYhQm7pmJlZpbilY2ZmFeExHTMzqxxBXY3fHeqkY2aWIx7TMTOzikieHFrtKIqr8YaY3TPxbnbZcXt23GFbLrrw5584/9CDD7DXHoPYoFtnbr7pb2ucW79rJ4buvitDd9+VI484rFIhW04csPcAnrrlbJ697VzOOvGAT5zv/elNuPuy05l0/Q94/K8/5KDPDASgz5ab8taksTw6bjSPjhvNb380stKhd2hqxb9qcEunhq1cuZLvnv5t7rzr/6jv1YvP7LkHhxxyGAMGDlxdpnfvPlx25TX8ZuwvP/H+ddddl8eemFbJkC0n6urEb0YfxYhTf8+8fy3hof/9L+64/xmen/XG6jI/OGk4N/3fVC6/8SF22PrT3Pq7U9lhxLkAzJq7kD1HfvKPJCu/Wp+95pZODZv8+ONss8229Nt6a7p06cJXvjqSO26/bY0yW/Xty8677EJdrY8eWruyx059eXnOQl6Zt4jlK1Zy48SpHDJslzXKRAQbrd8NgI03WJfXFyytRqjWSK23dPybqobNnz+PXr0+fjx5fX0v5s2bV+Qda/rwww/ZZ+hg9t1nT8bfdms5QrSc6rn5xsz91+LV+/P+tZj6HhuvUeaCP05g5BeHMPPun3LL707lzF/cuPpc3/rNmHT9D7jniu+wz27bVCzujq5hTCfrVg1l616TdBVwCPBmROxUrutY8154+VXq6+uZPWsWww/8AjvttDNbb+NfAFYaRw0fzHW3P8rF1/6Dobv048rz/4Pdj/wZbyx8m+0OPoe3lr7HbgN6c8PYkxl05AW8896H1Q65A6j9FQnK2dK5Bhhexvpzr2fPeubOnbN6f968udTX12d+f0PZfltvzb77DmPatCdLHqPl0/w3l9Jri01W79dvsQnzGnWfHf+lvbjpnqkAPPb0bLp1WYfun1qfZctX8NbS9wB48rk5zJq7kP5bbV654DsyJWM6WbdqKFvSiYgHgLfKVX9HMHiPPZg58yVemT2bZcuWceNfxzHikGyz0BYvXsxHH30EwMKFC5k06WEGDBjYwrvMElOmv8q2fXqwVc/NWKdzJ75y0CDuvO/pNcrMeeMthg3ZHoDt+21Bt67rsGDxu3TfZAPq0r6bvvWbsW2fHsyeu7Din6GjUiu2aqj67DVJJwMnA/Tu06fK0dSWzp078+uLf8+hIw5i5cqVHH/CfzJwxx0Zc945DNp9MIccehhTJk/mq185giWLFzPhzts5f8y5TH1qOs8/9xynfeub1NXVsWrVKs76r9FrzHozK2blylWc8YsbuP3Sb9OpTvzptkd5btYbnH3qCKbOeI0773+G0WNv4dKzj+a04z5PBHzjnGsB+MygbTn71BEsX7GSVauC0y4Yx+K336/yJ+oYkjGd2u5eU0SUr3KpL3BH1jGd3XcfHA8/NqVs8Zg1Z5M9RlU7BOuAPnrhBla9/2bJssSAnXeLq2/5Z+bye/Xf5ImIGFyq62dR9ZaOmZmVUG03dJx0zMzypNa718o2kUDS9cAkYHtJcyV9vVzXMjOzRK1PJCjn7LWjI2LLiFgnInpFxJXlupaZmaVKnHUkDZf0gqSZkkY3cf4USc9ImibpIUlFZyx5RQIzs5xIcknplsGR1Am4BDgYGAgc3URS+UtE7BwRuwIXAmOL1emkY2aWF6W/OXQIMDMiZkXEMmAccHhhgYh4u2B3faDolGhPJDAzy5FWjtV0l1R4n8plEXFZwX49MKdgfy4w9BPXlL4NnAl0Ab5Q7IJOOmZmedK6rLOwFPfpRMQlwCWSjgF+DBzfXFl3r5mZ5UZrRnQyZad5QO+C/V7pseaMA75UrEInHTOzHCnxmM5koL+kfpK6ACOB8WteT/0LdkcALxWr0N1rZmY5Uer7byJihaRRwESgE3BVREyXNAaYEhHjgVGS9geWA4sp0rUGTjpmZvlS4rs+I2ICMKHRsXMKXn+nNfU56ZiZ5UitP8TNScfMLEeq9RjqrJx0zMzyopqLqmXkpGNmliPuXjMzs4oQmadCV42TjplZjtR4znHSMTPLlRrPOk46ZmY54jEdMzOrGI/pmJlZxdR4znHSMTPLlRrPOk46ZmY50fC46lrmpGNmlhfZH1lQNU46ZmY5UuM5x0nHzCw/hGq8qeOkY2aWIzWec5x0zMzyoh0sMu2kY2aWKzWedZx0zMxyxFOmzcysYjymY2ZmFVPjOcdJx8wsN3xzqJmZVVZtZx0nHTOznGgPj6uuq3YAZmZWOmrFlqk+abikFyTNlDS6ifNnSpoh6WlJ90raqlh9TjpmZjkiZd9arkudgEuAg4GBwNGSBjYq9iQwOCJ2Af4GXFisTicdM7MckZR5y2AIMDMiZkXEMmAccHhhgYj4Z0S8n+4+CvQqVqGTjplZjrSye627pCkF28mNqqsH5hTsz02PNefrwF3F4vNEAjOznMjabVZgYUQMLs21dRwwGPhcsXJOOmZmOVLiZXDmAb0L9nulx9a8prQ/8CPgcxHxUbEK3b1mZpYnpZ2+NhnoL6mfpC7ASGD8GpeTdgP+CBwWEW+2VKGTjplZjpQy50TECmAUMBF4DrghIqZLGiPpsLTYRcAGwI2Spkka30x1gLvXzMxypdQ3h0bEBGBCo2PnFLzevzX1OemYmeWG/GgDMzOrDC+DY2ZmVsAtHTOzHKn1lo6TjplZjnhMx8zMKsMPcTMzs0ppDxMJnHTMzHLE3WtmZlYxbumYmVnF1HjOcdIxM8uVGs86TjpmZjlS62M6iohqx7CapAXAq9WOo53qDiysdhDWIflnr+22iogepapM0t0k34+sFkbE8FJdP4uaSjrWdpKmlOoJgGat4Z89aw2vvWZmZhXjpGNmZhXjpJMfl1U7AOuw/LNnmXlMx8zMKsYtHTMzqxgnHTMzqxgnHTMzqxivSNBOSdoBOByoTw/NA8ZHxHPVi8rMrDi3dNohST8AxpGssvR4ugm4XtLoasZmZlaMZ6+1Q5JeBHaMiKzq31YAAAO4SURBVOWNjncBpkdE/+pEZh2ZpBMj4upqx2G1zS2d9mkV0LOJ41um58yq4SfVDsBqn8d02qfvAvdKegmYkx7rA2wLjKpaVJZ7kp5u7hSwRSVjsfbJ3WvtlKQ6YAhrTiSYHBErqxeV5Z2kfwEHAYsbnwIeiYimWuBmq7ml005FxCrg0WrHYR3OHcAGETGt8QlJ91U+HGtv3NIxM7OK8UQCMzOrGCcdMzOrGCcdqzhJKyVNk/SspBslrbcWdV0j6cj09RWSBhYpO0zS3m24xiuSPvEI4OaONyrzbiuvdZ6ks1obo1l74aRj1fBBROwaETsBy4BTCk9KatMEl4g4KSJmFCkyDGh10jGz0nHSsWp7ENg2bYU8KGk8MENSJ0kXSZos6WlJ3wRQ4veSXpD0d2Dzhook3SdpcPp6uKSpkp6SdK+kviTJ7Yy0lfVZST0k3ZReY7KkfdL3bibpHknTJV1BMh24KEm3Snoifc/Jjc79Oj1+r6Qe6bFtJN2dvufBdC09s9zzlGmrmrRFczBwd3poELBTRMxOf3EvjYg9JHUFHpZ0D7AbsD0wkORmxBnAVY3q7QFcDuyb1rVpRLwl6Q/AuxHxy7TcX4BfR8RDkvoAE4EBwLnAQxExRtII4OsZPs5/ptdYF5gs6aaIWASsD0yJiDMknZPWPYrkaZunRMRLkoYClwJfaMOX0axdcdKxalhXUsN9Hg8CV5J0ez0eEbPT4wcCuzSM1wAbA/2BfYHr05tg50v6RxP17wk80FBXRLzVTBz7AwOl1Q2ZjSRtkF7j39P33imp8Y2QTTld0hHp695prItIliX6a3r8OuDm9Bp7AzcWXLtrhmuYtXtOOlYNH0TEroUH0l++7xUeAk6LiImNyn2xhHHUAXtGxIdNxJKZpGEkCWyviHg/vUmyWzPFI73uksZfA7OOwGM6VqsmAqdKWgdA0naS1gceAL6ajvlsCXy+ifc+CuwrqV/63k3T4+8AGxaUuwc4rWFHUkMSeAA4Jj12MLBJC7FuDCxOE84OJC2tBnVAQ2vtGJJuu7eB2ZK+kl5Dkv6thWuY5YKTjtWqK0jGa6ZKehb4I0nL/BbgpfTcn4FJjd8YEQuAk0m6sp7i4+6t24EjGiYSAKcDg9OJCjP4eBbdT0iS1nSSbrbXWoj1bqCzpOeAn7Pm8kTvAUPSz/AFYEx6/Fjg62l800keyGeWe14Gx8zMKsYtHTMzqxgnHTMzqxgnHTMzqxgnHTMzqxgnHTMzqxgnHTMzqxgnHTMzq5j/D/BRwUA/2ISQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "source_df = pd.DataFrame(m.run_data)\n",
    "print(data_set)\n",
    "def plot_confusion_matrix(df_row, normalize=True, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    print(df_row.confusion_matrix.values)\n",
    "    cm = deepcopy(df_row.confusion_matrix.values[0])\n",
    "    classes = global_labels #df_row['label_subset'].values[0]\n",
    "    print(cm)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(f\"Normalized Confusion Matrix (Run #{df_row.run.values[0]})\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "#     new = [[] for class_ in range(len(classes))]\n",
    "#     print(new[0])\n",
    "#     for row_i, row in enumerate(cm):\n",
    "#         for col_i, col in enumerate(row):\n",
    "#             print(row, col)\n",
    "#             print(row_i, col_i)\n",
    "#             print(col.item(), row.sum().item(), round(col.item() / row.sum().item(), 2))\n",
    "#             new[row_i].append(round(col.item() / row.sum().item(), 2))\n",
    "#             print(new)\n",
    "#             print()\n",
    "#         print()\n",
    "    \n",
    "#     cm = np.asarray(new)\n",
    "#     print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, aspect='auto')\n",
    "    plt.title(f'{data_set}: Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "\n",
    "    x_label = \"Predicted label\"\n",
    "#     for variable in variables:\n",
    "#         x_label += f\"\\n{variable} = {df_row[variable].values[0]}\"\n",
    "    plt.xlabel(x_label)\n",
    "    \n",
    "    save_string = \"confusion_matrix.png\"\n",
    "#     for variable in variables:\n",
    "#         save_string = f\"{data_set}_{variable}_{df_row[variable].values[0]}_\" + save_string\n",
    "#     plt.savefig(f\"./{variables[0]}/\" + save_string, bbox_inches='tight')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "for data_set in list(source_df.data_set.unique()):\n",
    "    df = source_df.loc[source_df.data_set == data_set]\n",
    "#     display(df)\n",
    "    for run_i in df['run'].unique():\n",
    "        final_epoch_df = df.loc[df.run == run_i][-1:]\n",
    "#         print(final_epoch_df.confusion_matrix)\n",
    "#         plot_confusion_matrix(final_epoch_df)#, variables)\n",
    "#         break\n",
    "#     break\n",
    "plot_confusion_matrix(df.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = 100\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo",
   "language": "python",
   "name": "algo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
